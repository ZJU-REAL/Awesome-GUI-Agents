#### 统计结果

GUI 相关文章共70篇，最高平均分：6，最低平均分：2.5，总体平均分：4.22

最高分文章:**1、ViMo: 一种用于应用智能体的生成式视觉 GUI 世界模型**

​					**2、OmniActor: 适用于 2D 和 3D 世界的通用 GUI 与具身智能体**

​					**3、ScaleCUA: 利用跨平台数据扩展开源计算机使用智能体**

最低分文章:**1、通用扫描器遇见专业定位器：一种用于鲁棒 GUI 定位的协同粗细粒度框架（2.5）**

​					**2、GUI-PRA：面向 GUI 任务的流程奖励智能体（2.8）**

​					**3、GUI-Shepherd：面向长序列 GUI 任务的可靠流程奖励与验证（2.7）**

[2,3):三篇（4.3%）     [3,4):二十二篇（31.4%）     [4,5):二十七篇（38.6%）     [5,6):十五篇（21.4%）     [6,7):三篇（4.3%）



#### 具体统计如下

标题: 通过置信度引导的负强化学习实现无标注 GUI 定位

TLDR: 为解决 GUI 智能体扩展中人工标注数据成本高的瓶颈，基于坐标令牌置信度特征和稀疏 GUI 坐标空间中负样本学习信号更可靠的洞察，提出的无标注训练范式（CRL 和 CNRL）在多个基准测试中表现优异，证实坐标令牌置信度可作为人工标注的有效替代方案。

初试分数: 6，4，6，4

置信度: 3，4，4，5

平均分:5



标题:V2P：通过背景抑制与中心峰值进行视觉注意力校准以实现 GUI 定位

TLDR: 为解决 GUI 智能体扩展中人工标注数据成本高的瓶颈，基于坐标令牌置信度特征和稀疏 GUI 坐标空间中负样本学习信号更可靠的洞察，提出的无标注训练范式（CRL 和 CNRL）在多个基准测试中表现优异，证实坐标令牌置信度可作为人工标注的有效替代方案。

初试分数: 4，4，4，4

置信度: 3，4，4，4

平均分:4



标题:通过视觉反馈学习具有空间推理能力的 GUI 定位

TLDR: 为解决高分辨率、复杂布局 GUI 的定位问题，将该任务重构为交互式搜索任务的 GUI-Cursor 7B 模型，通过多步在线强化学习训练，能输出光标移动动作定位 UI 元素，在 ScreenSpot-v2 和 ScreenSpot-Pro 基准上实现最先进精度，且训练中移动步数随精度提升而减少，可自适应应对不同难度样本。

初试分数: 4，6，6，4

置信度: 4，4，4，3

平均分:5



标题:UI-Ins：通过多视角指令即推理增强 GUI 定位

TLDR:为解决现有 GUI 定位方法忽视指令多样性影响的问题，基于 “指令即推理” 范式及两阶段训练框架（多样化指令的监督微调 + 路径选择与组合的强化学习）提出的 UI-Ins 系列模型，在多个基准测试中取得最先进成果，还展现出涌现推理能力与优异的智能体性能。

初试分数: 2，6，6，6

置信度: 4，5，4，3

平均分:5



标题: UItron：具有先进感知与规划能力的基础 GUI 智能体

TLDR: 通过创新的数据流水线和训练框架，所提出的 UItron 在中文和英文 GUI 智能体交互场景中均取得了显著进展。

初试分数: 4，6，2，2，3

置信度: 3，3，4，4，5

平均分:3.4



标题: GUI-ReWalk：通过随机探索与意图感知推理为 GUI 智能体进行大规模数据生成

TLDR: 为解决 GUI 智能体发展中高质量轨迹数据稀缺的问题，推理增强的多阶段框架 GUI-ReWalk 通过随机探索与推理引导相结合的方式，生成真实多样且支持长程工作流的 GUI 轨迹数据，用其训练的模型在多个基准测试中表现优异，证实该框架具备可扩展性与数据高效性，能推动 GUI 智能体的真实场景自动化发展。

初试分数: 4，4，2

置信度: 4，4，3

平均分:3.3



标题: GOLD：GUI 智能体高效视觉定位中的全局概览到局部细节

TLDR: 为解决基于 VLM 的 GUI 定位计算开销大、难以边缘部署的问题，无调优且适配不同界面密度的 GOLD 框架通过全局剪枝、局部细化与全局 - 局部上下文融合三阶段流程，在 ScreenSpot-V2 基准上减少 78% 计算量的同时提升 0.7% 准确率，证实了全局到局部定位方案的高效性。

初试分数: 4，4，6

置信度: 3，2，3

平均分:4.7



标题: GUI-R1：一种用于 GUI 智能体的通用 R1 风格视觉-语言-动作模型

TLDR: 为解决现有基于 LVLM 的 SFT 训练范式在 GUI 智能体开发中数据需求大、泛化能力弱的问题，首个针对高级现实任务场景的强化学习框架 GUI-R1 通过统一动作空间规则建模，利用少量跨平台高质量数据及改进的策略优化算法，在三大平台的八个基准测试中以仅 0.02% 的数据量（3K vs. 13M）超越此前 SOTA 方法，证实了该强化学习方案提升 LVLM 实际 GUI 任务执行能力的巨大潜力。

初试分数: 2，6，4，4

置信度: 5，2，4，4

平均分:3.5



标题: GUI‑AIMA：通过上下文锚点对齐内在多模态注意力以实现 GUI 定位

TLDR: 我们提出了 GUI-AIMA，一种基于注意力的 GUI 视觉定位方法，通过查询自适应的多头加权，在锚定注意力上进行监督。

初试分数: 4，4，6，2

置信度: 4，4，4，5

平均分:4



标题: 通过显式位置到坐标映射改进 GUI 定位

TLDR: RULER 令牌提供显式的位置到像素映射，I-MRoPE 平衡空间编码，将 GUI 定位从回归任务转变为引用任务，并具有强大的高分辨率泛化能力。

初试分数: 2，4，4，4

置信度: 3，3，4，5

平均分:3.5



标题: 超越点击：通过文本拖拽迈向通用 GUI 定位的一步

TLDR: 我们通过自动化的数据收集流水线和一个全面的基准测试，增强并评估了现有定位模型的文本拖拽能力。

初试分数: 4，4，2，6

置信度: 4，3，4，3

平均分:4



标题: 误差即信号：通过嵌入式龙格-库塔引导进行刚度感知的扩散采样

TLDR: ERK-Guid 基于扩散模型中刚性区域 ODE 轨迹局部截断误差（LTE）与主导特征向量相关的关键观察，无需辅助网络，通过利用检测到的刚性来减少 LTE、稳定采样，在合成数据集和 ImageNet 基准上持续优于现有 SOTA 方法。

初试分数: 4，6，4，4

置信度: 3，3，3，3

平均分:4.5



标题: 合而为一：通过掩码轨迹预测统一预训练 GUI 智能体

TLDR: 为解决现有 GUI 智能体预训练策略直接统一导致的目标不一致与数据异质性问题，统一框架 MTP 通过掩码方式将多样预训练策略整合为一致目标，结合角色感知适配器学习模块处理数据异质性，在四大 GUI 导航基准测试中显著优于此前方法并达成 SOTA，展现出优异的有效性与泛化能力。

初试分数: 4，4，2，4

置信度: 4，4，3，5

平均分:3.5



标题: VistaGUI：迈向更鲁棒和智能的 GUI 自动化

TLDR: 为解决现有 GUI 自动化智能体在 RAG 检索精度低、依赖单模态感知及缺乏故障恢复机制导致的脆弱性问题，多模态 GUI 智能体 VistaGUI 通过并行指令理解模块、自适应多模态感知模块与环境感知状态管理系统的统一框架，在各类 GUI 自动化任务中显著优于强基线模型，在任务成功率、恢复速度及整体稳健性上表现突出。

初试分数: 4，4，4，4，2

置信度: 2，2，3，4，3

平均分:3.6



标题: ViMo：一种用于应用智能体的生成式视觉 GUI 世界模型

TLDR: 一种视觉世界模型，能够生成应用 GUI 以帮助应用智能体预见动作结果并做出更好决策。

初试分数: 6，4，6，8

置信度: 3，3，4，4

平均分:6 



标题: GUI-Spotlight：通过自适应迭代焦点细化增强 GUI 视觉定位

TLDR: 一种为图像接地推理训练的模型，能够动态调用多个专用工具来迭代地缩小其焦点到屏幕的相关区域，从而显著提高视觉定位的准确性。

初试分数: 4，6，4，2

置信度: 5，3，3，5

平均分:4



标题: 通用扫描器遇见专业定位器：一种用于鲁棒 GUI 定位的协同粗细粒度框架

TLDR: 为提升 GUI 定位性能，受人类交互方式启发的粗细粒度协同框架 GMS 将通用 VLM 作为 “扫描器” 识别潜在兴趣区域、微调定位模型作为 “定位器” 输出精准坐标，通过五级架构与跨模态通信的分层搜索，在 ScreenSpot-Pro 数据集上实现显著精度提升，且大幅优于各类强基线模型，展现出通用 GUI 定位的稳健性与潜力。

初试分数: 0，4，4，2

置信度: 5，5，4，4

平均分:2.5 



标题: SCREEN-SBERT：嵌入 GUI 屏幕的功能语义以支持 GUI 智能体

TLDR: 为解决 GUI 智能体知识检索中结构化元数据可用性低、纯视觉方法功能匹配不足的问题，聚焦于功能语义层面截图检索的纯视觉方法 Screen-SBERT，通过对比学习嵌入框架定义功能等价性与功能页面类概念，在真实移动 App 场景中检索功能等价截图的效果优于多个基线模型，为两阶段检索框架的第一阶段提供有效支撑。

初试分数: 4，6，4，8

置信度: 4，3，4，4

平均分:5.5



标题: 定位 GUI 万物：通过连续坐标解码实现高效且语义感知的解析

TLDR: 为解决现有 GUI 定位方法中离散令牌生成限制精度与效率、预测受限于预定义元素的问题，端到端框架 GGA 通过将 MLLM 与回归解码器结合实现连续坐标解码，融入拒绝机制减少幻觉，并配套推出 ScreenParse 基准，在多个数据集上持续优于 SOTA 方法，提升了定位精度、推理速度与泛化能力

初试分数: 4，8，2，2

置信度: 4，4，4，5

平均分:4



标题: Ferret-UI Lite：构建小型端侧 GUI 智能体的经验教训

TLDR: 为解决小型端侧模型在跨平台 GUI 交互中的自主智能体开发难题，3B 参数的紧凑端到端 GUI 智能体 FERRET-UI LITE 通过融合真实与合成 GUI 数据、结合思维链推理与视觉工具使用、设计奖励机制的强化学习等端侧优化技术，在 GUI 定位和导航基准测试中取得与其他小型 GUI 智能体相当的竞争力，同时公开了开发紧凑端侧 GUI 智能体的方法与经验

初试分数: 4，4，4，6

置信度: 4，3，5，5

平均分:4.5



标题: D-Artemis：一种用于移动 GUI 多智能体的审慎认知框架

TLDR: 为解决 GUI 智能体端到端训练数据瓶颈、错误检测成本高及指导矛盾风险等问题，受人类 “思考 - 对齐 - 反思” 认知循环启发的审慎框架 D-Artemis，通过应用专属提示检索、执行前思想 - 动作一致性检查与动作修正、执行后状态反思等组件，在不依赖复杂轨迹数据训练的情况下增强 MLLM 的 GUI 任务能力，在 AndroidWorld（75.8% 成功率）和 ScreenSpot-V2（96.8%）基准上达成新 SOTA，各组件贡献经消融实验验证。

初试分数: 4，2，2，4

置信度: 3，4，4，4

平均分:3



标题: MobileGUI-RL：通过在线环境中的强化学习推进移动 GUI 智能体

TLDR: 为解决现有基于视觉的 GUI 智能体依赖离线预收集轨迹训练导致的扩展性差、过拟合特定 UI 模板及对未知环境鲁棒性弱等问题，在线训练框架 MobileGUI-RL 通过自探索与过滤合成可学习任务课程，并将 GRPO 适配于 GUI 导航（结合轨迹感知优势与平衡任务成功和执行效率的复合奖励），在三个在线移动智能体基准测试中实现持续性能提升，验证了其有效性。

初试分数: 4，4，2，4，4

置信度: 4，4，4，4，5

平均分:3.6



标题: 通过强化学习微调实现移动 GUI 自动化的难度感知推理

TLDR: 为解决现有 GUI 智能体采用统一思维链（CoT）推理导致的计算开销冗余与简单步骤性能下降问题，AdaGUI-R1 开创难度感知推理范式，通过自监督机制生成难度感知推理轨迹以赋予动态调节推理深度的基础能力，并借助 GAPO 算法（含自适应思维奖励与难度感知高斯带宽探索奖励）增强推理性能，最终在减少 40% 冗余推理令牌的同时提升 5% 动作精度，达成 GUI 自动化新 SOTA。

初试分数: 4，4，2，4，4

置信度: 3，4，3，4，5

平均分:3.6



标题: GUI-360°：计算机使用智能体的综合数据集与基准

TLDR: 为填补现实世界计算机使用智能体（CUAs）任务稀缺、多模态轨迹自动采集标注管道缺失及统一基准缺乏的空白，大规模综合数据集与基准套件 GUI-360° 通过自动化流程构建，包含 120 万 + Windows 办公应用动作步骤及多模态数据，支持 GUI 定位、屏幕解析、动作预测三大任务与混合动作空间，基准测试显示 SOTA 视觉语言模型存在原生短板，而监督微调可带来显著性能提升。

初试分数: 6，4，6，2

置信度: 3，3，3，5

平均分:4.5



标题: UltraCUA：通过 GUI 和程序化控制扩展计算机使用智能体

TLDR: 为解决计算机使用智能体（CUAs）依赖原始 GUI 动作导致的级联故障与性能瓶颈、缺乏编程接口能力的问题，基础模型 UltraCUA 通过混合控制将 GUI 原语与高级编程工具调用无缝融合，依托自动化工具扩展管道、合成数据引擎、多智能体轨迹生成及两阶段训练 pipeline，在 OSWorld 实现 27% 相对性能提升且步骤快 11%，在 WindowsAgentArena 跨域测试达成 21.7% 成功率，既减少错误传播又保障执行效率，构建了连接原始 GUI 交互与编程智能的可扩展范式。

初试分数: 6，4，6，4

置信度: 3，4，4，5

平均分:5



标题: 用于 GUI 智能体的自动扩展连续记忆

TLDR: 该研究提出一种连续记忆机制（以 VLM 为编码器将 GUI 轨迹编码为固定长度连续嵌入）与自动扩展数据飞轮，在降低上下文成本、保留视觉细节的同时实现记忆规模与检索深度的单调性能提升，仅微调少量参数便使 Qwen-2.5-VL-7B 在长视野和分布偏移场景下达到接近 GPT-4o 等闭源模型的真实 GUI 任务表现，相关数据与代码将公开。

初试分数: 4，6，2，4

置信度: 4，3，4，4

平均分:4



标题: 通过解耦训练与自适应数据筛选实现 GUI 智能体的高效多轮强化学习

TLDR: 该研究提出解耦智能体强化学习训练框架 DART，通过异步分离环境集群、滚动服务、数据管理器和训练器四大模块提升系统效率，结合自适应数据筛选策略（补充高难度任务成功轨迹等），使 DART-GUI-7B 在 OSWorld 基准测试中实现 42.13% 的任务成功率（较基础模型提升 14.61%，超开源 SOTA 7.34%），相关训练框架、数据及模型检查点将通过指定网址开源。

初试分数: 6，4，6

置信度: 4，4，3

平均分:5.3



标题: GUI-PRA：面向 GUI 任务的流程奖励智能体

TLDR: 我们介绍 GUI-PRA，一种无需训练的评判智能体，通过动态记忆和自适应 UI 感知的视角，解决了 GUI 任务监督中的关键失败模式。

初试分数: 4，2，2，4，2

置信度: 2，4，3，2，3

平均分:2.8



标题: GUI-KV：通过具有时空感知能力的 KV 缓存实现高效 GUI 智能体

TLDR: 该研究针对 VLM 驱动的 GUI 智能体长序列高分辨率截图处理效率低、推理慢的问题，基于 GUI 任务中所有 Transformer 层注意力稀疏度均较高的洞察，提出无需重训练的即插即用 KV 缓存压缩方法 GUI-KV，通过空间显著性引导和时间冗余评分利用 GUI 特有冗余，在多个基准测试中优于同类缓存压缩基线，在 AgentNetBench 的 5 截图场景下解码 FLOPs 降低 38.9% 且步骤准确率较全缓存基线提升 4.1%。

初试分数: 2，4，6，2

置信度: 5，3，3，4

平均分:3.5



标题: GUI-Shepherd：面向长序列 GUI 任务的可靠流程奖励与验证

TLDR: 我们提出了一个 PRM，并对 GUI 智能体中的流程监督进行了首次系统性研究，范围从在线长视野任务到离线单步预测，从 RL 训练到推理验证。

初试分数: 2，2，4

置信度: 3，4，4

平均分:2.7



标题: InfiGUI-R1：将多模态 GUI 智能体从反应型执行者推进到审慎型推理者

TLDR: 该研究针对 MLLM 驱动的 GUI 智能体在整体训练范式中存在的能力层级不匹配问题，提出 “先赋予、后内化” 的两阶段分层训练范式 Actor2Reasoner，通过第一阶段认知赋予（目标监督微调注入关键推理技能）和第二阶段策略内化（RL 将能力内化为稳健决策策略），使实例化的 InfiGUI-R1 在 AndroidControl 等基准测试中达成 SOTA 性能，证实分离基础能力赋予与策略内化是构建高性能 GUI 智能体的更优路径。

初试分数: 8，2，4，6

置信度: 5，3，3，3

平均分:5



标题: ProRe：通过推理器-执行者协作实现 GUI 智能体的主动奖励系统

TLDR: 该研究针对现有奖励方法难以泛化到 GUI 智能体、静态 LLM 评判法准确率有限的问题，提出主动奖励系统 ProRe，通过通用推理器调度目标状态探测任务，由领域特定评估智能体与环境主动交互收集额外观测，实现更准确可验证的奖励分配，在 3K + 轨迹上奖励准确率和 F1 分数最高提升 5.3% 和 19.4%，与 SOTA 策略智能体集成后成功率最高提升 22.4%。

初试分数: 4，6，6，6

置信度: 4，3，4，4

平均分:5.5



标题: Agent-ScanKit：通过敏感性扰动揭示多模态智能体的记忆与推理能力

TLDR: 我们提出了 Agent-ScanKit，通过受控扰动来探测基于 MLLM 的智能体的记忆和推理能力。结果表明，大多数智能体依赖记忆而非真正的推理，违背了安全优先于自主性的原则。

初试分数: 4，4，8，4

置信度: 4，2，3，3

平均分:5



标题: GAIA：用于训练 GUI 测试时缩放批评模型的数据飞轮系统

TLDR: 该研究针对 LVLM 驱动的 GUI 智能体操作不可逆（错误操作可能导致灾难性偏差）的问题，提出训练框架 GAIA，通过迭代式批评能力提升基础 GUI 智能体的测试时缩放（TTS）性能：先利用基础智能体的正负动作样本训练直觉批评模型（ICM）以评估动作即时正确性，再通过批评模型引导智能体收集优化样本形成自改进循环，训练出辨别能力更强的多轮批评模型。实验表明，ICM 能提升多种闭源和开源模型的测试时性能，且随数据循环利用性能逐步提升，相关代码和数据集将公开。

初试分数: 4，6，4，4

置信度: 3，4，3，5

平均分:4.5



标题: GUI-Shift：通过自监督强化学习增强基于 VLM 的 GUI 智能体

TLDR: 该研究针对 GUI 智能体训练依赖大规模标注数据集（耗时且易出错）的问题，提出自监督逆动力学任务 K-step GUI Transition（让 VLM 通过预测导致两个 GUI 状态转换的初始动作来学习 GUI 动态，无需自然语言指令，可从现有轨迹或自动探索构建可扩展数据集），并基于此提出结合规则优化与数据过滤的 RL 框架 GUI-Shift。在四个基准测试中，GUI-Shift 训练对 GUI 自动化和定位任务均有良好泛化性，使 GUI 自动化准确率最高提升 11.2%，为利用无标注 GUI 轨迹训练提供了可扩展方案。

初试分数: 6，4，6，6

置信度: 2，4，4，3

平均分:5.5



标题: 将计算机使用智能体作为自动 GUI 设计的评判者

TLDR: \bench 基准涵盖 52 个不同领域应用及 1560 个可验证的真实场景模拟任务，提出 “编码员（设计师）-CUA（评估者）协作” 框架，通过 CUA 仪表盘将导航历史转化为可解释指导以优化设计，核心以任务可解性和导航成功率为衡量标准，推动界面设计向智能体原生的高效可靠转变，助力智能体从被动使用转向数字环境主动参与。

初试分数: 2，4，4

置信度: 4，4，4

平均分:3.3



标题: MobileA3gent：利用来自不同用户的去中心化自源数据训练移动 GUI 智能体

TLDR: MobileA3gent 是一款利用全球用户分布式自源数据训练移动 GUI 智能体的协作框架，通过 Auto-Annotation 组件在用户日常手机使用中低成本自动收集高质量数据集，借助 FedVLM-A 组件结合情节级和步骤级变异性的自适应全局聚合增强非独立同分布下的联邦 VLM 训练，仅需传统方法 1% 的成本便实现更优性能，其代码已公开，为解决移动 GUI 智能体训练的数据收集与隐私保护难题提供了方案。

初试分数: 2，4，6，6

置信度: 4，3，4，3

平均分:4.5



标题: RISK：电子商务风险管理中的 GUI 智能体框架

TLDR: 我们介绍 RISK，一个使 GUI 智能体能够自动化复杂、多步骤的 Web 交互以进行电子商务风险管理的框架。

初试分数: 4，4，2

置信度: 3，4，3

平均分:3.3



标题: $M^2$-Miner：用于移动 GUI 智能体数据挖掘的多智能体增强 MCTS

TLDR: 我们提出了 M-Miner，一个基于蒙特卡洛树搜索的协作多智能体框架，能够高效挖掘 GUI 交互轨迹数据，从而降低人工标注的高成本。

初试分数: 4，4，6，4

置信度: 3，3，2，4

平均分:4.5



标题: GUI 知识基准：揭示 VLM 在 GUI 任务失败背后的知识差距

TLDR: 大型视觉语言模型（VLMs）在图形用户界面（GUI）任务自动化方面虽有进展但仍不及人类，研究团队提出该差距源于核心 GUI 知识缺失，将其提炼为界面感知、交互预测和指令理解三个维度，构建跨 6 大平台 292 款应用的 GUI Knowledge Bench 基准，评估证实当前 VLMs 在系统状态感知、动作预测等方面存在不足，且 GUI 知识与任务成功率密切相关，该研究为 VLMs 选型和更高效 GUI 智能体的构建提供了支撑。

初试分数: 2，6，4，6

置信度: 4，3，4，4

平均分:4.5



标题: HyperClick：通过不确定性校准推进可靠的 GUI 定位

TLDR: HyperClick 通过不确定性校准与双重奖励机制，提升GUI智能体在定位任务中的准确率与置信度一致性，缓解过度自信，实现更可靠的 GUI 自动化。

初试分数: 2，4，2，6，4

置信度: 4，3，2，3，4

平均分:3.6



标题: GAIR：通过信息联合推理与群体反思实现 GUI 自动化

TLDR: GAIR 通过引入通用 MLLM 融合多专业 GUI 模型信息并驱动其“群体反思”，整合异构能力，显著提升跨任务 GUI 自动化性能与可靠性。

初试分数: 4，4，2，2

置信度: 3，3，4，3

平均分:3



标题: LightAgent：轻量级且成本高效的移动智能体

TLDR: LightAgent 通过“端侧小模型主导+云端大模型协同”与两阶段训练，在降低调用成本的同时实现接近大模型的手机 GUI 任务表现。

初试分数: 4，2，4，4

置信度: 3，4，4，4

平均分:3.5



标题: MobileWizard：具有结构化推理和渐进式强化学习的数据高效 GUI 智能体

TLDR: MobileWizard 凭借结构化思维链与渐进式强化学习，仅用 24.8k 轨迹训练的7B模型便在 AndroidWorld 上超越72B对手，实现数据高效的移动端 GUI 智能体。

初试分数: 6，6，2，2

置信度: 4，5，3，3

平均分:4



标题: OmniActor：适用于 2D 和 3D 世界的通用 GUI 与具身智能体

TLDR: OmniActor通过“浅层共享、深层分离”的层异质 MoE 结构化解 GUI 与具身数据冲突，统一动作空间并大规模训练，成为在两类任务上均领先的全能型多模态智能体。

初试分数: 10，4，4，6

置信度: 5，3，3，4

平均分:6



标题: A3：移动 GUI 智能体的安卓智能体竞技场

TLDR: A3通过 100 个在线真实 App 任务与“关键状态” MLLM 自动评估，弥补现有静态基准缺陷，为移动端 GUI 智能体提供动态、高效的实战评测平台。

初试分数: 4，6，4，2，2

置信度: 4，4，3，3，4

平均分:3.6



标题: GuirlVG：通过强化学习的经验探索激励 GUI 视觉定位

TLDR: GuirlVG仅用5.2K样本、以强化学习+对抗KL因子稳定训练，就在GUI视觉定位上超越千万级SFT方法，为低成本高性能GUI-VG提供新范式。

初试分数: 6，6，4，4

置信度: 4，3，3，5

平均分:5



标题: 迈向具有记忆驱动知识演进的自适应 GUI 智能体

TLDR: 我们提出了 MAGNET，一个记忆驱动的框架，使移动应用智能体能够适应 UI 和工作流的变化，以实现鲁棒的长期可靠性。

初试分数: 4，2，4，2

置信度: 4，4，4，4

平均分:3



标题: WorldGUI：一个用于从任意起点进行桌面 GUI 自动化的交互式基准

TLDR: 我们提出了一个名为 WorldGUI 的新型 GUI 基准，它设计了具有各种初始状态的 GUI 任务来模拟真实的人机交互。

初试分数: 2，4，2，6

置信度: 3，4，4，3

平均分:3.5



标题: MobileRL：用于移动 GUI 智能体的在线智能体强化学习

TLDR: MOBILERL 以“难度自适应 GRPO”在线强化学习，用重尾分布重采样与最短路径奖励塑形，把 9B 模型推至 AndroidWorld 80.2% 的新 SOTA，为移动端通用 GUI 智能体提供高效训练框架。

初试分数: 4，8，4，2

置信度: 4，5，4，3

平均分:4.5



标题: SWIRL：移动 GUI 控制中交错强化学习的分阶段工作流

TLDR: 我们提出了 SWIRL，一个为 GUI 多智能体系统设计的交错强化学习的分阶段工作流。

初试分数: 4，4，2，6，2

置信度: 4，2，4，3，4

平均分:3.6



标题: MemGUI-Bench：在动态环境中对移动 GUI 智能体的记忆能力进行基准测试

TLDR: MemGUI-Bench 首次系统评测移动端 GUI 智能体的跨时空记忆能力，用 128 个高内存依赖任务和 8 层“渐进审查”指标揭示现有智能体在记忆场景下 4–10× 性能暴跌，为构建类人长效 GUI 代理立下基准。

初试分数: 6，6，2

置信度: 4，3，4

平均分:4.7



标题: VEM：使用价值环境模型训练 GUI 智能体的无环境探索

TLDR: VEM 用离线预训练的价值环境模型把策略优化与环境交互解耦，仅靠“动作是否推进目标”的语义估值就在 Android 与 Web 双平台达到 SOTA，实现零交互成本、跨布局鲁棒的 GUI 强化学习新范式。

初试分数: 4，4，2，4

置信度: 3，3，3，4

平均分:3



标题: 劫持 JARVIS：针对无特权第三方的移动 GUI 智能体基准测试

TLDR: 本研究首次系统性地调查了移动 GUI 智能体在面对不可信第三方操纵屏幕内容时的脆弱性。

初试分数: 4，4，2，4

置信度: 4，2，3，3

平均分:3.5



标题: WebFactory：将基础语言智能自动压缩为接地的 Web 智能体

TLDR: 一个开源、完全可控的离线 Web 环境，其内置的站点知识驱动着一个流水线，用于生成可执行任务和高质量的 RL 数据，显著提升了 Web 智能体的性能。

初试分数: 6，4，6

置信度: 4，4，4

平均分:5.3 



标题: DKRF：用于移动 GUI 智能体分布外泛化的动态知识推理

TLDR: 我们的 GUI 智能体不仅仅是记忆，而是学习如何利用外部知识来在新环境中解决任务。

初试分数: 4，4，4，4

置信度: 4，4，3，2

平均分:4



标题: GUIrilla：一个可扩展的自动化桌面 UI 探索框架

TLDR: 我们提出了 GUIrilla，一个用于 macOS GUI 探索的自动化框架，以及 GUIrilla-Task，这是首个大规模 macOS 数据集（27,171 个任务，1,108 个应用），将 GUI 截图与详细的无障碍元数据配对。

初试分数: 4，2，2，6

置信度: 3，4，4，4

平均分:3.5



标题: LPO：通过位置偏好优化实现精确的 GUI 智能体交互

TLDR: 我们介绍了位置偏好优化（LPO），一种新颖的方法，通过利用位置数据和信息熵来提高空间精度，从而增强 GUI 交互。

初试分数: 4，4，4，6

置信度: 2，4，2，3

平均分:4.5



标题: Automotive-ENV：在车辆接口系统中对多模态智能体进行基准测试

TLDR: ASURADA 借助 GPS 上下文在首个车机 GUI 基准 Automotive-ENV 上完成 185 项安全与意图任务，验证“地理感知”对车载智能体决策的关键提升。

初试分数: 4，4，4，4

置信度: 3，1，3，4

平均分:4



标题: FingerTip 20K：主动和个性化移动 LLM 智能体的基准

TLDR: FingerTip 20K 用 2 万条真实长期用户轨迹首创“主动建议+个性化执行”双赛道，暴露现有 GUI 智能体与人类巨大差距，为构建以用户为中心的移动智能体提供数据与基准。

初试分数: 8，4，6，4

置信度: 3，3，4，4

平均分:5.5



标题: GAMBIT：一个用于移动 GUI 任务的图结构且决策感知的基准

TLDR: 我们介绍了 GAMBIT，一个用于决策感知移动 GUI 智能体的图结构基准，它揭示了在长视野和分支任务中严重的性能下降，为未来智能体开发提供了一个具有挑战性的诊断测试平台。

初试分数: 4，4，4，4

置信度: 4，4，4，3

平均分:4



标题: ReGUIDE：通过空间推理与搜索实现数据高效的 GUI 定位

TLDR: ReGUIDE 以自生成推理+空间自监督在线强化学习，用 0.2 % 数据量刷新网页 GUI 元素定位 SOTA，并借测试时空间搜索聚合实现低成本高精度定位。

初试分数: 4，6，4

置信度: 4，3，5

平均分:4.7



标题: 训练视觉语言模型以在开放 GUI 世界中进行多样化探索

TLDR: ScreenExplorer 以“好奇心+状态变化”双探索奖励和流式经验蒸馏，让 VLM 在无任何标注的全新应用中自主持续交互，突破静态数据依赖，实现 GUI 智能体的终身式泛化探索。

初试分数: 6，2，4，4

置信度: 3，4，4，4

平均分:4



标题: GTA1：GUI 测试时缩放智能体

TLDR: GTA 通过“测试时并行采样-评判选优”解决大动作空间规划，再以强化学习精修定位，实现规划与点击双 SOTA 的跨平台 GUI 智能体。

初试分数: 4，4，8，6

置信度: 4，4，3，5

平均分:5.5 



标题: LongHorizonUI：一个用于 GUI 智能体鲁棒长视野任务自动化的统一框架

TLDR: LongHorizonUI 集成了元素索引多模态感知、分层反思决策和基于回滚的补偿执行，用于长视野 GUI 控制。

初试分数: 6，6，2，4

置信度: 5，3，4，3

平均分:4.5



标题: ScaleCUA：利用跨平台数据扩展开源计算机使用智能体

TLDR: ScaleCUA 开源最大规模跨 6 系统 GUI 数据集并训练统一模型，以 +26.6 等平均增幅刷新三项基准 SOTA，验证“数据即算力”对通用计算机使用智能体的放大效应。

初试分数: 6，6，6，6

置信度: 4，1，5，2

平均分:6 



标题: 移动智能体在线强化学习中的泛化

TLDR: AndroidWorld-Generalization 以 CMDP 形式化移动 GUI 泛化挑战，配套开源 GRPO-RL 框架与三阶基准显示：7B 模型 RL 训练后在未见实例提升 26.1%，但跨模板/跨应用增益有限，为后续小样本在线适应指明方向。

初试分数: 4，2，6

置信度: 5，3，4

平均分:4



标题: PSBench：通过 GUI 智能体在 Photoshop 中编辑图像

TLDR: PSBench 首发面向 Photoshop 的 600 分层非破坏编辑任务基准，揭示顶尖 MLLM 成功率虽低却能显著加速新手实操，为专业图形软件智能体奠定评测与辅助设计新标杆。

初试分数: 4，6，4

置信度: 3，4，2

平均分:4.7



标题: PrecogUI：通过预认知模拟与经验检索实现主动式 GUI 智能体

TLDR: PrecogUI 将 GUI 智能体从反应型重塑为有远见的决策者，能够预见干扰并闭环处理错误，在长视野、动态任务上实现了最先进的鲁棒性和成功率。

初试分数: 6，4，2，4，4

置信度: 2，3，4，4，4

平均分:4



标题: 让我们分两步思考：通过自接地验证缓解 MLLM 中的同意偏差

TLDR: SGV 用“自设标准再评判”两步法破解 MLLM 评审“同意偏差”，在 Web/机器人/GUI 三大任务上平均提升 20%，并同步开源 10× 加速的 (Visual)WebArena，为数字智能体提供实时、可靠、可扩展的奖励信号。

初试分数: 4，8，4，6

置信度: 3，3，3，4

平均分:5.5