# **<center >ICLR 2026 Paper List</center>**
# **Grounding**
1. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Label-free GUI Grounding via Confidence-guided Negative Reinforcement Learning</font>**

**Abstract**：Graphical User Interface (GUI) grounding maps natural language instructions to precise interface locations for autonomous interaction. Current supervised fine-tuning and reinforcement learning approaches rely heavily on costly annotated data, creating a bottleneck for scaling GUI agents. We introduce a label-free training paradigm leveraging two key insights: (1) coordinate tokens in model outputs exhibit distinct confidence patterns that reliably identify correct predictions, and (2) in sparse GUI coordinate spaces, negative samples provide more reliable learning signals than potentially corrupted positive ones. We propose Confidence-Guided Reinforcement Learning (CRL), which uses coordinate-token confidence to select pseudo-labels from multiple samples and assigns distance-based rewards. We further develop Confidence-Guided Negative Reinforcement Learning (CNRL), which exclusively learns from negative samples. Without using any annotations, CNRL-7B achieves 92.1% on ScreenSpot-V2, surpassing UI-TARS-72B (90.3%) trained on 18.4M labels. On ScreenSpot-Pro, CNRL-7B reaches 33.8%, improving 8.9% absolute over the base model and exceeding GUI-R1-7B (31.0%) trained on 3K labels. On challenging high-resolution benchmarks, CNRL consistently outperforms CRL by 1-1.5%, demonstrating that learning what to avoid can be more effective than learning from uncertain positive examples. Our findings establish coordinate-token confidence as a powerful alternative to manual annotations for scalable GUI agent development.

<div style="text-align: center;">
<img src="figures\Grounding1.png"  width="60%" >
</div>

2. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">V2P: Visual Attention Calibration for GUI Grounding via Background Suppression and Center Peaking</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract</font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">：Precise localization of GUI elements is crucial for the development of GUI agents. Traditional methods rely on bounding box or center-point regression, neglecting spatial interaction uncertainty and visual-semantic hierarchies. Recent methods incorporate attention mechanisms but still face two key issues: (1) ignoring processing background regions causes attention drift from the desired area, and (2) uniform modeling the target UI element fails to distinguish between its center and edges, leading to click imprecision. Inspired by how humans visually process and interact with GUI elements, we propose the Valley-to-Peak (V2P) method to address these issues. To mitigate background distractions, V2P introduces a suppression attention mechanism that minimizes the model's focus on irrelevant regions to highlight the intended region. For the issue of center-edge distinction, V2P applies a Fitts' Law-inspired approach by modeling GUI interactions as 2D Gaussian heatmaps where the weight gradually decreases from the center towards the edges. The weight distribution follows a Gaussian function, with the variance determined by the target's size. Consequently, V2P effectively isolates the target area and teaches the model to concentrate on the most essential point of the UI element. The model trained by V2P achieves the performance with 92.3% and 50.5% on two benchmarks ScreenSpot-v2 and ScreenSpot-Pro. Ablations further confirm each component's contribution, underscoring V2P's generalizability in precise GUI grounding tasks and its potential for real-world deployment in future GUI agents.</font>


<div style="text-align: center;">
<img src="figures\Grounding2.png"  width="60%" >
</div>

3. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Learning GUI Grounding with Spatial Reasoning from Visual Feedback</font>**

**Abstract**：Graphical User Interface (GUI) grounding is a fundamental task for GUI agents, commonly framed as a coordinate prediction task that identifies an on-screen pixel for actions such as clicks and keystrokes. Though recent Vision Language Models (VLMs) show strong capabilities in understanding GUIs, they often fail in grounding when processing GUIs with high resolution and complex layouts. To address this issue, we reframe GUI grounding as an interactive search task, where the VLM agent outputs actions to move a cursor in the GUI to locate UI elements. At each step, the model determines the target object, evaluates the spatial relations between the cursor and the target, and moves the cursor closer to the target conditioned on the movement history. We train our GUI grounding agent, GUI-Cursor 7B, using multi-step online reinforcement learning with a dense trajectory-based reward function. Our experimental results show that GUI-Cursor 7B achieves state-of-the-art accuracy on ScreenSpot-v2 (93.9%) and ScreenSpot-Pro (56.5%). Moreover, the number of movement steps decreases as the grounding accuracy improves during training, and the final model learns to solve the problem within two turns for 95% of instances and can adaptively conduct more steps on more difficult examples.



<div style="text-align: center;">
<img src="figures\Grounding3.png"  width="60%" >
</div>

4. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">ManiCoG: Training-Free Improvement for GUI Grounding via Manipulation Chains</font>**

**Abstract**：GUI grounding is a critical capability for enabling GUI agents to execute tasks such as clicking and dragging. However, in complex scenarios like the ScreenSpot-Pro benchmark, existing models often suffer from suboptimal performance. Utilizing the proposed Masked Prediction Distribution (MPD) attribution method, we identify that the primary sources of errors are twofold: high image resolution (leading to precision bias) and intricate interface elements (resulting in ambiguity bias). To address these challenges, we introduce the Manipulation-based Chain of GUI Grounding (ManiCoG), which incorporates two key manipulations, coarse-to-fine focus and candidate selection, to effectively mitigate these biases. Our extensive experimental results demonstrate that ManiCoG significantly enhances the accuracy of various GUI grounding models in a training-free setting. For instance, applying our method to the TianXi-Action-7B model boosts its accuracy on the ScreenSpot-Pro benchmark from 51.9% to 57.8%. Furthermore, ablation studies confirm the robustness of the ManiCoG approach across diverse parameter configurations, highlighting its stability and effectiveness.



<div style="text-align: center;">
<img src="figures\Grounding4.png"  width="60%" >
</div>

5. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">UI-Ins: Enhancing GUI Grounding with Multi-Perspective Instruction as Reasoning</font>**

**Abstract**：<font style="color:rgb(51, 51, 51);background-color:rgb(255, 253, 250);">GUI grounding, which maps natural-language instructions to actionable UI elements, is a core capability of GUI agents. Prior work largely treats instructions as a static proxy for user intent, overlooking the impact of instruction diversity on grounding performance. Through a careful investigation of existing grounding datasets, we find a 23.3% flaw rate in their instructions and show that inference-time exploitation of instruction diversity yields up to a 76% relative performance improvement. In this paper, we introduce the "Instruction as Reasoning" paradigm, treating instructions as dynamic analytical pathways that offer distinct perspective and enabling the model to select the most effective pathway during reasoning. To achieve this, we propose a two-stage training framework: supervised fine-tuning (SFT) on synthesized, diverse instructions to instill multi-perspective reasoning, followed by reinforcement learning (RL) to optimize pathway selection and composition. Our resulting models, UI-Ins-7B and UI-Ins-32B, achieve state-of-the-art results on five challenging benchmarks and exhibit emergent reasoning, selectively composing and synthesizing novel instruction pathways at inference. In particular, UI-Ins-32B attains the best grounding accuracy: 87.3% on UI-I2E-Bench and 84.9% on MMBench-GUI L2, besides, UI-Ins-7B yields superior agent performance, achieving a 66.1% success rate on the AndroidWorld. All code, data, and models will be publicly released.</font>



<div style="text-align: center;">
<img src="figures\Grounding5.png"  width="60%" >
</div>

6. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">UItron: Foundational GUI Agent with Advanced Perception and Planning</font>**

**Abstract**：The GUI agent aims to enable automated operations on mobile and PC devices, which is an important task as part of the broader goal of achieving artificial general intelligence. The rapid advancement of visual language models has accelerated the development of GUI agents, owing to their powerful capabilities in visual understanding and task planning. However, building a GUI agent remains a challenging task due to the scarcity of operation trajectories, the lack of interactive infrastructure, and the limitation of initial capabilities in foundation models. In this work, we introduce UItron, an open-source foundational model for automatic GUI agents, featuring advanced GUI perception, grounding, and planning capabilities. UItron highlights the necessity of systemic data engineering and interactive infrastructure as foundational components for advancing GUI agent development. It not only systematically studies a series of data engineering strategies to enhance training effects, but also establishes an interactive environment connecting both Mobile and PC devices. In training, UItron adopts supervised finetuning over perception and planning tasks in various GUI scenarios, and then develops a curriculum reinforcement learning framework to enable complex reasoning and exploration for online environments. As a result, UItron achieves superior performance in benchmarks of GUI perception, grounding, and planning. In particular, UItron highlights the proficiency in interaction with top-tier Chinese mobile Apps, we manually collect over one million steps of operation trajectories across the top 100 most popular Apps, and build offline and online agent evaluation environments. Experimental results demonstrate that UItron achieves significant progress in Chinese App scenarios, propelling GUI agents one step closer to real-world applications.


<div style="text-align: center;">
<img src="figures\Grounding6.png"  width="60%" >
</div>

7. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GOLD: Global Overview to Local Detail in Efficient Visual Grounding for GUI Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract：</font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical User Interface (GUI) agents powered by Vision-Language Models (VLMs) have recently emerged as a promising direction for multimodal automation. However, VLM-based GUI grounding incurs substantial computational overhead, making deployment on edge devices infeasible and leading to prohibitively high cloud serving costs. Prior attempts to reduce background or history vision tokens partially alleviate this issue, but either rely on sparsity in foreground elements or require extensive fine-tuning. In this work, we present GOLD, Global Overview to Local Detail for efficient GUI grounding that is tuning-free and robust across varying interface densities. GOLD operates in three stages. At the Global Pruning Stage, we downsample GUI screenshots and feed them into the VLM to identify relevant regions, thereby achieving efficient context reduction. In the Local Refinement Stage, only crops of detected regions are passed to the VLM at high resolution. To retain broader contexts, we aggregate the outputs of both stages to integrate both global and local information in Global-Local Context Fusion Stage. Experimental results show that GOLD reduces TFLOPs by 78%, while even improving accuracy by 0.7%p when it is integrated into the state-of-the-art GUI grounding method on the ScreenSpot-V2 benchmark. These findings highlight the efficiency of our global-to-local grounding framework.</font>


<div style="text-align: center;">
<img src="figures\Grounding7.png"  width="60%" >
</div>

8. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI-R1: A Generalist R1-Style Vision-Language Action Model For GUI Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Existing efforts in building graphical user interface (GUI) agents largely rely on the training paradigm of supervised fine-tuning (SFT) on large vision-language models (LVLMs). However, this approach not only demands extensive amounts of training data but also struggles to effectively understand GUI screenshots and generalize to unseen interfaces. The issue significantly limits its application in real-world scenarios, especially for high-level tasks. Inspired by reinforcement fine-tuning (RFT) in large reasoning models (e.g., DeepSeek-R1), which efficiently enhances the problem-solving capabilities of large language models in real-world settings, we propose GUI-R1, the first reinforcement learning framework designed to enhance the GUI capabilities of LVLMs in high-level real-world task scenarios, through unified action space rule modeling. By leveraging a small amount of carefully curated high-quality data across multiple platforms (including Windows, Linux, MacOS, Android, and Web) and improved policy optimization algorithms to update the model, GUI-R1 achieves superior performance using only 0.02% of the data (3K vs. 13M) compared to previous state-of-the-art methods like OS-Atlas across eight benchmarks spanning three different platforms (mobile, desktop, and web). These results demonstrate the immense potential of reinforcement learning based on unified action space rule modeling in improving the execution capabilities of LVLMs for real-world GUI agent tasks. We will fully open-source GUI-R1 to benefit the research field.</font>


<div style="text-align: center;">
<img src="figures\Grounding8.png"  width="60%" >
</div>

9. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI‑AIMA: Aligning Intrinsic Multi-Modal Attention with a Context Anchor for GUI Grounding</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract：</font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical user interface (GUI) grounding is a key function of computer-use agents, which maps natural-language instructions to actionable screen regions. Existing approaches based on Multimodal Large Language Models (MLLMs) typically formulate it as a text-based coordinate generation task, yet directly generating precise coordinates from visual inputs remains challenging and computationally intensive. An intuitive way to implement GUI grounding is to first select visual patches relevant to the instructions and then determine the precise click location within those patches. Based on the observations that general MLLMs have native grounding capability, which is highly correlated with query-to-visual attentions, we propose GUI-AIMA, an attention-only and coordinate-free supervised fine-tuning framework for efficient GUI grounding. This framework aligns the intrinsic multimodal attention of MLLMs with patch-wise grounding signals. Specifically, we convert coordinate-based grounding boxes into soft patch-wise labels considering patch overlap and the center click manner. For attention aggregation, we simplify the merging of attention predictions among all query tokens into a single anchored attention vector with learnable <ANCHOR> token. More importantly, GUI-AIMA includes a query-adaptive multi-head weighting mechanism for multi-head attention aggregation by prioritizing text-vision affinity heads with visual-sink query tokens. GUI-AIMA-3B, trained on a small training set only with 85k screenshots, achieves the state-of-the-art performance among 3B models (i.e. ,~44.9 average on ScreenSpot-Pro and 90.8 average on ScreenSpot-v2).</font>


<div style="text-align: center;">
<img src="figures\Grounding9.png"  width="60%" >
</div>

10.   **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Improving GUI Grounding with Explicit Position-to-Coordinate Mapping</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI grounding, the task of mapping natural-language instructions to pixel coordinates, is crucial for autonomous agents, yet remains difficult for current VLMs. The core bottleneck is reliable patch-to-pixel mapping, which breaks when extrapolating to high-resolution displays unseen during training. Current approaches generate coordinates as text tokens directly from visual features, forcing the model to infer complex position-to-pixel mappings implicitly; as a result, accuracy degrades and failures proliferate on new resolutions. We address this with two complementary innovations. First, RULER tokens serve as explicit coordinate markers, letting the model reference positions similar to gridlines on a map and adjust rather than generate coordinates from scratch. Second, Interleaved MRoPE (I-MRoPE) improves spatial encoding by ensuring that width and height dimensions are represented equally, addressing the asymmetry of standard positional schemes. Experiments on ScreenSpot, ScreenSpot-V2, and ScreenSpot-Pro show consistent gains in grounding accuracy, with the largest improvements on high-resolution interfaces. By providing explicit spatial guidance rather than relying on implicit learning, our approach enables more reliable GUI automation across diverse resolutions and platforms.


<div style="text-align: center;">
<img src="figures\Grounding10.png"  width="60%" >
</div>

11.  **<font style="color:rgb(44, 58, 74);">GUI-Spotlight: Adaptive Iterative Focus Refinement for Enhanced GUI Visual Grounding</font>**

**<font style="color:rgb(44, 58, 74);">Abstract</font>**<font style="color:rgb(44, 58, 74);">：Multimodal large language models (MLLMs) have markedly expanded the competence of graphical user‑interface (GUI) systems, propelling them beyond controlled simulations into complex, real‑world environments across diverse platforms. Yet their practical usefulness is still constrained by the reliability of visual grounding—the ability to map textual references to precise on‑screen elements. This limitation prevents the system from accurately performing pointer‑level actions such as clicking or dragging. To address it, we introduce GUI‑Spotlight—A model trained for image-grounded reasoning that dynamically invokes multiple specialized tools to iteratively narrow its focus to the relevant region of the screen, thereby substantially improving visual grounding accuracy.</font>


<div style="text-align: center;">
<img src="figures\Grounding11.png"  width="60%" >
</div>

12.  **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Generalist Scanner Meets Specialist Locator: A Synergistic Coarse-to-Fine Framework for Robust GUI Grounding</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Grounding natural language queries in graphical user interfaces (GUIs) presents a challenging task that requires models to comprehend diverse UI elements across various applications and systems, while also accurately predicting the spatial coordinates for the intended operation. To tackle this problem, we propose GMS: Generalist Scanner Meets Specialist Locator, a synergistic coarse-to-fine framework that effectively improves GUI grounding performance. GMS leverages the complementary strengths of general vision-language models (VLMs) and small, task-specific GUI grounding models by assigning them distinct roles within the framework. Specifically, the general VLM acts as a "Scanner" to identify potential regions of interest, while the fine-tuned grounding model serves as a "Locator" that outputs precise coordinates within these regions. This design is inspired by how humans perform GUI grounding, where the eyes scan the interface and the brain focuses on interpretation and localization. Our whole framework consists of five stages and incorporates hierarchical search with cross-modal communication to achieve promising prediction results. Experimental results on the ScreenSpot-Pro dataset show that while the "Scanner" and "Locator" models achieve only 2.0% and 3.7% accuracy respectively when used independently, their integration within GMS framework yields an overall accuracy of 35.7%, representing a 10 * improvement. Additionally, GMS significantly outperforms other strong baselines under various settings, demonstrating its robustness and potential for general-purpose GUI grounding.


<div style="text-align: center;">
<img src="figures\Grounding12.png"  width="60%" >
</div>

13.   **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Grounding GUI Anything: Efficient and Semantically-Aware Parsing via Continuous Coordinate Decoding</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Recent advances in Multimodal Large Language Models (MLLMs) have substantially improved GUI grounding tasks. However, the following challenges still exist in prior methods: (1) They predict coordinates as discrete tokens in an autoregressive text generation paradigm, which constrains grounding accuracy and leads to sub-optimal inference efficiency; (2) Their predictions are restricted to predefined element sets, and lack the ability to comprehensively parse the entire interface, thereby impeding the versatility and generalizability required for downstream applications. To address these challenges, we introduce Grounding GUI Anything (GGA), an efficient end-to-end framework that enables semantically-aware and fine-grained interface parsing with continuous coordinate decoding. By bridging the MLLM with a dedicated regression-based decoder, the enhanced visual and textual representations are jointly leveraged to regress target coordinates within a continuous spatial domain. This design overcomes the quantization and sequential limitations of traditional discrete token modeling, thus enhancing both localization accuracy and inference speed. Furthermore, to improve robustness and mitigate hallucination, we incorporate a rejection mechanism that enables the model to identify non-existent elements. To facilitate systematic evaluation, we introduce ScreenParse, a comprehensive benchmark designed to assess the structural perception capabilities of GUI grounding models across diverse real-world scenarios. Extensive experiments on ScreenSpot, ScreenSpot-v2, CAGUI-Grounding and ScreenParse benchmarks demonstrate that GGA consistently achieves superior performance compared to existing state-of-the-art methods. All resources will be made publicly available for future research.</font>


<div style="text-align: center;">
<img src="figures\Grounding13.png"  width="60%" >
</div>

14.  **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">HyperClick: Advancing Reliable GUI Grounding via Uncertainty Calibration</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Autonomous Graphical User Interface (GUI) agents rely on accurate GUI grounding, which maps language instructions to on-screen coordinates, to execute user commands. However, current models, whether trained via supervised fine-tuning (SFT) or reinforcement fine-tuning (RFT), lack self-awareness of their capability boundaries, leading to overconfidence and unreliable predictions. We first systematically evaluate probabilistic and verbalized confidence in general and GUI-specific models, revealing a misalignment between confidence and actual accuracy, which is particularly critical in dynamic GUI automation tasks, where single errors can cause task failure. To address this, we propose HyperClick, a novel framework that enhances reliable GUI grounding through uncertainty calibration. HyperClick introduces a dual reward mechanism, combining a binary reward for correct actions with a truncated Gaussian–based spatial confidence modeling, calibrated using the Brier score. This approach jointly optimizes grounding accuracy and confidence reliability, fostering introspective self-criticism. Extensive experiments on seven challenge benchmarks show that HyperClick achieves state-of-the-art performance while providing well-calibrated confidence. By enabling explicit confidence calibration and introspective self-criticism, HyperClick reduces overconfidence and supports more reliable GUI automation.</font>


<div style="text-align: center;">
<img src="figures\Grounding14.png"  width="60%" >
</div>

15.   **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical user interface visual grounding (GUI-VG)—a core capability for GUI agents—has primarily relied on supervised fine-tuning (SFT) of multimodal large language models (MLLMs), demanding extensive data curation and significant training costs. However, as MLLMs continue to advance and even cover GUI domains during pretraining, the necessity of exhaustive SFT post-training becomes increasingly questionable. Meanwhile, the recent successes of rule-based reinforcement fine-tuning (RFT) suggest a more efficient alternative. However, despite its promise, the optimal manner of RFT for GUI-VG remains unexplored. To bridge this gap, we introduce GuirlVG, a reinforcement learning–based GUI-VG method built on a systematic empirical study and a novel stabilization technique. Preliminarily, we find that naive application of RFT underperforms the SFT baseline, motivating a deeper exploration of RFT. First, we decompose RFT into its core components and analyze the optimal formulation of each. Second, as part of this exploration, we propose a novel Adversarial KL Factor that dynamically stabilizes training to mitigate reward over-optimization. Third, we further explore the training configurations of RFT to enhance the effectiveness. Extensive experiments show that GuirlVG, with only 5.2K training samples, outperforms SFT methods trained on over 10M samples, achieving a +7.7% improvement on ScreenSpot, a +17.2% improvement on ScreenSpotPro and 91.9% accuracy on ScreenSpotV2.</font>


<div style="text-align: center;">
<img src="figures\Grounding15.png"  width="60%" >
</div>

16.  **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">WebFactory: Automated Compression of Foundational Language Intelligence into Grounded Web Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Current paradigms for training GUI agents are fundamentally limited by a reliance on either unsafe, non-reproducible live web interactions or costly, scarce human-crafted data and environments. We argue this focus on data volume overlooks a more critical factor: the efficiency of compressing a large language model's (LLM) latent knowledge into actionable agent behavior. We introduce WebFactory, a novel, fully automated closed-loop reinforcement learning pipeline for GUI agents, systematically compressing LLM-encoded internet intelligence into efficient, grounded actions. Our pipeline features a process of scalable environment synthesis → knowledge-aware task generation → LLM-powered trajectory collection → decomposed reward RL training → systematic agent evaluation. Remarkably, our agent demonstrates exceptional data efficiency and generalization. Trained on synthetic data from only 10 websites within WebFactory, it achieves performance comparable to GUI agents trained on same amount of human-annotated data from a much larger set of environments. This superior performance is consistent across our internal offline and online transferring benchmarks, where our agent also significantly outperforms the base foundation model. We further provide critical insights into the "embodiment potential" of different LLM foundations, offering a new axis for model evaluation. This work presents a scalable and cost-effective paradigm for transforming passive internet knowledge into active, grounded intelligence, marking a critical step towards general-purpose interactive agents.</font>


<div style="text-align: center;">
<img src="figures\Grounding16.png"  width="60%" >
</div>

17.  **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">ReGUIDE: Data Efficient GUI Grounding via Spatial Reasoning and Search</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Recent advances in Multimodal Large Language Models (MLLMs) have enabled autonomous agents to interact with computers via Graphical User Interfaces (GUIs), where accurately localizing the coordinates of interface elements (e.g., buttons) is often required for fine-grained actions. However, this remains significantly challenging, leading prior works to rely on large-scale web datasets to improve the grounding accuracy. In this work, we propose Reasoning Graphical User Interface Grounding for Data Efficiency (ReGUIDE), a novel and effective framework for web grounding that enables MLLMs to learn data efficiently through self-generated reasoning and spatial-aware criticism. More specifically, ReGUIDE learns to (i) self-generate a language reasoning process for the localization via online reinforcement learning, and (ii) criticize the prediction using spatial priors that enforce equivariance under input transformations. At inference time, ReGUIDE further boosts performance through a test-time scaling strategy, which combines spatial search with coordinate aggregation. Our experiments demonstrate that ReGUIDE significantly advances web grounding performance across multiple benchmarks, outperforming baselines with substantially fewer training data points (e.g., only 0.2% samples compared to the best open-sourced baselines).</font>


<div style="text-align: center;">
<img src="figures\Grounding17.png"  width="60%" >
</div>

# **Navigation**
1. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">PAL-UI: Planning with Active Look-back for Vision-Based GUI Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical User Interface (GUI) agents powered by Multimodal Large Language Models (MLLMs) promise human-like interaction with software applications, yet long-horizon tasks remain challenging due to memory limitations. Existing approaches either truncate history or rely on simple textual summaries, which risk losing critical information when past visual details become necessary for future decisions. In this paper, we propose PAL-UI (Planning with Active Look-back), a novel framework that enables GUI agents to adaptively retrieve past observations when required. PAL-UI combines a dual-level summarization agent, capturing both observation-level cues and action-level outcomes, with a dedicated retrieval tool that allows the agent to recall specific historical screenshots during planning. We curate a step-level instruction dataset of 8.6K samples from mobile GUI navigation trajectories and train PAL-UI-3B and PAL-UI-7B models based on Qwen2.5-VL. Extensive experiments demonstrate that PAL-UI significantly outperforms baseline models and prior methods in mobile GUI navigation tasks, even under data-efficient settings. Moreover, PAL-UI exhibits strong cross-domain generalization, achieving notable improvements in web navigation without additional training. Our work highlights the potential of active memory retrieval for long-horizon planning capabilities of vision-based GUI agents.</font>


<div style="text-align: center;">
<img src="figures\Navigation1.png"  width="60%" >
</div>

2. **Auto-scaling Continuous Memory for GUI Agent**

**Abstract:** We study how to endow GUI agents with scalable memory that help generalize across unfamiliar interfaces and long-horizon tasks. Prior GUI agents compress past trajectories into text tokens, which balloons context length and misses decisive visual cues (eg. exact widget size and position). We propose a continuous memory that encodes each GUI trajectory into a fixed-length sequence of continuous embeddings using the VLM itself as an encoder; these embeddings are plugged directly into the backbone’s input layer, sharply reducing context cost while preserving fine-grained visual information. As memory size and retrieval depth increase, performance improves monotonically, unlike text memories that degrade with long prompts. To grow memory at low cost, we introduce an auto-scaling data flywheel that (i) discovers new environments via search, (ii) synthesizes tasks with an open-source VLM, (iii) rolls out trajectories with the agent, and (iv) verifies success with the same VLM. Using this pipeline, we collect 10k trajectories for about $500 and fine-tune only the memory encoder (LoRA on a Q-Former, 1.2% parameters) with 1,500 samples. On real-world GUI benchmarks, our memory-augmented agent consistently improves success rates under long horizons and distribution shifts. Notably, Qwen-2.5-VL-7B + continuous memory achieves performance comparable to state-of-the-art closed-source models (eg. GPT-4o, Claude-4). Our data and code will be publicly released.


<div style="text-align: center;">
<img src="figures\Navigation2.png"  width="60%" >
</div>

3. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI-PRA: Process Reward Agent for GUI Tasks</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical User Interface (GUI) Agents powered by Multimodal Large Language Models (MLLMs) show significant potential for automating tasks. However, they often struggle with long-horizon tasks, leading to frequent failures. Process Reward Models (PRMs) are a promising solution, as they can guide these agents with crucial process signals during inference. Nevertheless, their application to the GUI domain presents unique challenges. When processing dense artificial inputs with long history data, PRMs suffer from a "lost in the middle" phenomenon, where the overwhelming historical context compromises the evaluation of the current step. Furthermore, standard PRMs lacks GUI changing awareness, providing static evaluations that are disconnected from the dynamic consequences of actions, a critical mismatch with the inherently dynamic nature of GUI tasks. In response to these challenges, we introduce GUI-PRA (Process Reward Agent for GUI Tasks), a judge agent designed to better provide process reward than standard PRM by intelligently processing historical context and actively perceiving UI state changes. Specifically, to directly combat the ``lost in the middle'' phenomenon, we introduce a dynamic memory mechanism consisting of two core components: a Relevance-based Retrieval Module to actively fetch pertinent information from long histories and a Progressive Summarization Module to dynamically condense growing interaction data, ensuring the model focuses on relevant context. Moreover, to address the lack of UI changing awareness, we introduce an Aadaptive UI Perception mechanism. This mechanism enables the agent to reason about UI state changes and dynamically select the most appropriate tool to gather grounded visual evidence, ensuring its evaluation is always informed by the current UI context. To validate the practical utility of our approach, we conduct experiments on two online benchmarks for GUI task. Our best results demonstrate an average success rate improvement of 14.53% across the two benchmarks, a significant outperformance of the 8.56% gain achieved by the standard PRM baseline.</font>


<div style="text-align: center;">
<img src="figures\Navigation3.png"  width="60%" >
</div>

4. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Towards Adaptive GUI Agents with Memory-Driven Knowledge Evolution</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Mobile App Agents powered by large foundation models represent a transformative approach to human-computer interaction, enabling autonomous task execution within dynamic mobile applications. However, the volatile nature of mobile ecosystems characterized by frequent application updates poses challenges to agent reliability and long-term viability. We identify two critical problems: UI element identification failure when visual or structural changes occur, and task logic drift when fundamental workflows are altered. To address these challenges, we propose MAGNET, a Memory-driven Adaptive aGENT framework, equipped with a novel dual-level memory consisting of stationary memory and procedural memory. The stationary memory maintains rich multimodal representations of UI elements, enabling robust action grounding despite interface modifications, while the procedural memory captures and adapts structured task workflows to handle logical changes in operations. This framework effectively bridges the update gap that has limited the practical deployment of mobile agents. Comprehensive experiments demonstrate that \modelname achieves robust generalization across various in-domain scenarios and strong adaptability to novel task domains.</font>


<div style="text-align: center;">
<img src="figures\Navigation4.png"  width="60%" >
</div>

# **Benchmark**
1. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">MMBench-GUI: Hierarchical Multi-Platform Evaluation Framework for GUI Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">We introduce MMBench-GUI, a hierarchical benchmark for evaluating GUI automation agents across Windows, macOS, Linux, iOS, Android, and Web. The benchmark spans four levels: Content Understanding, Element Grounding, Task Automation, and Task Collaboration, covering essential skills for GUI agents. To assess both effectiveness and efficiency, we further propose the Efficiency–Quality-Aware (EQA) metric, which measures task success alongside action redundancy. Extensive evaluations reveal that precise visual grounding is the critical determinant of performance, underscoring the advantages of modular designs with specialized grounding modules. Moreover, all agents suffer from substantial inefficiencies, frequently completing tasks with excessive steps despite eventual success. Performance also degrades on complex or cross-application tasks, exposing weaknesses in memory, planning, and adaptive reasoning. By providing broad coverage, standardized protocols, and novel metrics, MMBench-GUI establishes the first comprehensive foundation for advancing GUI agent research.


<div style="text-align: center;">
<img src="figures\Bench1.png"  width="60%" >
</div>

2. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Beyond Clicking: A Step Towards Generalist GUI Grounding via Text Dragging</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract：</font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical user interface (GUI) grounding, the process of mapping human instructions to GUI actions, serves as a fundamental basis to autonomous GUI agents. While existing grounding models achieve promising performance to simulate the mouse click action on various click-based benchmark, another essential mode of mouse interaction, namely dragging, remains largely underexplored. Yet, dragging the mouse to select and manipulate textual content represents a prevelant and important usage in practical GUI scenarios. To narrow this gap, we first introduce GUIDrag, a diverse dataset of 161K text dragging examples synthesized through a scalable pipeline. To support systematic and robust evaluation, we further construct ScreenDrag, a benchmark with 5,333 examples spanning three levels of interface context, together with three dedicated metrics designed for assessing text dragging capability. Models trained on GUIDrag with an efficient continual training strategy achieve substantial improvements on ScreenDrag, while preserving the original click-based performance on ScreenSpot, ScreenSpot-v2, and OSWorld-G. Our work encourages further research on broader GUI grounding beyond just clicking and paves way toward a truly generalist GUI grounding model.</font>


<div style="text-align: center;">
<img src="figures\Bench2.png"  width="60%" >
</div>

3. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI-360° : A Comprehensive Dataset and Benchmark for Computer-Using Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">We introduce GUI-360°, a large-scale, comprehensive dataset and benchmark suite designed to advance computer-using agents (CUAs). CUAs present unique challenges and is constrained by three persistent gaps: a scarcity of real-world CUA tasks, the lack of automated collection-and-annotation pipelines for multi-modal trajectories, and the absence of a unified benchmark that jointly evaluates GUI grounding, screen parsing, and action prediction. GUI-360° addresses these gaps with a largely automated pipeline for query sourcing, environment-template construction, task instantiation, batched execution, and LLM-driven quality filtering. The released corpus contains over 1.2M executed action steps across thousands of trajectories in popular Windows office applications, and includes full-resolution screenshots, accessibility metadata when available, instantiated goals, intermediate reasoning traces, and both successful and failed action trajectories. The dataset supports three canonical tasks, GUI grounding, screen parsing, and action prediction, and a hybrid GUI+API action space that reflects modern agent designs. Benchmarking state-of-the-art vision--language models on GUI-360° reveals substantial out-of-the-box shortcomings in grounding and action prediction; supervised fine-tuning yield significant gains.</font>


<div style="text-align: center;">
<img src="figures\Bench3.png"  width="60%" >
</div>

4. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI Knowledge Bench: Revealing the Knowledge Gap Behind VLM Failures in GUI Tasks</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Large vision–language models (VLMs) have advanced graphical user interface (GUI) task automation but still lag behind humans. We hypothesize this gap stems from missing core GUI knowledge, which existing training schemes (such as supervised fine-tuning and reinforcement learning) alone cannot fully address. By analyzing common failure patterns in GUI task execution, we distill GUI knowledge into three dimensions: (1) interface perception, knowledge about recognizing widgets and system states; (2) interaction prediction, knowledge about reasoning action–state transitions; and (3) instruction understanding, knowledge about planning, verifying, and assessing task completion progress. We further introduce GUI Knowledge Bench, a benchmark with multiple choice and yes/no questions across six platforms (Web, Android, MacOS, Windows, Linux, iOS) and 292 applications. Our evaluation shows that current VLMs identify widget functions but struggle with perceiving system states, predicting actions, and verifying task completion. Experiments on real world GUI tasks further validate the close link between GUI knowledge and task success. By providing a structured framework for assessing GUI knowledge, our work supports the selection of VLMs with greater potential prior to downstream training and provides insights for building more capable GUI agents.</font>


<div style="text-align: center;">
<img src="figures\Bench4.png"  width="60%" >
</div>

5. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">A3: Android Agent Arena For Mobile GUI Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">The advancement of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) has catalyzed the development of autonomous AI agents. Mobile graphic user interface (GUI) agents, designed to perform tasks on mobile devices, represent a promising application of this technology. However, a significant gap persists in mobile GUI agent evaluation, where existing benchmarks predominantly rely on either static frame assessments such as AndroidControl or offline static apps such as AndroidWorld and thus fail to capture agent performance in dynamic, real-world online mobile applications. To address this gap, we present Android Agent Arena (A3), a novel evaluation system for mobile GUI agents. Unlike existing dynamic evaluation systems, A3 introduces a benchmark of 100 tasks derived from 20 widely-used, online apps across 20 distinct categories from the Google Play Store, ensuring evaluation comprehension. A3 also presents a novel "essential-state" based evaluation method that leverages MLLMs (either commercial or open-source models) as reward models to progressively verify task completion and process achievement. This automated evaluation approach significantly reduces the reliance on manual labor and coding expertise compared with traditional evaluation methods such as in AndroidWorld. Furthermore, A3 includes a toolkit and an evaluator to streamline Android device interaction and facilitate data collection from both human and agent demonstrations. The complete A3 system, including the benchmark and pipeline, will be publicly released to provide a robust foundation for future research and development in mobile GUI agents.</font>


<div style="text-align: center;">
<img src="figures\Bench5.png"  width="60%" >
</div>

6. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">WorldGUI: An Interactive Benchmark for Desktop GUI Automation from Any Starting Point</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI agents have achieved outstanding performance in GUI element grounding. However, planning remains highly challenging, especially due to the sensitivity to the initial state of the environment. Specifically, slight differences in the initial state-such as the target software not being open or the interface not being in its default state, often lead to planning errors. This issue is widespread in real application scenarios, but existing benchmarks fail to evaluate it. To address this gap, we introduce WorldGUI, a comprehensive GUI benchmark containing tasks across ten widely used desktop and web applications (e.g., PowerPoint, VSCode, Acrobat), each instantiated with diverse initial states to simulate authentic human–computer interactions. Complementing this, we propose GUI-Thinker, a universal framework that unifies three core modules: Planner-Critic for high-level plan refinement, Step-Check for intermediate verification, and Actor-Critic for action-level optimization to proactively detect and correct errors. Experimental evaluation shows that WorldGUI-Agent outperforms the outstanding existing model (Claude-3.5-Sonnet Claude Computer Use) by 12.4% in success rate on WorldGUI, and achieves a 31.2% overall success rate on WindowsAgentArena, surpassing the prior state-of-the-art by 11.7%. Our analysis further reveals that dynamic augmentation tasks and desktop environments pose substantial hurdles, underscoring the necessity of adaptive planning and feedback-driven execution for advancing real-world GUI automation.</font>


<div style="text-align: center;">
<img src="figures\Bench6.png"  width="60%" >
</div>

7. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Current mobile GUI agent benchmarks systematically fail to assess memory capabilities, with only 5.2-11.8% memory-related tasks and no cross-episode learning evaluation. We introduce MemGUI-Bench, the first comprehensive benchmark designed to assess both short-term and long-term memory in mobile GUI agents. Our contributions include: (1) a systematic memory taxonomy with analysis of 11 prominent agents; (2) 128 tasks across 26 applications where 89.8% challenge memory through cross-temporal and cross-spatial information retention; (3) MemGUI-Eval, an automated evaluation pipeline with novel "Progressive Scrutiny" and 8 hierarchical metrics for memory fidelity and learning effectiveness; and (4) comprehensive assessment revealing significant memory deficits across all evaluated agents. Our experiments expose 4-10× performance gaps between memory-intensive and standard tasks, demonstrate the potential of explicit long-term memory mechanisms, and identify 7 distinct failure modes through systematic analysis. MemGUI-Bench establishes crucial empirical baselines for developing more capable and human-like GUI agents. Code and results: </font>[https://anonymous.4open.science/r/MemGUI-Bench-Anonymous](https://anonymous.4open.science/r/MemGUI-Bench-Anonymous%7D)<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">.</font>


<div style="text-align: center;">
<img src="figures\Bench7.png"  width="60%" >
</div>

8. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Hijacking JARVIS: Benchmarking Mobile GUI Agents against Unprivileged Third Parties</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI agents are designed to autonomously execute diverse device-control tasks by interpreting and interacting with device screens. Despite notable advancements, their resilience in real-world scenarios—where screen content may be partially manipulated by untrustworthy third parties—remains largely unexplored. In this work, we present the first systematic investigation into the vulnerabilities of mobile GUI agents. We introduce a scalable attack simulation framework named AgentHazard, which enables flexible and targeted modifications of screen content within existing applications. Leveraging this framework, we develop a comprehensive benchmark suite comprising both a dynamic task execution environment and a static dataset of state-rule pairs. The dynamic environment encompasses 122 reproducible tasks in an emulator with various types of hazardous UI content, while the static dataset consists of over 3,000 attack scenarios constructed from screenshots collected from a wide range of commercial apps. Importantly, our content modifications are designed to be feasible for unprivileged third parties. We perform experiments on 6 widely-used mobile GUI agents and 5 common backbone models using our benchmark. Our findings reveal that all examined agents are significantly influenced by misleading third-party contents (with an average misleading rate of 42.1% and 40.7% in dynamic and static environments, respectively). We also find that the vulnerabilities are closely linked to the perception modalities and backbone LLMs.</font>


<div style="text-align: center;">
<img src="figures\Bench8.png"  width="60%" >
</div>

9. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Automotive-ENV: Benchmarking Multimodal Agents in Vehicle Interface Systems</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Multimodal agents have demonstrated strong performance in general GUI interactions, but their application in automotive systems has been largely unexplored. In-vehicle GUIs present distinct challenges: drivers' limited attention, strict safety requirements, and complex location-based interaction patterns. To address these challenges, we introduce Automotive-ENV, the first high-fidelity benchmark and interaction environment tailored for vehicle GUIs. This platform defines 185 parameterized tasks spanning explicit control, implicit intent understanding, and safety-aware tasks, and provides structured multimodal observations with precise programmatic checks for reproducible evaluation. Building on this benchmark, we propose ASURADA, a geo-aware multimodal agent that integrates GPS-informed context to dynamically adjust actions based on location, environmental conditions, and regional driving norms. Experiments show that geo-aware information significantly improves success on safety-aware tasks, highlighting the importance of location-based context in automotive environments. We will release Automotive-ENV, complete with all tasks and benchmarking tools, to further the development of safe and adaptive in-vehicle agents.</font>


<div style="text-align: center;">
<img src="figures\Bench9.png"  width="60%" >
</div>

10.   **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">FingerTip 20K: A Benchmark for Proactive and Personalized Mobile LLM Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Mobile GUI agents are becoming critical tools to improve user experience on smart devices, with multimodal large language models (MLLMs) emerging as the dominant paradigms in this domain. Current agents, however, rely on explicit human instructions, overlooking the potential to leverage the contextual information (like location, time, previous interactions) for proactive task suggestions. Besides, previous works focus on optimizing the success rate during task execution, but pay less attention to the personalized execution trajectory, thereby neglecting potentially vast differences in user preferences. To address these challenges, we introduce the FingerTip 20K benchmark. We collected 20K unique human demonstrations of multi-step Android device interactions across a variety of everyday apps. These demonstrations are not isolated but are continuously acquired from the users' long-term usage in their real lives, and encompass essential user-related contextual information. The benchmark contains two new tracks: proactive task suggestions by analyzing environment observation and users' previous intents, and personalized task execution by catering to users' action preferences. Our experiments reveal that the tracks we propose pose significant challenges for leveraging user-related information in GUI tasks. We also performed a human study to show that there exists a huge gap between existing agents and humans. The model fine-tuned with the data we collected effectively utilized user information and achieved good results, highlighting the potential of our approach in building more user-oriented mobile LLM agents. Our code is open-source at </font>[https://anonymous.4open.science/r/FingerTip-57B8](https://anonymous.4open.science/r/FingerTip-57B8%7D)<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);"> for reproducibility.</font>


<div style="text-align: center;">
<img src="figures\Bench10.png"  width="60%" >
</div>

11.   **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GAMBIT: A Graph-structured and Decision-Aware Benchmark for MoBile GUI Tasks</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Mobile GUI agents powered by LMMs can perceive screens and follow instructions, yet existing benchmarks largely target short, linear workflows and step-level accuracy, offering limited insight into long-horizon planning and branching tasks. We present GAMBIT, a graph-structured, decision-aware benchmark comprising 830 task episodes and 11,345 actions across 35 applications on Android and iOS. Tasks are organized into Sequential, Conjunctive, Conditional, and Hierarchical workflows with dual-level annotations, capturing realistic multi-step and branching scenarios. To move beyond step metrics, we introduce weighted longest common subsequence for length-sensitive progress and decision accuracy for branch correctness. Evaluations on 7 diverse agents show that GAMBIT induces a substantial accuracy drop compared to prior datasets, with success rates falling below 5% on 6–8 step tasks and branch accuracy averaging 38%, underscoring weaknesses in conditional reasoning. By systematically exposing these failure modes, GAMBIT provides a challenging, diagnostic testbed for advancing decision-aware mobile GUI agents. Our code and dataset are available at: </font>[https://anonymous.4open.science/r/GAMBIT-40BB/](https://anonymous.4open.science/r/GAMBIT-40BB/)<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">.</font>


<div style="text-align: center;">
<img src="figures\Bench11.png"  width="60%" >
</div>

12.   **PSBench: Editing Image via GUI Agents in Photoshop**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);"> Photoshop is a powerful and widely used professional software for image editing, design, and creative production. Its complex multi-level menu structure, extensive set of graphical processing tools, and reliance on precise manipulations make automated operation and agent interaction particularly challenging. Despite recent progress in GUI agents, existing datasets and methods are primarily designed for web-based interfaces and short-horizon, low-complexity tasks in operating systems, falling short in addressing the fine-grained control, multi-step decision-making, and semantic understanding required in professional graphic software. To this end, we propose the first benchmark specifically tailored for image editing in Adobe Photoshop environment, with a particular focus on its core principle of non-destructive editing through layers. The benchmark consists of 600 human-annotated tasks, spanning three difficulty levels. Easy and medium tasks are distilled from official Photoshop tutorials, capturing the most common basics. Hard tasks are directly collected from the most popular Photoshop tutorials in Youtube, ensuring both challenge and real-world relevance. Task categories cover fundamental functionalities such as canvas adjustment, layer manipulation, and filter application, each accompanied by dedicated fine-grained evaluation metrics. Through various experiments in PSBench, we find that current leading MLLMs, like Qwen2.5-VL, GPT-5 and Gemini-2.5-Pro, exhibit generally low task success rates but can demonstrate remarkable planning ability. Further via a human-in-loop experiment, we find that MLLMs can serve as highly effective Photoshop assistants, substantially boosting novice users’ task success rates while dramatically reducing their operation time.


<div style="text-align: center;">
<img src="figures\Bench12.png"  width="60%" >
</div>

# **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Pretraining</font>**
1. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">All in One: Unified Pretraining of GUI Agents via Masked Trajectory Prediction</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical User Interface (GUI) agents are intelligent systems that interact with software applications by perceiving visual elements and taking appropriate actions. Existing studies typically explore a wide range of pretraining strategies with heterogeneous corpora and directly unify these tasks through mixture training to enhance the generalization of GUI agents. However, the direct unification of existing pretraining strategies leads to inconsistent training objectives and data heterogeneity, preventing the full potential of each pretraining task from being realized. In this paper, we present a unified framework, Masked Trajectory Prediction (MTP), which consolidates diverse pretraining strategies into a consistent training objective via a masking-based manner. Specifically, we collect open-source GUI corpora that encompass a broad range of logical and semantic coherence, including randomly generated action–screenshot pairs, GUI tutorial data, and human-annotated datasets. Then, MTP models each GUI multi-interaction as a trajectory and defines pretraining objectives through component masking and prediction. Furthermore, to handle the heterogeneity across open-source corpora, we design a role-aware adapter learning module that dynamically routes each token to an appropriate optimization path. Extensive experiments on four representative GUI navigation benchmarks (AndroidControl, GUI-Odyssey, AITZ, and Mind2Web) demonstrate the effectiveness and generalization ability of our framework. By unifying existing pretraining objectives, MTP significantly outperforms prior methods and achieves SOTA results. The code and dataset will be publicly released.


<div style="text-align: center;">
<img src="figures\Pretraining.png"  width="60%" >
</div>

# **Multi-agent framework**
1. **VistaGUI: Towards More Robust and Intelligent GUI Automation**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">：The proliferation of Large Language Models (LLMs) and Vision-Language Models (VLMs) has driven the development of general-purpose agents for Graphical User Interface (GUI) automation.

Despite this progress, the practical application of these agents is hindered by their fragility, which stems from three primary limitations: low retrieval accuracy in retrieval-augmented generation (RAG), over-reliance on single-modality perception, and inadequate failure recovery mechanisms.

To address these challenges, we introduce VistaGUI, a robust, multi-modal GUI agent that integrates optimized retrieval, adaptive sensing, and environment-aware state management into a unified framework.

The core contributions of VistaGUI are threefold. First, a parallel instruction-understanding module enhances retrieval accuracy to better comprehend user intent, enabling more precise, context-aware decision-making.

Second, an adaptive multi-modal sensing module dynamically selects the optimal perception modality—including API-based queries, visual perception, and OCR—to achieve a comprehensive understanding of diverse GUI environments.

Third, an environment-aware state management system records and analyzes interaction trajectories to proactively detect and efficiently recover from execution failures, thereby reducing replanning overhead.

VistaGUI is implemented within a modular architecture comprising a Knowledge Manager, Planner, Action Executor, and History Context Recorder.

Extensive experiments conducted on a diverse set of GUI automation tasks demonstrate that VistaGUI significantly outperforms strong baselines in task success rate, recovery speed, and overall robustness.


<div style="text-align: center;">
<img src="figures\Multiagent1.png"  width="60%" >
</div>

2. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">D-Artemis: A Deliberative Cognitive Framework for Mobile GUI Multi-Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical User Interface (GUI) agents aim to automate a wide spectrum of human tasks by emulating user interaction. Despite rapid advancements, current approaches are hindered by several critical challenges: data bottleneck in end-to-end training, high cost of delayed error detection, and risk of contradictory guidance. Inspired by the human cognitive loop of Thinking, Alignment, and Reflection, we present D-Artemis—a novel deliberative framework in this paper. D-Artemis leverages a fine-grained, app-specific tip retrieval mechanism to inform its decision-making process. It also employs a proactive Pre-execution Alignment stage, where Thought-Action Consistency (TAC) Check module and Action Correction Agent (ACA) work in concert to mitigate the risk of execution failures. A post-execution Status Reflection Agent (SRA) completes the cognitive loop, enabling strategic learning from experience. Crucially, D-Artemis enhances the capabilities of general-purpose Multimodal large language models (MLLMs) for GUI tasks without the need for training on complex trajectory datasets, demonstrating strong generalization. D-Artemis establishes new state-of-the-art (SOTA) results across both major benchmarks, achieving a 75.8% success rate on AndroidWorld and 96.8% on ScreenSpot-V2. Extensive ablation studies further demonstrate the significant contribution of each component to the framework.</font>


<div style="text-align: center;">
<img src="figures\Multiagent2.png"  width="60%" >
</div>

3. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">UltraCUA: Scaling Computer Use Agent through GUI and Programmatic Control</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Multi-modal agents for computer use rely exclusively on primitive actions (click, type, scroll) that require accurate visual grounding and lengthy execution chains, leading to cascading failures and performance bottlenecks. While other agents leverage rich programmatic interfaces (APIs, MCP servers, tools), computer-use agents (CUAs) remain isolated from these capabilities. We present UltraCUA, a foundation model that bridges this gap through hybrid control—seamlessly integrating GUI primitives with high-level programmatic tool calls. To achieve this, our approach comprises four key components: (1) an automated pipeline that scales programmatic tools from software documentation, open-source repositories, and code generation; (2) a synthetic data engine producing 17,000+ verifiable tasks spanning real-world computer-use scenarios; (3) a multi-agent system generating high-quality hybrid control trajectories with both low-level GUI actions and high-level programmatic tool calls; and (4) a two-stage training pipeline combining supervised fine-tuning with online reinforcement learning, enabling strategic alternation between low-level and high-level actions. Experiments with our 7B and 32B models demonstrate substantial improvements over state-of-the-art agents. On OSWorld, UltraCUA models achieve an average 27% relative improvement over base models, while being 11% faster in terms of steps. Out-of-domain evaluation on WindowsAgentArena shows our model reaches 21.7% success rate, outperforming baselines trained on Windows data. The hybrid control mechanism proves critical, reducing error propagation while maintaining execution efficiency. This work establishes a scalable paradigm that bridges primitive GUI interactions and programmatic intelligence for stronger and unified computer use.</font>


<div style="text-align: center;">
<img src="figures\Multiagent3.png"  width="60%" >
</div>

4. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">ProRe: A Proactive Reward System for GUI Agents via Reasoner–Actor Collaboration</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Reward is critical to the evaluation and training of large language models (LLMs). However, existing rule-based or model-based reward methods struggle to generalize to GUI agents, where access to ground-truth trajectories or application databases is often unavailable, and static trajectory-based LLM-as-a-Judge approaches suffer from limited accuracy. To address these challenges, we propose ProRe, a proactive reward system that leverages a general-purpose reasoner and domain-specific evaluator agents (actors). The reasoner schedules targeted state probing tasks, which the evaluator agents then execute by actively interacting with the environment to collect additional observations. This enables the reasoner to assign more accurate and verifiable rewards to GUI agents. Empirical results on over 3K trajectories demonstrate that ProRe improves reward accuracy and F1 score by up to 5.3% and 19.4%, respectively. Furthermore, integrating ProRe with state-of-the-art policy agents yields a success rate improvement of up to 22.4%.</font>


<div style="text-align: center;">
<img src="figures\Multiagent4.png"  width="60%" >
</div>

5. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Computer-Use Agents as Judges for Automatic GUI Design</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Computer-Use Agents (CUA) are becoming increasingly capable of autonomously operating digital environments through Graphical User Interfaces (GUI). Yet, most GUI remain designed primarily for humans—prioritizing aesthetics and usability—forcing agents to adopt human-oriented behaviors that are unnecessary for efficient task execution. At the same time, rapid advances in coding-oriented language models (Coder) have transformed automatic GUI design. This raises a fundamental question: Can CUA as judges to assist Coder for automatic GUI design? To investigate, we introduce bench, a benchmark for Automatic GUI development spanning 52 applications across diverse domains. Using language models, we synthesis 1560 tasks that simulate real-world scenarios. To ensure task reliability, we further develop a verifier that programmatically checks whether each task is executable within its environment. Building on this, we propose a Coder–CUA in Collaboration framework: the Coder acts as Designer, generating and revising websites, while the CUA serves as Judge, evaluating functionality and refining designs. Success is measured not by visual appearance, but by task solvability and CUA navigation success rate. To turn CUA feedback into usable guidance, we design an CUA Dashboard that compresses multi-step navigation histories into concise visual summaries, offering interpretable guidance for iterative redesign. By positioning agents as both designers and judges, our framework shifts interface design toward agent-native efficiency and reliability. Our work takes a step toward shifting agents from passive use toward active participation in digital environments.</font>


<div style="text-align: center;">
<img src="figures\Multiagent5.png"  width="60%" >
</div>

6. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">M2-Miner: Multi-Agent Enhanced MCTS for Mobile GUI Agent Data Mining</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical User Interface (GUI) agent is pivotal to advancing intelligent human-computer interaction paradigms. Constructing powerful GUI agents necessitates the large-scale annotation of high-quality user-behavior trajectory data (i.e., intent–trajectory pairs) for training. However, manual annotation methods and current GUI agent data mining approaches typically face three critical challenges: high construction cost, poor data quality, and low data richness. To address these issues, we propose M-Miner, the first low-cost and automated mobile GUI agent data-mining framework based on Monte Carlo Tree Search (MCTS). For better data mining efficiency and quality, we present a collaborative multi-agent framework, comprising InferAgent, OrchestraAgent, and JudgeAgent for guidance, acceleration, and evaluation. To further enhance the efficiency of mining and enrich intent diversity, we design an intent recycling strategy to extract extra valuable interaction trajectories. Additionally, a progressive model-in-the-loop training strategy is introduced to improve the success rate of data mining. Extensive experiments have demonstrated that the GUI agent fine-tuned using our mined data achieves state-of-the-art performance on several commonly used mobile GUI benchmarks. Our work will be released to facilitate the community research.</font>


<div style="text-align: center;">
<img src="figures\Multiagent6.png"  width="60%" >
</div>

7. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GAIR : GUI Automation via Information-Joint Reasoning and Group Reflection</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Building AI systems for GUI automation task has attracted remarkable research efforts, where MLLMs are leveraged for processing user requirements and give operations. However, GUI automation includes a wide range of tasks, from document processing to online shopping, from CAD to video editing. Diversity between particular tasks requires MLLMs for GUI automation to have heterogeneous capabilities and master multidimensional expertise, raising problems on constructing such a model. To address such challenge, we propose GAIR : GUI Automation via Information-Joint Reasoning and Group Reflection, a novel MLLM-based GUI automation agent framework designed for integrating knowledge and combining capabilities from heterogeneous models to build GUI automation agent systems with higher performance. Since different GUI-specific MLLMs are trained on different dataset and thus have different strengths, GAIR introduced a general-purpose MLLM for jointly processing the information from multiple GUI-specific models, further enhancing performance of the agent framework. The general-purpose MLLM also serves as decision maker, trying to execute a reasonable operation based on previously gathered information. When the general-purpose model thinks that there isn't sufficient information for a reasonable decision, GAIR would transit into group reflection status, where the general-purpose model would provide GUI-specific models with different instructions and hints based on their strengths and weaknesses, driving them to gather information with more significance and accuracy that can support deeper reasoning and decision. We evaluated the effectiveness and reliability of GAIR through extensive experiments on GUI benchmarks.</font>


<div style="text-align: center;">
<img src="figures\Multiagent7.png"  width="60%" >
</div>

8. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">LongHorizonUI: A Unified Framework for Robust long-horizon Task Automation of GUI Agent</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Although agents based on multimodal large language models (MLLMs) demonstrate proficiency in general short-term graphical user interface (GUI) tasks, their robustness remains a significant challenge for handling complex long-horizon tasks in dynamic environments . In response, the LongHorizonUI framework is proposed to improve the sustained reliability of agents in long-horizon GUI tasks. To overcome core limitations, we establish a comprehensive long-horizon benchmark, LongGUIBench, covering multiple categories of games and complex general applications, with long-horizon tasks defined as requiring more than 15 steps for rigorous evaluation of long-horizon reasoning capabilities. Based on this, a Multimodal Enhanced Perceiver is designed to incorporate element detection and text recognition models, assigning unique indices to interface elements, thereby reinforcing state representation. Furthermore, a Deep Reflection Decider engine is introduced, incorporating a structured multi-level feedback validation mechanism to enable progressive reasoning and ensure accurate action execution with predictable trajectories. Finally, we introduce a Compensatory Action Executor that combines multiple degradation compensation operations with a process rollback strategy based on execution progress monitoring to ensure operational effectiveness in long-horizon task logic. Experimental results demonstrate that LongHorizonUI achieves substantial long-horizon modeling improvements on LongGUIBench while retaining competitive performance on diverse public benchmarks. The code and models will be publicly available.</font>


<div style="text-align: center;">
<img src="figures\Multiagent8.png"  width="60%" >
</div>

# **World model**
1. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">ViMo: A Generative Visual GUI World Model for App Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">App agents, which autonomously operate mobile Apps through GUIs, have gained significant interest in real-world applications. Yet, they often struggle with long-horizon planning, failing to find the optimal actions for complex tasks with longer steps. To address this, world models are used to predict the next GUI observation based on user actions, enabling more effective agent planning. However, existing world models primarily focus on generating only textual descriptions, lacking essential visual details. To fill this gap, we propose ViMo, the first Visual world Model designed to generate future App observations as images. For the challenge of generating text in image patches, where even minor pixel errors can distort readability, we decompose GUI generation into graphic and text content generation. We propose a novel data representation, the Symbolic Text Representation (STR), to overlay text content with symbolic placeholders while preserving graphics. With this design, ViMo employs a STR Predictor to predict future GUIs’ graphics and a GUI-text Predictor for generating the corresponding text. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the outcome of actions. Experiments show that ViMo establishes visual world models as a compelling alternative to language-based approaches, producing visually plausible and functionally effective GUIs that empower App agents with more informed decisions.</font>


<div style="text-align: center;">
<img src="figures\Worldmodel1.png"  width="60%" >
</div>

2. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Training a Vision-Language Model for Diverse Exploration in Open GUI World</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Vision-language models have emerged as capable computer-use agents, showing increasing potential to automate a wide range of computer tasks through graphical user interfaces. However, their effectiveness remains bounded by a fundamental limitation: current LLM- or VLM-based agents struggle to generalize to unfamiliar applications and remain heavily dependent on large-scale, human-curated datasets. To address this, we introduce ScreenExplorer, a novel VLM-based agent designed for autonomous exploration in real, dynamic, open-ended GUI environments. Through end-to-end training with an exploration-driven objective, our approach enables sustained interaction and diverse discovery without relying on predefined task structures. Specifically, we introduce a world model-inspired curiosity reward that helps the agent to overcome the cold-start phase of exploration, coupled with state-change-based exploration rewards to encourage agent's intrinsic motivation for venturing into novel states. Additionally, an experience stream distillation mechanism is designed to systematically accumulate and refine exploratory policies, enabling continual learning from gathered experiences. Extensive evaluations demonstrate that ScreenExplorer achieves remarkable generalization and diverse exploration capabilities in unseen applications, significantly outperforming static deployment baselines. This work establishes a new paradigm for GUI agents to progressively learn through autonomous exploration, moving beyond static dataset dependency toward adaptive, lifelong learning in complex digital worlds.</font>


<div style="text-align: center;">
<img src="figures\Worldmodel2.png"  width="60%" >
</div>

# **Knowledge/Demonstration data**
1. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">SCREEN-SBERT: EMBEDDING FUNCTIONAL SEMANTICS OF GUI SCREENS TO SUPPORT GUI AGENTS</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Recent GUI agent studies show that augmenting LLM prompts with app-related knowledge constructed during a pre-exploration phase can effectively improve task success rates. However, retrieving relevant knowledge from the knowledge base remains a key challenge. Existing approaches often rely on structured metadata such as view hierarchies, which are frequently unavailable or outdated, thereby limiting their generalizability. Purely vision-based methods have emerged to address this issue, yet they typically compare GUI elements only by visual appearance, leading to mismatches between functionally different elements. We consider a two-stage retrieval framework, where the first stage retrieves screenshots sharing the same functional semantics, followed by fine-grained element-level retrieval. This paper focuses on the first stage by proposing Screen-SBERT, a purely vision-based method for embedding the functional semantics of GUI screenshots and retrieving functionally equivalent ones within the same mobile app. Experimental results on real-world mobile apps show that Screen-SBERT is more effective than several baselines for retrieving functionally equivalent screenshots. As a result, (1) we formally define the concepts of functional equivalence and functional page class; (2) design a contrastive learning-based embedding framework; and (3) conduct ablation studies that provide insights for future model design.</font>


<div style="text-align: center;">
<img src="figures\Knowledge1.png"  width="60%" >
</div>

2. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI-KV: Efficient GUI Agents via KV Cache with Spatio-Temporal Awareness</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical user interface (GUI) agents built on vision–language models have emerged as a promising approach to automate human-computer workflows. However, they also face the inefficiency challenge as they process long sequences of high-resolution screenshots and solving long-horizon tasks, making inference slow, costly and memory-bound. While key-value (KV) caching can mitigate this, storing the full cache is prohibitive for image-heavy contexts. Existing cache-compression methods are sub-optimal as they do not account for the spatial and temporal redundancy of GUIs. In this work, we first analyze attention patterns in GUI agent workloads and find that, unlike in natural images, attention sparsity is uniformly high across all transformer layers. This insight motivates a simple uniform budget allocation strategy, which we show empirically outperforms more complex layer-varying schemes. Building on this, we introduce GUI-KV, a plug-and-play KV cache compression method for GUI agents that requires no retraining. GUI-KV combines two novel techniques: (i) spatial saliency guidance, which augments attention scores with the L2 norm of hidden states to better preserve semantically important visual tokens, and (ii) temporal redundancy scoring, which projects previous frames’ keys onto the current frame’s key subspace to preferentially prune redundant history. Across standard GUI agent benchmarks and models, GUI-KV outperforms competitive KV compression baselines, closely matching full-cache accuracy at modest budgets. Notably, in a 5-screenshot setting on the AgentNetBench benchmark, GUI-KV reduces decoding FLOPs by 38.9% while increasing step accuracy by 4.1% over the full-cache baseline. These results demonstrate that exploiting GUI-specific redundancies enables efficient and reliable agent performance.</font>

3. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Instruction Agent: Enhancing Agent with Expert Demonstration</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical user interface (GUI) agents have advanced rapidly but still struggle with complex tasks involving novel UI elements, long-horizon actions, and personalized trajectories. In this work, we introduce Instruction Agent, a GUI agent that leverages expert demonstrations to solve such tasks, enabling completion of otherwise difficult workflows. Given a single demonstration, the agent extracts step-by-step instructions and executes them by strictly following the trajectory intended by the user, which avoids making mistakes during execution. The agent leverages the verifier and backtracker modules further to improve robustness. Both modules are critical to understand the current outcome from each action and handle unexpected interruptions(such as pop-up windows) during execution. Our experiments show that Instruction Agent achieves a 60% success rate on a set of tasks in OSWorld that all top-ranked agents failed to complete. The Instruction Agent offers a practical and extensible framework, bridging the gap between current GUI agents and reliable real-world GUI task automation.</font>


<div style="text-align: center;">
<img src="figures\Knowledge3.png"  width="60%" >
</div>

4. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">DKRF: Dynamic Knowledge Reasoning for Out-of-Distribution Generalization in Mobile GUI Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical User Interface (GUI) agents demonstrate significant potential in cross-application tasks, yet their performance often drops sharply when facing out-of-distribution (OOD) scenarios (e.g., unseen task, different layout, etc.) in the open world. Previous methods, modular agent frameworks and end-to-end native agents, are designed based on in-distribution (ID) mobile data, whether through manual designed modules or specially collected training sets, while neglecting the adaptability to diverse data in potential OOD mobile scenarios. To overcome these limitations, we propose Dynamic Knowledge Reasoning Fine-tune (DKRF), a paradigm that shifts the agent's core capability from memorizing ID patterns to reasoning dynamically with external knowledge. During training, the model explicitly receives dynamic knowledge (e.g., trajectories of similar tasks or reusable meta-functions) and need to incorporate this knowledge in its reasoning chain, thereby learning to make knowledge-driven decisions. Based on DKRF, 1) we train an end-to-end native agent, DKR-GUI, and 2) further propose a modular agent framework, MA-DKR, which uses DKR-GUI as the planning core combined with knowledge retrieval and an executing agent to achieve collaboration between complex reasoning and precise execution. Experiments on multiple mobile benchmarks show that both DKR-GUI and MA-DKR significantly outperform existing methods, achieving an average 9.2% improvement in success rate in OOD mobile scenarios while also maintaining state-of-the-art performance in ID mobile tasks. Our results demonstrate that dynamic knowledge reasoning provides a general and effective solution for OOD generalization, highlighting its potential as a foundation for robust, knowledge-driven interactive agents.</font>


<div style="text-align: center;">
<img src="figures\Knowledge4.png"  width="60%" >
</div>

5. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">PrecogUI: Proactive GUI Agents via Pre-cognitive Simulation and Experience Retrieval</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Existing reactive Graphical User Interface (GUI) agents often fail in long-horizon, dynamic scenarios where unexpected disturbances trigger attention hijacking and cascading failures. To address this, we propose Precog-UI, an agent based on a pre-cognitive architecture that shifts the paradigm from reactive execution to proactive decision-making. Specifically, we first design a Proactive Experience Pool (PEP) which caches frequently occurring anomaly and success patterns as "state-action-result" tuples within a graph structure, forming a composable prior memory. Furthermore, we introduce a Proactive Modelling Executor (PME) that learns a predictive foresight model to forecast the next symbolic UI layout following a candidate action, enabling the avoidance of potential anomalies and the evaluation of policy success rates. Finally, a Pre-cognitive Execution Controller (PEC) fuses these priors and predictions, prioritises handling of foreseen anomalies, and ensures execution robustness through a closed-loop error correction mechanism. For robust evaluation, we developed an automatic engine, AutoTraj, to construct InterfereBench, a benchmark for long-horizon tasks with strong disturbances. Experiments demonstrate that PRECOG-UI surpasses existing state-of-the-art methods on InterfereBench while maintaining competitive performance on public benchmarks. The code and models will be publicly available.</font>


<div style="text-align: center;">
<img src="figures\Knowledge5.png"  width="60%" >
</div>

# **RL**
1. **Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);"> Developing autonomous agents that effectively interact with Graphic User Interfaces (GUIs) remains a challenging open problem, especially for small on-device models. In this paper, we present FERRET-UI LITE, a compact, end-to-end GUI agent that operates across diverse platforms, including mobile, web, and desktop. Utilizing techniques optimized for developing small models, we build our 3B FERRET-UI LITE agent through curating a diverse GUI data mixture from real and synthetic sources, strengthening inference-time performance through chain-of-thought reasoning and visual tool-use, and reinforcement learning with designed rewards. FERRET-UI LITE achieves competitive performance with other small-scale GUI agents. In GUI grounding, FERRET-UI LITE attains scores of 91.6%, 53.3%, and 61.2% on the ScreenSpot-V2, ScreenSpot-Pro, and OSWorld-G benchmarks, respectively. For GUI navigation, FERRET-UI LITE achieves success rates of 28.0% on AndroidWorld and 19.8% on OSWorld. We share our methods and lessons learned from developing compact, on-device GUI agents.


<div style="text-align: center;">
<img src="figures\RL1.png"  width="60%" >
</div>

2. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Recently, there has been a surge of vision-based GUI agents designed to automate everyday mobile and web tasks. These agents interpret raw GUI screenshots and autonomously decide where to click, scroll, or type, which bypasses handcrafted rules and app-specific APIs. However, most existing methods trained GUI agent in the offline environment using pre-collected trajectories. This approach limits scalability, causes overfitting to specific UI templates, and leads to brittle policies when faced with unseen environment. We present MobileGUI-RL, a scalable framework that trains GUI agent in online environment. MobileGUI-RL contains two key components. It (i) synthesizes a curriculum of learnable tasks through self-exploration and filtering, and (ii) adapts GRPO to GUI navigation with trajectory-aware advantages and composite rewards that balance task success and execution efficiency. Experiments on three online mobile-agent benchmarks show consistent gains, validating the effectiveness of our approach.</font>


<div style="text-align: center;">
<img src="figures\RL2.png"  width="60%" >
</div>

3. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Difficulty-Aware Reasoning for Mobile GUI Automation via Reinforcement Fine-Tuning</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Automating GUI tasks remains challenging due to layout complexity, element density, and intent ambiguity, which requires effective and efficient reasoning to facilitate each operation. Existing agents typically employ a uniform chain-of-thought (CoT) reasoning process for all actions, a one-size-fits-all approach that incurs unnecessary computational overhead and even performance degradation on trivial steps. To address this, we introduce AdaGUI-R1, a GUI agent that pioneers a difficulty-aware reasoning paradigm by dynamically modulating its reasoning depth based on action complexity. Our methodology consists of reasoning inducing and reasoning enhancing. During reasoning inducing, we introduce a self-supervised mechanism to generate high-quality, difficulty-aware reasoning trajectories. Fine-tuning on this curated data endows the agent with the fundamental capability to adjust its reasoning depth according to action complexity. Subsequently, Group Adaptive Policy Optimization (GAPO) algorithm is implemented to enhance reasoning performance. It leverages an adaptive thought reward to encourage thinking on challenging steps, and a novel exploration reward with a difficulty-aware Gaussian bandwidth to improve action accuracy.Extensive experiments demonstrate that AdaGUI-R1 sets a new state-of-the-art. It concurrently reduces unnecessary reasoning tokens by 40% while improving action accuracy by 5%, underscoring the power of adaptive reasoning in GUI automation.</font>


<div style="text-align: center;">
<img src="figures\RL3.png"  width="60%" >
</div>

4. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Efficient Multi-turn RL for GUI Agents via Decoupled Training and Adaptive Data Curation</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Vision-language model (VLM) based GUI agents show promise for automating complex desktop and mobile tasks, but face significant challenges in applying reinforcement learning (RL): (1) slow multi-turn interactions with GUI environments for policy rollout, and (2) insufficient high-quality agent-environment interactions for policy learning. To address these challenges, we propose DART, a Decoupled Agentic RL Training framework for GUI agents, which coordinates heterogeneous modules in a highly decoupled manner. DART separates the training system into four asynchronous modules: environment cluster, rollout service, data manager, and trainer. This design enables non-blocking communication, asynchronous training, rollout-wise trajectory sampling, and per-worker model synchronization, significantly improving the system efficiency: 1.6× GPU utilization for rollout, 1.9× training throughput, and 5.5× environment utilization. To facilitate effective learning from abundant samples, we introduce an adaptive data curation scheme: (1) pre-collecting successful trajectories for challenging tasks to supplement sparse success in online sampling; (2) dynamically adjusting rollout numbers and trajectory lengths based on task difficulty; (3) training selectively on high-entropy steps to prioritize critical decisions; (4) stabilizing learning via truncated importance sampling for policy mismatch between policy rollout and updating. On the OSWorld benchmark, DART-GUI-7B achieves a 42.13% task success rate, a 14.61% absolute gain over the base model, and 7.34% higher than open-source SOTA. To ensure reproducibility, we will fully open-source our training framework, data, and model checkpoints via </font>[https://iclr26-dart-gui.github.io](https://iclr26-dart-gui.github.io/)<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);"> (anonymous), which we believe is a timely contribution to the open-source community of agentic RL training.</font>


<div style="text-align: center;">
<img src="figures\RL4.png"  width="60%" >
</div>

5. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI-Shepherd: Reliable Process Reward and Verification for Long-Sequence GUI Tasks</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Autonomous agents for long-sequence Graphical User Interface tasks are hindered by sparse rewards and the intractable credit assignment problem. To address these challenges, we introduce GUI-Shepherd, a Process Reward Model that provides dense, step-by-step feedback to guide agents. GUI-Shepherd is trained on a diverse large-scale data set of 52k interactions that features human-annotated scores and GPT-4o generated rationales, enabling it to serve both as a reward provider for RL training and as a verifier for inference. As far as we know, we are the first to conduct a systematic study of process supervision in GUI agents, across diverse settings from online long-horizon tasks to offline single-step prediction. On the online AndroidWorld benchmark, GUI-Shepherd improves success rate by 7.7 points via multi-turn online PPO, significantly outperforming Outcome Reward Model based competitors. When used as an inference verifier, it brings 5.1 points improvements. The benefits generalize to the offline AndroidControl benchmark, with gains of 2.2 points as a reward provider and 4.3 points as a verifier. Collectively, our results establish that high-fidelity process supervision is critical for building more capable GUI agents and present a generalizable solution.</font>


<div style="text-align: center;">
<img src="figures\RL5.png"  width="60%" >
</div>

6. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Multimodal Large Language Models (MLLMs) have shown significant promise in powering Graphical User Interface (GUI) agents to automate complex digital tasks. However, the prevailing monolithic training paradigms often create a structural mismatch with the hierarchical nature of capabilities required for robust performance. Specifically, the efficacy of methods like Reinforcement Learning (RL) is critically predicated on the agent possessing a high-quality behavioral prior of key reasoning skills, such as spatial reasoning and goal decomposition, which are often absent. To resolve this impasse, we propose Actor2Reasoner, a novel two-stage hierarchical training paradigm grounded in the principle of Endow First, Internalize Later. The first stage, Cognitive Endowment, employs targeted supervised fine-tuning to instill these crucial thinking patterns, forging a Capable Actor. Subsequently, the second stage, Policy Internalization, utilizes RL to evolve this actor into a Deliberative Reasoner by internalizing the endowed abilities into a robust, context-aware decision-making policy. We instantiate our paradigm in InfiGUI-R1, an agent that achieves state-of-the-art performance on challenging benchmarks, including AndroidControl. Our work demonstrates that decoupling the endowment of foundational abilities from the internalization of policy provides a more effective and principled path toward developing sophisticated and resilient GUI agents.</font>


<div style="text-align: center;">
<img src="figures\RL6.png"  width="60%" >
</div>

7. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI-Shift: Enhancing VLM-Based GUI Agents through Self-supervised Reinforcement Learning</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Training effective Vision-Language Models (VLMs) for GUI agents typically depends on large-scale annotated datasets, whose collection is both labor-intensive and error-prone. We introduce K-step GUI Transition, a self-supervised inverse dynamics task in which VLMs learn GUI dynamics by predicting the initial action that causes a transition between two GUI states. This approach eliminates the need for natural language instructions and enables scalable dataset construction from existing GUI trajectories or automated exploration. Building on this task, we propose GUI-Shift, a reinforcement learning (RL) framework that combines rule-based optimization with data filtering to improve VLM performance. We conduct extensive experiments using multiple VLM backbones across four benchmarks, spanning GUI task automation (AndroidControl, GUI Odyssey) and GUI grounding (ScreenSpot-v2, ScreenSpot-Pro). Our results show that training on GUI-Shift generalizes well to both GUI automation and grounding tasks, yielding up to an 11.2% increase in GUI automation accuracy. This study underscores the potential of self-supervised RL to leverage unlabeled GUI trajectories and offers a scalable alternative to training with annotated samples.</font>


<div style="text-align: center;">
<img src="figures\RL7.png"  width="60%" >
</div>

8. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">MobileWizard: A Data-Efficient GUI Agent with Structured Reasoning and Progressive Reinforcement Learning</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">This paper introduces MobileWizard, a data-efficient framework designed to enhance the reasoning and precision of mobile GUI agents. Trained on merely 24.5k public trajectories and 300 remedial trajectories, MobileWizard-7B demonstrates exceptional performance, achieving a 47.2% success rate on AndroidWorld, outperforming prominent larger open-source models like UI-TARS-72B. This high efficiency stems from two core innovations: 1) Structured Reasoning: A new structured Chain-of-Thought (CoT) paradigm that decomposes the agent’s reasoning process into four explicit and interpretable modules: self-verification, screen analysis, planning, and action guidance. The proposed CoT guides the LLM to achieve logical consistency, extraction of key insights, and provides clear paths for failure analysis. 2) Progressive Reinforcement Learning: We propose a comprehensive RL strategy that features four key components: efficient cold-start training, a dynamic reward system with Progressive Reward Shrinking to boost precision, History Self-Alignment to narrow the training-inference gap, and a Corrective Teaching Pipeline for self-improvement from online failures. The experimental results demonstrate that our framework enables superior generalization from limited data. We believe that our method presents a scalable and efficient path toward building more robust and versatile GUI agents.</font>


<div style="text-align: center;">
<img src="figures\RL8.png"  width="60%" >
</div>

9. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">LightAgent: Lightweight and Cost-Efficient Mobile Agents</font>**

**<font style="color:rgb(44, 58, 74);">Abstract</font>**<font style="color:rgb(44, 58, 74);">: </font><font style="color:rgb(44, 58, 74);">With the advancement of multimodal large language models (MLLMs), building GUI agent systems has become an increasingly promising direction—especially for mobile platforms, given their rich app ecosystems and intuitive touch interactions. Yet mobile GUI agents face a critical dilemma: truly on-device models (4B or smaller) lack sufficient performance, while capable models (starting from 7B) are either too large for mobile deployment or prohibitively costly (e.g., cloud-only closed-source MLLMs). To resolve this, we propose LightAgent, a mobile GUI agent system that leverages device-cloud collaboration to tap the cost-efficiency of on-device models and the high capability of cloud models, while avoiding their drawbacks. Specifically, LightAgent enhances Qwen2.5-VL-3B via two-stage SFT→GRPO training on synthetic GUI data for strong decision-making, integrates an efficient long-reasoning mechanism to utilize historical interactions under tight resources, and defaults to on-device execution—only escalating challenging subtasks to the cloud via real-time complexity assessment. Experiments on the online AndroidLab benchmark and diverse apps show LightAgent matches or nears larger models, with a significant reduction in cloud costs.</font>


<div style="text-align: center;">
<img src="figures\RL9.png"  width="60%" >
</div>

10.   **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">MobileRL: Online Agentic Reinforcement Learning for Mobile GUI Agents</font>**

**<font style="color:rgb(44, 58, 74);">Abstract</font>**<font style="color:rgb(44, 58, 74);">: </font><font style="color:rgb(44, 58, 74);">Building general-purpose graphical user interface (GUI) agents has become increasingly promising with the progress in vision language models. However, developing effective mobile GUI agents with reinforcement learning (RL) remains challenging due to the heavy-tailed distribution of task difficulty and the inefficiency of large-scale environment sampling. We present an online agentic reinforcement learning framework MOBILERL to enhance GUI agents in mobile environments. Its core component is the Difficulty-Adaptive GRPO (ADAGRPO) algorithm. In ADAGRPO, we design difficulty-adaptive positive replay and failure curriculum filtering to adapt the model to different task difficulties. We introduce the shortest-path reward adjustment strategy to reshape rewards concerning the task length in multi-turn agentic tasks. Those strategies jointly stabilize RL training, improve sample efficiency, and generate strong performance across diverse mobile apps and tasks. We apply MOBILERL to two open models (Qwen2.5-VL-7B-Instruct and GLM-4.1V-9B-Base). The resultant MOBILERL-9B model achieves state-of-the-art results in terms of success rates on both AndroidWorld (60.2%) and Android-Lab (53.6%). The MOBILERL framework is open-sourced at </font>[https://anonymous.4open.science/r/MobileRL-iclr-4513](https://anonymous.4open.science/r/MobileRL-iclr-4513)<font style="color:rgb(44, 58, 74);">.</font>


<div style="text-align: center;">
<img src="figures\RL10.png"  width="60%" >
</div>

11.   **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">SWIRL: A Staged Workflow for Interleaved Reinforcement Learning in Mobile GUI Control</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">The rapid advancement of large vision language models (LVLMs) and agent systems has heightened interest in mobile GUI agents that can reliably translate natural language into interface operations. Existing single-agent approaches, however, remain limited by structural constraints. Although multi-agent systems naturally decouple different competencies, recent progress in multi-agent reinforcement learning (MARL) has often been hindered by inefficiency and remains incompatible with current LVLM architectures. To address these challenges, we introduce SWIRL, a staged workflow for interleaved reinforcement learning designed for multi-agent systems. SWIRL reformulates MARL into a sequence of single-agent reinforcement learning tasks, updating one agent at a time while keeping the others fixed. This formulation enables stable training and promotes efficient coordination across agents. Theoretically, we provide a stepwise safety bound, a cross-round monotonic improvement theorem, and convergence guarantees on return, ensuring robust and principled optimization. In application to mobile GUI control, SWIRL instantiates a Navigator that converts language and screen context into structured plans, and an Interactor that grounds these plans into executable atomic actions. Extensive experiments demonstrate superior performance on both high-level and low-level GUI benchmarks. Beyond GUI tasks, SWIRL also demonstrates strong capability in multi-agent mathematical reasoning, underscoring its potential as a general framework for developing efficient and robust multi-agent systems.</font>



12.  **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">VEM: Environment-Free Exploration for Training GUI Agent with Value Environment Model</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Training Vision-Language Models (VLMs) for Graphical User Interfaces (GUI) agents via Reinforcement Learning (RL) faces critical challenges: environment-based RL requires costly interactions, while environment-free methods struggle with distribution shift and reward generalization. We propose an environment-free RL framework that decouples value estimation from policy optimization by leveraging a pretrained Value Environment Model (VEM), which requires no live environment interaction during policy optimization. VEM predicts state-action values directly from offline data, distilling human-like priors about GUI interaction outcomes without requiring next-state prediction or environmental feedback. This avoids compounding errors and enhances resilience to UI changes by focusing on semantic reasoning (e.g., “Does this action advance the user’s goal?”). The framework operates in two stages: (1) pretraining VEM to estimate long-term action utilities and (2) guiding policy exploration with frozen VEM signals, enabling layout-agnostic GUI automation. Evaluated across diverse benchmarks including Android-in-the-Wild for mobile apps and Multimodal-Mind2Web for web environments, VEM achieves state-of-the-art or highly competitive performance in both offline and online settings. It significantly outperforms environment-free baselines and matches or exceeds environment-based approaches, crucially without incurring interaction costs. Importantly, VEM demonstrates that robust, generalizable GUI agents can be trained efficiently using semantic-aware value estimation, proving effective across distinct interaction platforms like mobile and web. The code is available at </font>[https://anonymous.4open.science/r/VEM-Agent-51E7](https://anonymous.4open.science/r/VEM-Agent-51E7)<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">.</font>


<div style="text-align: center;">
<img src="figures\RL12.png"  width="60%" >
</div>

13.  **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">The advent of autonomous agents is transforming interactions with Graphical User Interfaces (GUIs) by employing natural language as a powerful intermediary. Despite the predominance of supervised fine-tuning (SFT) methods in current GUI agents for achieving spatial localization, these methods face substantial challenges due to their limited capacity to accurately perceive positional data. Existing strategies, such as reinforcement learning, often fail to assess positional accuracy effectively, thereby restricting their utility. In response, we introduce Location Preference Optimization (LPO), a novel approach that leverages locational data to optimize interaction preferences. LPO uses information entropy to predict interaction positions by focusing on zones rich in information. Besides, we further introduce a dynamic location reward function based on physical distance, reflecting the varying importance of interaction positions. Supported by Group Relative Preference Optimization (GRPO), LPO facilitates an extensive exploration of GUI environments and significantly enhances interaction precision. Comprehensive experiments demonstrate LPO's superior performance, achieving SOTA results across both offline benchmarks and real-world online evaluations. Our code will be made publicly available soon.</font>


<div style="text-align: center;">
<img src="figures\RL13.png"  width="60%" >
</div>

14.   **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Generalization in Online Reinforcement Learning for Mobile Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical user interface (GUI)-based mobile agents automate digital tasks on mobile devices by interpreting natural-language instructions and interacting with the screen. While recent methods apply reinforcement learning (RL) to train vision-language-model(VLM) agents in interactive environments with a primary focus on performance, generalization remains underexplored due to the lack of standardized benchmarks and open-source RL systems. In this work, we formalize the problem as a Contextual Markov Decision Process (CMDP) and introduce AndroidWorld-Generalization, a benchmark with three increasingly challenging regimes for evaluating zero-shot generalization to unseen task instances, templates, and applications. We further propose an end-to-end RL framework that integrates Group Relative Policy Optimization (GRPO) with a scalable rollout collection system, consisting of containerized infrastructure, asynchronous execution, and error recovery to support reliable and efficient training. Experiments on AndroidWorld-Generalization show that RL enables a 7B-parameter VLM agent to surpass supervised fine-tuning baselines, yielding a 26.1% improvement on unseen instances but only limited gains on unseen templates (15.7%) and apps (8.3%), underscoring the challenges of generalization. As a preliminary step, we demonstrate that few-shot adaptation at test-time improves performance on unseen apps, motivating future research in this direction. To support reproducibility and fair comparison, we open-source the full framework, including environment, tasks, models, prompts, and training infrastructure.</font>


<div style="text-align: center;">
<img src="figures\RL14.png"  width="60%" >
</div>

# **Special Ideas**
1. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Agent-ScanKit: Unraveling Memory and Reasoning of Multimodal Agents via Sensitivity Perturbations</font>**
**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Although numerous strategies have recently been proposed to enhance the autonomous interaction capabilities of multimodal agents in graphical user interface (GUI), their reliability remains limited when faced with complex or out-of-domain tasks. This raises a fundamental question: Are existing multimodal agents reasoning spuriously? In this paper, we propose \textbf{Agent-ScanKit}, a systematic probing framework to unravel the memory and reasoning capabilities of multimodal agents under controlled perturbations. Specifically, we introduce three orthogonal probing paradigms: visual-guided, text-guided, and structure-guided, each designed to quantify the contributions of memorization and reasoning without requiring access to model internals. In five publicly available GUI benchmarks involving 18 multimodal agents, the results demonstrate that mechanical memorization often outweighs systematic reasoning. Most of the models function predominantly as retrievers of training-aligned knowledge, exhibiting limited generalization. Our findings underscore the necessity of robust reasoning modeling for multimodal agents in real-world scenarios, offering valuable insights toward the development of reliable multimodal agents. Our code is available at anonymous.

<div style="text-align: center;">
<img src="figures\Special1.png"  width="60%" >
</div>

2. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">OmniActor: A Generalist GUI and Embodied Agent for 2D&3D Worlds</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Multimodal large language models are progressively advancing toward multimodal agents that can proactively execute tasks. Existing research on multimodal agents primarily targets either GUI or embodied scenarios, corresponding to interactions within 2D virtual world and 3D physical world, respectively. However, many real-world tasks inherently require agents to interleave interactions across both types of environments. We initially mix GUI and embodied data to train models, but find performance degradation caused by data conflicts. Further analysis reveals that GUI and embodied data exhibit synergy at shallow layers but conflict at deep layers, resembling the cerebrum-cerebellum mechanism in the human brain. To this end, we introduce a high-performance generalist agent OmniActor, designed from both structural and data perspectives. First, we propose Layer-heterogeneous MoE that separates parameters at deep layers to eliminate conflict, while sharing parameters at shallow layers to leverage synergy. This design enables OmniActor to outperform agents trained solely on GUI or embodied data in their respective tasks. Furthermore, we unify the action spaces of GUI and embodied tasks and collect large-scale datasets from diverse sources for training. This substantially enhances the performance of OmniActor across various scenarios, especially in GUI tasks. The code will be publicly available.</font>


<div style="text-align: center;">
<img src="figures\Special2.png"  width="60%" >
</div>

3. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Say One Thing, Do Another? Diagnosing Reasoning-Execution Gaps in VLM-Powered Mobile-Use Agents</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Mobile-use agents powered by vision-language models (VLMs) have shown great potential in interpreting natural language instructions and generating corresponding actions based on mobile graphical user interface. Recent studies suggest that incorporating chain-of-thought (CoT) reasoning tends to improve the execution accuracy. However, existing evaluations emphasize execution accuracy while neglecting whether CoT reasoning aligns with ground-truth actions. This oversight fails to assess potential reasoning-execution gaps, which in turn foster over-trust: users relying on seemingly plausible CoTs may unknowingly authorize harmful actions, potentially resulting in financial loss or trust crisis. In this work, we introduce a new evaluation framework to diagnose reasoning-execution gaps. At its core lies Ground-Truth Alignment (GTA), which measures whether the action implied by a CoT matches the ground-truth action. By combining GTA with the standard Exact Match (EM) metric, we jointly assess both the reasoning accuracy and execution accuracy. This joint perspective reveals two types of reasoning-execution gaps: (i) Execution Gap (EG), where the reasoning correctly identifies the correct action but execution fails, and (ii) Reasoning Gap (RG), where execution succeeds but reasoning process conflicts with the actual execution. Experimental results across a wide range of mobile interaction tasks reveal that reasoning-execution gaps are prevalent, with execution gaps occurring more frequently than reasoning gaps. Moreover, while scaling up model size reduces the overall gap, sizable execution gaps persist even in the largest models. Further analysis shows that our framework reliably reflects systematic EG/RG patterns in state-of-the-art models. These findings offer concrete diagnostics and support the development of more trustworthy mobile-use agents. Our data and code are publicly available at anonymity.</font>


# **Test-time Scaling**
1. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GAIA: A Data Flywheel System for Training GUI Test-Time Scaling Critic Models</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">While Large Vision-Language Models (LVLMs) have significantly advanced GUI agents' capabilities in parsing textual instructions, interpreting screen content, and executing tasks, a critical challenge persists: the irreversibility of agent operations—where a single erroneous action can trigger catastrophic deviations. To address this, we propose the GUI Action Critic's Data Flywheel System (GAIA), a training framework that enables the models to have iterative critic capabilities, which are used to improve the Test-Time Scaling (TTS) of basic GUI agents' performance. Specifically, we train an Intuitive Critic Model (ICM) using positive and negative action examples from a base agent first. This critic evaluates the immediate correctness of the agent's intended actions, thereby selecting operations with higher success probability. Then, the initial critic guides agent actions to collect refined positive/negative samples, initiating the self-improving cycle. The augmented data then trains a second-round critic with enhanced discernment capability. We conduct experiments on various datasets and demonstrate that the proposed ICM can improve the test-time performance of various closed-source and open-source models, and the performance can be gradually improved as the data is recycled. The code and dataset will be publicly released.</font>


<div style="text-align: center;">
<img src="figures\Test-time1.png"  width="60%" >
</div>

2. **GTA1: GUI Test-time Scaling Agent**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);"> Graphical user interface (GUI) agents autonomously complete tasks across platforms (\eg, Linux) by sequentially decomposing user instructions into action proposals that iteratively interact with visual elements in the evolving environment. However, two main challenges arise: i) planning (\ie, the action proposal sequence) under expansive action space, where selecting an appropriate plan is non-trivial, as many valid ones may exist; ii) accurately grounding actions in complex and high-resolution interfaces, \ie, precisely interacting with visual targets. This paper investigates the aforementioned challenges with our GUI Test-time Scaling Agent, namely \name. First, we conduct test-time scaling to select the most appropriate action proposal: at each step, multiple candidate proposals are sampled and evaluated and selected by a judge model. It trades off computation for better decision quality by concurrent sampling. Second, we propose a model that improves grounding of the selected action proposals to its corresponding visual elements. Our key insight is that reinforcement learning (RL) facilitates grounding through inherent objective alignments, rewarding successful clicks on interface elements. Experimentally, \name achieves state-of-the-art performance on both grounding and agent task execution benchmarks.


<div style="text-align: center;">
<img src="figures\Test-time2.png"  width="60%" >
</div>

3. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Confidence-Guided MCTS for Efficient Long-Horizon Web Agent Tasks</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">LLM agents that solve long-horizon tasks on the web often rely on Monte Carlo Tree Search (MCTS) to plan and reason over extended trajectories. While effective, standard MCTS requires wide branching and repeated value evaluations, making it very compute-intensive. We introduce confidence-guided MCTS, a method that uses internal certainty signals from the model’s own log-probabilities to efficiently allocate search power to MCTS. The guided MCTS enables adaptive branching that adjusts the width of the tree depending on how confident the model is, reducing expansion when predictions are already decisive and vice versa. Our paper also includes multiple variants for integrating confidence into tree search; variants like weighted backpropagation incorporate certainty directly into value updates, amplifying reliable rollouts and reducing the impact of noisy ones. The method demonstrates that lightweight internal signals can guide search more effectively, reducing inference computation while preserving or even improving success on complex long-horizon tasks, moving closer to the Pareto frontier. Confidence-guided MCTS highlights a simple but powerful direction: using the model’s own certainty to make search-augmented agents more efficient without extra supervision.</font>


<div style="text-align: center;">
<img src="figures\Test-time3.png"  width="60%" >
</div>

# **Data Generation**
1. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">MobileA3gent: Training Mobile GUI Agents Using Decentralized Self-Sourced Data from Diverse Users</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">The advancement of mobile GUI agents has opened new opportunities for automating tasks on mobile devices. Training these agents requires large-scale high-quality data, which is prohibitively expensive when relying on human labor. Given the vast population of global mobile phone users, if automated data collection from them becomes feasible, the resulting data volume and the subsequently trained mobile agents could reach unprecedented levels. Nevertheless, two major challenges arise: (1) extracting user instructions without human intervention and (2) utilizing distributed user data while preserving privacy.</font>

<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">To tackle these challenges, we propose MobileA3gent, a collaborative framework that trains mobile GUI Agents using decentralized self-sourced data from diverse users. The framework comprises two components, each targeting a specific challenge: (1) Auto-Annotation, which enables the automatic collection of high-quality datasets during users' routine phone usage with minimal cost. (2) FedVLM-A, which enhances federated VLM training under non-IID distributions by incorporating adapted global aggregation based on both episode-level and step-level variability. Extensive experiments prove that MobileA3gent achieves superior performance over traditional approaches at only 1% of the cost, highlighting its potential for real-world applications. Our code is publicly available at: </font>[https://anonymous.4open.science/r/MobileA3gent-Anonymous](https://anonymous.4open.science/r/MobileA3gent-Anonymous)<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">.</font>


<div style="text-align: center;">
<img src="figures\DataGen1.png"  width="60%" >
</div>

2. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUIrilla: A Scalable Framework for Automated Desktop UI Exploration</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Autonomous agents capable of operating complex graphical user interfaces (GUIs) have the potential to transform desktop automation. While recent advances in large language models (LLMs) have significantly improved UI understanding, navigating full-window, multi-application desktop environments remains a major challenge. Data availability is limited by costly manual annotation, closed-source datasets and surface-level synthetic pipelines.</font>

<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">We introduce GUIrilla, an automated scalable framework that systematically explores applications via native accessibility APIs to address the critical data collection challenge in GUI automation. Our framework focuses on macOS - an ecosystem with limited representation in current UI datasets - though many of its components are designed for broader cross-platform applicability. GUIrilla organizes discovered interface elements and crawler actions into hierarchical GUI graphs and employs specialized interaction handlers to achieve comprehensive application coverage.</font>

<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Using the application graphs from GUIrilla crawler, we construct and release GUIrilla‑Task, a large-scale dataset of 27,171 functionally grounded tasks across 1,108 macOS applications, each annotated with full-desktop and window-level screenshots, accessibility metadata, and semantic action traces.</font>

<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Empirical results show that tuning LLM-based agents on GUIrilla‑Task significantly improves performance on downstream UI tasks, outperforming synthetic baselines on the ScreenSpot Pro benchmark while using 97% less data. We also release macapptree, an open-source library for reproducible collection of structured accessibility metadata, along with the full GUIrilla‑Task dataset, the manually verified GUIrilla-Gold benchmark, and the framework code to support open research in desktop autonomy.</font>


<div style="text-align: center;">
<img src="figures\DataGen2.png"  width="60%" >
</div>

3. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">ScaleCUA: Scaling Open-Source Computer Use Agents with Cross-Platform Data</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Vision-Language Models (VLMs) have enabled computer use agents (CUAs) that operate GUIs autonomously, showing great potential, yet progress is limited by the lack of large-scale, open-source computer use data and foundation models. In this work, we introduce ScaleCUA, a step toward scaling open-source CUAs. It offers a large-scale dataset spanning 6 operating systems and 3 task domains, built via a closed-loop pipeline uniting automated agents with human experts. Trained on this scaled-up data, ScaleCUA can operate seamlessly across platforms. Specifically, it delivers strong gains over baselines (+26.6 on WebArena-Lite-v2, +10.7 on ScreenSpot-Pro) and sets new state-of-the-art results (94.4% on MMBench-GUI L1-Hard, 60.6% on OSWorld-G, 47.4% on WebArena-Lite-v2). These findings underscore the power of data-driven scaling for general-purpose computer use agents. We will release data, models, and code to advance future research.</font>


<div style="text-align: center;">
<img src="figures\DataGen3.png"  width="60%" >
</div>

4. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">GUI-ReWalk: Massive Data Generation for GUI Agent via Stochastic Exploration and Intent-Aware Reasoning</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Graphical User Interface (GUI) Agents, powered by large language and vision-language models, hold promise for enabling end-to-end automation in digital environments. However, their progress is fundamentally constrained by the scarcity of scalable, high-quality trajectory data. Existing data collection strategies either rely on costly and inconsistent manual annotations or on synthetic generation methods that trade off between diversity and meaningful task coverage. To bridge this gap, we present GUI-ReWalk: a reasoning-enhanced, multi-stage framework for synthesizing realistic and diverse GUI trajectories. GUI-ReWalk begins with a stochastic exploration phase that emulates human trial-and-error behaviors, and progressively transitions into a reasoning-guided phase where inferred goals drive coherent and purposeful interactions. Moreover, it supports multi-stride task generation, enabling the construction of long-horizon workflows across multiple applications. By combining randomness for diversity with goal-aware reasoning for structure, GUI-ReWalk produces data that better reflects the intent-aware, adaptive nature of human-computer interaction. We further train Qwen2.5-VL-7B on the GUI-ReWalk dataset and evaluate it across multiple benchmarks, including Screenspot-Pro, OSWorld-G, UI-Vision, AndroidControl, and GUI-Odyssey. Results demonstrate that GUI-ReWalk enables superior coverage of diverse interaction flows, higher trajectory entropy, and more realistic user intent. These findings establish GUI-ReWalk as a scalable and data-efficient framework for advancing GUI agent research and enabling robust real-world automation.</font>


<div style="text-align: center;">
<img src="figures\DataGen4.png"  width="60%" >
</div>

# **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Security</font>**
1. **<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">RISK: A Framework for GUI Agents in E-commerce Risk Management</font>**

**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">Abstract: </font>**<font style="color:rgb(44, 58, 74);background-color:rgb(255, 253, 250);">E-commerce risk management requires aggregating diverse, deeply embedded web data through multi-step, stateful interactions, which traditional scraping methods and most existing Graphical User Interface (GUI) agents cannot handle. These agents are typically limited to single-step tasks and lack the ability to manage dynamic, interactive content critical for effective risk assessment. To address this challenge, we introduce RISK, a novel framework designed to build and deploy GUI agents for this domain. RISK integrates three components: (1) RISK-Data, a dataset of 8,492 single-step and 2,386 multi-step interaction trajectories, collected through a high-fidelity browser framework and a meticulous data curation process; (2) RISK-Bench, a benchmark with 602 single-step and 320 multi-step trajectories across three difficulty levels for standardized evaluation; and (3) RISK-R1, a R1-style reinforcement fine-tuning framework considering four aspects: (i) Output Format: Updated format reward to enhance output syntactic correctness and task comprehension, (ii) Single-step Level: Stepwise accuracy reward to provide granular feedback during early training stages, (iii) Multi-step Level: Process reweight to emphasize critical later steps in interaction sequences, and (iv) Task Level: Level reweight to focus on tasks of varying difficulty. Experiments show that RISK-R1 outperforms existing baselines, achieving a 6.8% improvement in offline single-step and an 8.8% improvement in offline multi-step. Moreover, it attains a top task success rate of 70.5% in online evaluation. RISK provides a scalable, domain-specific solution for automating complex web interactions, advancing the state of the art in e-commerce risk management.</font>



<div style="text-align: center;">
<img src="figures\Security.png"  width="60%" >
</div>
