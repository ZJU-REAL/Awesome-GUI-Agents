<img width="176" height="51" alt="image" src="https://github.com/user-attachments/assets/1df2d7bb-a4a1-4d3a-9d73-6c3367b9328b" /># Awesome-GUI-Agents
A curated list for **GUI Agents**

**Table Content:** 

- [Updates](#updates) 
- [Contributing](#contri) 
- [Four Module of GUI Agents](#module)
- [Paper List](#paper-list) 
  - [GUI Agents List](#agentlist) 
  - [GUI Agents Datasets & Benchmark List](#datasets) 

![Alt text](figures/overview.png)

## üöÄ [Updates](#updates) 
* September 16, 2025: We released our new paper on GUI Automation: **UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning**
    - üìÑ [arXiv 2509.11543](https://arxiv.org/pdf/2509.11543)  
    - ü§ó [Hugging Face Papers](https://huggingface.co/papers/2509.11543)  
    - üíª [GitHub Project](https://github.com/X-PLUG/MobileAgent/tree/main/UI-S1)  
* Augest 18, 2025: We open-source our GUI-G2-3B and GUI-G2-7B models. Try it out.

  - <a href="https://huggingface.co/inclusionAI/GUI-G2-3B"><img src="https://img.shields.io/badge/Model-GUI--G2--3B-007ec6?style=flat&logo=huggingface" alt="GUI-G2 3B Model"></a>
  - <a href="https://huggingface.co/inclusionAI/GUI-G2-7B"><img src="https://img.shields.io/badge/Model-GUI--G2--7B-007ec6?style=flat&logo=huggingface" alt="GUI-G2 7B Model"></a>

* Augest 14, 2025: We released our new paper on GUI Grounding: **Test-Time Reinforcement Learning for GUI Grounding via Region Consistency**
  - üìÑ [arXiv 2508.05615](https://arxiv.org/abs/2508.05615)   
  - üíª [GitHub Project](https://github.com/zju-real/gui-rcpo)  

* August 1, 2025: **We will add a weekly section summarizing GUI Agent research papers. Stay tuned!**

* July 22, 2025: **We released our new paper on GUI Grounding: GUI-G¬≤: Gaussian Reward Modeling for GUI Grounding. [Check it out](https://arxiv.org/abs/2507.15846)!**
  - üìÑ [arXiv 2507.15846](https://arxiv.org/abs/2507.15846)  
  - ü§ó [Hugging Face Papers](https://huggingface.co/papers/2507.15846)  
  - üíª [GitHub Project](https://github.com/zju-real/GUI-G2)  
  - üåê [Project Page](https://zju-real.github.io/GUI-G2/)

* April 22, 2025: We're excited to announce that our paper has been published and is now available on arXiv. We welcome your attention and feedback! [Check it out](https://arxiv.org/abs/2504.13865). We updated GUI Agents List based on RL (R1 Style). 
  
* April 2, 2025: We have already uploaded the paper to arXiv, please wait for some time. Meanwhile, we will keep updating this repo.

* March 24, 2025: We updated the repository and released our comprehensive survey on GUI Agents.

## üíÆ [Contributing](#contri)

If you'd like to include your paper, or need to update any details such as github repo information or code URLs, please feel free to submit a pull request. 

## üìö Weekly Paper List
<details style="background: #f7f3e9; border-left: 5px solid #8b4513; padding: 20px; margin: 10px 0; box-shadow: 2px 2px 10px rgba(0,0,0,0.1); font-family: Georgia, serif;">
<summary style="color: #8b4513; font-weight: bold; cursor: pointer; list-style: none; font-size: 18px;">
üìö 2025-8-25 to 2025-8-29 üìö
</summary>


**Weekly Paper ListÔºö**
1. Structuring GUI Elements through Vision Language Models: Towards Action Space Generation
2. WEBSIGHT: A Vision-First Architecture for Robust Web Agents
3. PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration
4. UItron: Foundational GUI Agent with Advanced Perception and Planning
</details>

<details style="background: #f7f3e9; border-left: 5px solid #8b4513; padding: 20px; margin: 10px 0; box-shadow: 2px 2px 10px rgba(0,0,0,0.1); font-family: Georgia, serif;">
<summary style="color: #8b4513; font-weight: bold; cursor: pointer; list-style: none; font-size: 18px;">
üìö 2025-8-18 to 2025-8-22 üìö
</summary>

**Weekly Paper ListÔºö**
1. CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks
2. COMPUTERRL: SCALING END-TO-END ONLINE REINFORCEMENT LEARNING FOR COMPUTER USE AGENTS
3. Mobile-Agent-v3: Foundamental Agents for GUI Automation
4. SWIRL: A STAGED WORKFLOW FOR INTERLEAVED REINFORCEMENT LEARNING IN MOBILE GUI CONTROL
</details>

<details style="background: #f7f3e9; border-left: 5px solid #8b4513; padding: 20px; margin: 10px 0; box-shadow: 2px 2px 10px rgba(0,0,0,0.1); font-family: Georgia, serif;">
<summary style="color: #8b4513; font-weight: bold; cursor: pointer; list-style: none; font-size: 18px;">
üìö 2025-8-11 to 2025-8-15 üìö
</summary>

**Weekly Paper ListÔºö**
1. OPENCUA: Open Foundations for Computer-Use Agents
2. Test-Time Reinforcement Learning for GUI Grounding via Region Consistency
3. WinSpot: A Windows GUI Grounding Benchmark with Multimodal Large Language Models
4. InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization
5. UI-Venus Technical Report: Building High-performance UI Agents with RFT
</details>

## [Four Module of GUI Agents](#module)

We have divided GUI Agents into four modules: perception, exploration, planning, and interaction, as shown below:

![Alt text](figures/module.jpg)

## [Paper List](#paperlist)

We have dedicated a separate chapter to datasets and benchmarks for GUI Agents, with all content presented in chronological order.

### Technical Report

Mano Technical Report

SCALECUA: SCALING OPEN-SOURCE COMPUTER USE AGENTS WITH CROSS-PLATFORM DATA

UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning

CODA: COORDINATING THE CEREBRUM AND CEREBELLUM FOR A DUAL-BRAIN COMPUTER USE AGENT WITH DECOUPLED REINFORCEMENT LEARNING.

UItron: Foundational GUI Agent with Advanced Perception and Planning

Mobile-Agent-v3: Foundamental Agents for GUI Automation

UI-Venus Technical Report: Building High-performance UI Agents with RFT

OPENCUA: Open Foundations for Computer-Use Agents

MAGICGUI: A FOUNDATIONAL MOBILE GUI AGENT WITH SCALABLE DATA PIPELINE AND REINFORCEMENT FINE-TUNING

Magentic-UI: Towards Human-in-the-loop Agentic Systems

Phi-Ground Tech Report: Advancing Perception in GUI Grounding

[MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment](https://www.arxiv.org/pdf/2507.05720)

[GTA1: GUI Test-time Scaling Agent](https://arxiv.org/pdf/2507.05791v1)

[MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569)

[AgentCPM-GUI: Building Mobile-Use Agents with Reinforcement Fine-Tuning](https://arxiv.org/pdf/2506.01391)

[UI-TARS: Pioneering Automated GUI Interaction with Native Agents](https://arxiv.org/abs/2501.12326)

### Memory for GUI Agents
2025-10-13: AUTO-SCALING CONTINUOUS MEMORY FOR GUI AGENT

### Online RL 
EFFICIENT MULTI-TURN RL FOR GUI AGENTS VIA DECOUPLED TRAINING AND ADAPTIVE DATA CURATION

UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning

MOBILERL: ADVANCING MOBILE USE AGENTS WITH ADAPTIVE ONLINE REINFORCEMENT LEARNING

COMPUTERRL: SCALING END-TO-END ONLINE REINFORCEMENT LEARNING FOR COMPUTER USE AGENTS

Mobile-Agent-v3: Foundamental Agents for GUI Automation

### GUI Navigation Benchmark
MemGUI-Bench: Benchmarking Memory of Mobile GUI Agents in Dynamic Environments

2025-09: MAS-Bench: A Unified Benchmark for Shortcut-Augmented Hybrid Mobile GUI Agents 

2025-08: UI-NEXUS: Atomic-to-Compositional Generalization for Mobile Agents with A New Benchmark and Scheduling System

2025-07: MMBENCH-GUI: HIERARCHICAL MULTI-PLATFORM EVALUATION FRAMEWORK FOR GUI AGENTS
We will organize these points later:

1„ÄÅAndroidWorld

2„ÄÅAndroidControl

3„ÄÅGUI-Odyssey

4„ÄÅAmex

5„ÄÅWebArena

6„ÄÅWebSRC_v1.0

7„ÄÅMind2Web 2

8„ÄÅ... stay tuned


### Multi-turn GUI Agent
2025-10: EFFICIENT MULTI-TURN RL FOR GUI AGENTS VIA DECOUPLED TRAINING AND ADAPTIVE DATA CURATION

2025-09: UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning

2025-05: ARPO: End-to-End Policy Optimization for GUI Agents with Experience Replay

2025-05: WEBAGENT-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning

### Training Free GUI Grounding
1„ÄÅAttention-driven GUI Grounding: Leveraging Pretrained Multimodal Large Language Models without Fine-Tuning [2025AAAI]

### Zero Cost GUI Agent
2025-6-02: ZeroGUI: Automating Online GUI Learning at Zero Human Cost

### GUI Grounding Benchmark
WinSpot: A Windows GUI Grounding Benchmark with Multimodal Large Language Models

UI-Vision: A Desktop-centric GUI Benchmark for Visual Perception and Interaction [2025ICML]

[ScreenSpot](https://huggingface.co/datasets/rootsautomation/ScreenSpot): [SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents](https://arxiv.org/abs/2401.10935)

[ScreenSpot-V2](https://huggingface.co/datasets/OS-Copilot/ScreenSpot-v2): [OS-ATLAS: Foundation Action Model for Generalist GUI Agents](https://arxiv.org/abs/2410.23218)

[ScreenSpot-Pro](https://huggingface.co/datasets/likaixin/ScreenSpot-Pro): [ScreenSpot-Pro: GUI Grounding for Professional High-Resolution Computer Use](https://likaixin2000.github.io/papers/ScreenSpot_Pro.pdf)

[CA-GUI](https://huggingface.co/datasets/openbmb/CAGUI): [AgentCPM-GUI: An on-device GUI agent for operating Android apps, enhancing reasoning ability with reinforcement fine-tuning for efficient task execution.](https://github.com/OpenBMB/AgentCPM-GUI)

### Video GUI
desc: using video to assist gui agent learn

Scalable Video-to-Dataset Generation for Cross-Platform Mobile Agents

ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation

VideoGUI: A Benchmark for GUI Automation from Instructional Videos

Mobile-Agent-V: A Video-Guided Approach for Effortless and Efficient Operational Knowledge Injection in Mobile Automation

Watch and Learn: Learning to Use Computers from Online Videos

### Test-Scaling Method (GUI Grounding && GUI Navigation) (Zoom In && Zoom Out)
1„ÄÅTest-Time Reinforcement Learning for GUI Grounding via Region Consistency

2„ÄÅGTA1: GUI Test-time Scaling Agent

3„ÄÅDiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning [2025 EMNLP]

4„ÄÅImproved GUI Grounding via Iterative Narrowing

5„ÄÅVisual Test-time Scaling for GUI Agent Grounding

6„ÄÅUI-AGILE: Advancing GUI Agents with Effective Reinforcement Learning and Precise Inference-Time Grounding

7„ÄÅGENERALIST SCANNER MEETS SPECIALIST LOCATOR: A SYNERGISTIC COARSE-TO-FINE FRAMEWORK FOR ROBUST GUI GROUNDING

### Prune
2025-10-01: GUI-KV: EFFICIENT GUI AGENTS VIA KV CACHE WITH SPATIO-TEMPORAL AWARENESS

### R1-Style GUI Agents
2025-10-04: GUI-SPOTLIGHT: ADAPTIVE ITERATIVE FOCUS REFINEMENT FOR ENHANCED GUI VISUAL GROUNDING

2025-9-30: Ferret-UI Lite: Lessons from Building Small On-Device GUI Agents

2025-9-30: UI-UG: A Unified MLLM for UI Understanding and Generation

2025-9-28: GUI-SHEPHERD: RELIABLE PROCESS REWARD AND VERIFICATION FOR LONG-SEQUENCE GUI TASKS

2025-9-28: EFFICIENT MULTI-TURN RL FOR GUI AGENTS VIA DECOUPLED TRAINING AND ADAPTIVE DATA CURATION

2025-9-25: LEARNING GUI GROUNDING WITH SPATIAL REASONING FROM VISUAL FEEDBACK

2025-9-23: Orcust: Stepwise-Feedback Reinforcement Learning for GUI Agent

2025-9-22: GUI-ARP: ENHANCING GROUNDING WITH ADAPTIVE REGION PERCEPTION FOR GUI AGENTS

2025-9-22: BTL-UI: Blink-Think-Link Reasoning Model for GUI Agent (2025 NIPS)

2025-9-16: UI-S1: Advancing GUI Automation via Semi-online Reinforcement Learning

2025-9-06: WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents

2025-9-05: Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding 

2025-8-28: SWIRL: A STAGED WORKFLOW FOR INTERLEAVED REINFORCEMENT LEARNING IN MOBILE GUI CONTROL

2025-8-20: CODA: COORDINATING THE CEREBRUM AND CEREBELLUM FOR A DUAL-BRAIN COMPUTER USE AGENT WITH DECOUPLED REINFORCEMENT LEARNING.

2025-8-20: COMPUTERRL: SCALING END-TO-END ONLINE REINFORCEMENT LEARNING FOR COMPUTER USE AGENTS

2025-8-18: CRAFT-GUI: Curriculum-Reinforced Agent For GUI Tasks

2025-8-11: InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization

2025-8-06: SEA: Self-Evolution Agent with Step-wise Reward for Computer Use

2025-8-06: NaviMaster: Learning a Unified Policy for GUI and Embodied Navigation Tasks

2025-8-04: GuirlVG: Incentivize GUI Visual Grounding via Empirical Exploration on Reinforcement Learning

2025-7-22: GUI-G¬≤: Gaussian Reward Modeling for GUI Grounding  
  - üìÑ [arXiv 2507.15846](https://arxiv.org/abs/2507.15846)  
  - ü§ó [Hugging Face Papers](https://huggingface.co/papers/2507.15846) 
  - üíª [GitHub Project](https://github.com/zju-real/GUI-G2) 
  - üåê [Project Page](https://zju-real.github.io/GUI-G2/)
  - <a href="https://huggingface.co/inclusionAI/GUI-G2-3B"><img src="https://img.shields.io/badge/Model-GUI--G2--3B-007ec6?style=flat&logo=huggingface" alt="GUI-G2 3B Model"></a>
  - <a href="https://huggingface.co/inclusionAI/GUI-G2-7B"><img src="https://img.shields.io/badge/Model-GUI--G2--7B-007ec6?style=flat&logo=huggingface" alt="GUI-G2 7B Model"></a>

2025-7-09: MobileGUI-RL: Advancing Mobile GUI Agent through Reinforcement Learning in Online Environment

2025-7-09: GTA1: GUI Test-time Scaling Agent

2025-6-25: Mobile-R1: Towards Interactive Reinforcement Learning for VLM-Based Mobile Agent via Task-Level Rewards

2025-6-13: LPO: Towards Accurate GUI Agent Interaction via Location Preference Optimization

2025-5-29: Grounded Reinforcement Learning for Visual Reasoning

2025-5-22: WEBAGENT-R1: Training Web Agents via End-to-End Multi-Turn Reinforcement Learning

2025-6-06: Look Before You Leap: A GUI-Critic-R1 Model for Pre-Operative Error Diagnosis in GUI Automation

2025-5-21: GUI-G1: Understanding R1-Zero-Like Training for Visual Grounding in GUI Agents

2025-5-18: Enhancing Visual Grounding for GUI Agents via Self-Evolutionary Reinforcement Learning

2025-4-19: InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners

2025-4-14: GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents

2025-3-27: UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning

### [GUI Agents List](#agentlist)
2025-10 MLLM as a UI Judge: Benchmarking Multimodal LLMs for Predicting Human Perception of User Interfaces

2025-10 ReInAgent: A Context-Aware GUI Agent Enabling Human-in-the-Loop Mobile Task Navigation

2025-10 IMPROVING GUI GROUNDING WITH EXPLICIT POSITION-TO-COORDINATE MAPPING

2025-10 PAL-UI: PLANNING WITH ACTIVE LOOK-BACK FOR VISION-BASED GUI AGENTS (SFT)

2025-10 AGENT-SCANKIT: UNRAVELING MEMORY AND REASONING OF MULTIMODAL AGENTS VIA SENSITIVITY PERTURBATIONS

2025-9 Log2Plan: An Adaptive GUI Automation Framework Integrated with Task Mining Approach

2025-9 Retrieval-augmented GUI Agents with Generative Guidelines

2025-9 Instruction Agent: Enhancing Agent with Expert Demonstration

2025-9 Learning Active Perception via Self-Evolving Preference Optimization for GUI Grounding

2025-8 SWIRL: A STAGED WORKFLOW FOR INTERLEAVED REINFORCEMENT LEARNING IN MOBILE GUI CONTROL

2025-8 PerPilot: Personalizing VLM-based Mobile Agents via Memory and Exploration

2025-8 WEBSIGHT: A Vision-First Architecture for Robust Web Agents

2025-8 Structuring GUI Elements through Vision Language Models: Towards Action Space Generation

2025-8 You Don‚Äôt Know Until You Click: Automated GUI Testing for Production-Ready Software Evaluation

2025-8 Browsing Like Human: A Multimodal Web Agent with Experiential Fast-and-Slow Thinking [2025ACL]

2025-8 Uncertainty-Aware GUI Agent: Adaptive Perception through Component Recommendation and Human-in-the-Loop Refinement

2025-7 ZonUI-3B: A Lightweight Vision‚ÄìLanguage Model for Cross-Resolution GUI Grounding

2025-7 Qwen-GUI-3B: A Lightweight Vision‚ÄìLanguage Model for Cross-Resolution GUI Grounding

2025-6 Understanding GUI Agent Localization Biases through Logit Sharpness

2025-6 DiMo-GUI: Advancing Test-time Scaling in GUI Grounding via Modality-Aware Visual Reasoning

2025-6 GUI-Actor: Coordinate-Free Visual Grounding for GUI Agents [Github](https://github.com/microsoft/GUI-Actor) [Paper](https://arxiv.org/abs/2506.03143)

2025-5 UI-Genie: A Self-Improving Approach for Iteratively Boosting MLLM-based Mobile GUI Agents
| Title & Time | Introduction | Links |
|:--|  :----: | :---:|
| Less is More: Empowering GUI Agent with Context-Aware Simplification (2025-7, 2025ICCV) | <img width="1002" alt="image" src="figures/simpagent.png" >| [Github](https://github.com/JiuTian-VL/SimpAgent) <br/> [Paper](https://arxiv.org/pdf/2507.03730)|
| Mirage-1: Augmenting and Updating GUI Agent with Hierarchical Multimodal Skills (2025-6) | <img width="1002" alt="image" src="figures/Mirage_1.png" >| [Github](https://cybertronagent.github.io/Mirage-1.github.io/) <br/> [Paper](https://arxiv.org/abs/2506.10387)|
| GUI-Explorer: Autonomous Exploration and Mining of Transition-aware Knowledge for GUI Agent (2025-5, 2025ACL) | <img width="1002" alt="image" src="figures/gui_explorer.png" >| [Github](https://github.com/JiuTian-VL/GUI-explorer) <br/> [Paper](https://arxiv.org/pdf/2505.16827)|
| MobileA^3gent: Training Mobile GUI Agents Using Decentralized Self-Sourced Data from Diverse Users (2025-5) | <img width="1002" alt="image" src="figures/mobile3agent.png" >| [Github](https://anonymous.4open.science/r/MobileA3gent-Anonymous) <br/> [Paper](https://arxiv.org/pdf/2502.02982)|
| Advancing Mobile GUI Agents: A Verifier-Driven  Approach to Practical Deployment (2025-3) | <img width="1002" alt="image" src="figures/verifiergui.png" > | [Github]() <br/> [Paper](https://www.arxiv.org/abs/2503.15937) |
| API Agents vs. GUI Agents: Divergence and Convergence (2025-3) | <img width="1002" alt="image" src="figures/apivsgui.png" > | [Github]() <br/> [Paper](https://arxiv.org/abs/2503.11069) |
| AppAgentX: Evolving GUI Agents as Proficient Smartphone Users (2025-3) | <img width="1002" alt="image" src="figures/appagentx.png" > | [Github](https://github.com/Westlake-AGI-Lab/AppAgentX) <br/> [Paper](https://arxiv.org/pdf/2503.02268) |
| Think Twice, Click Once: Enhancing GUI Grounding via Fast and Slow Systems (2025-3) | <img width="1002" alt="image" src="figures/focus.png" > | [Github](https://github.com/sugarandgugu/Focus) <br/> [Paper](https://arxiv.org/pdf/2503.06470) |
| UI-TARS: Pioneering Automated GUI Interaction with Native Agents  (2025-2) | <img width="1002" alt="image" src="figures/uitars.png" > | [Github](https://github.com/bytedance/UI-TARS) <br/> [Paper](https://arxiv.org/abs/2501.12326) |
| Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks (2025-1) | <img width="1002" alt="image" src="figures/mobileagente.png" > | [Github](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-E) <br/> [Paper](https://arxiv.org/abs/2501.11733) |
| InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection (2025-1) | <img width="1002" alt="image" src="figures/infiagent.png" > | [Github](https://github.com/Reallm-Labs/InfiGUIAgent) <br/> [Paper](https://arxiv.org/abs/2501.04575) |
| OS-Genesis: Automating GUI Agent Trajectory Construction via  Reverse Task Synthesis (2024-12) | <img width="1002" alt="image" src="figures/osgenesis.png" > | [Github](https://github.com/OS-Copilot/OS-Genesis) <br/> [Paper](https://arxiv.org/abs/2412.19723) |
| PC Agent: While You Sleep, AI Works - A Cognitive Journey into Digital World (2024-12) | <img width="1002" alt="image" src="figures/pcagent.png" > | [Github](https://github.com/GAIR-NLP/PC-Agent) <br/> [Paper](https://arxiv.org/abs/2412.17589) |
| Aria-UI: Visual Grounding for GUI Instructions (2024-12) | <img width="1002" alt="image" src="figures/ariaui.png" > | [Github](https://github.com/AriaUI/Aria-UI) <br/> [Paper](https://arxiv.org/abs/2412.16256) |
| Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining (2024-12) | <img width="1002" alt="image" src="figures/iris.png" > | [Github]() <br/> [Paper](https://arxiv.org/abs/2412.10342) |
| AgentTrek : AGENT TRAJECTORY SYNTHESIS  VIA GUIDING REPLAY WITH WEB TUTORIALS (2024-12) | <img width="1002" alt="image" src="figures/agenttrek.png" > | [Github](https://github.com/xlang-ai/AgentTrek) <br/> [Paper](https://arxiv.org/abs/2412.09605) |
| AGUVIS: UNIFIED PURE VISION AGENTS FOR  AUTONOMOUS GUI INTERACTION (2024-12) | <img width="1002" alt="image" src="figures/aguvis.png"> | [Github](https://github.com/xlang-ai/aguvis) <br/> [Paper](https://arxiv.org/abs/2412.04454) |
| Ponder & Press: Advancing Visual GUI Agent  towards General Computer Control (2024-12) |   <img width="1002" alt="image" src="figures/ponder.png">    |  [Github]() <br/> [Paper](https://arxiv.org/abs/2412.01268)  |
| ShowUI: One Vision-Language-Action Model for GUI Visual Agent (2024-11) |   <img width="1002" alt="image" src="figures/showui.png">    | [Github](https://github.com/showlab/ShowUI) <br/> [Paper](https://arxiv.org/abs/2411.17465) |
| The Dawn of GUI Agent: A Preliminary  Case Study with Claude 3.5 Computer Use (2024-11) |  <img width="1002" alt="image" src="figures/claude3.5.png">  |  [Github]() <br/> [Paper](https://arxiv.org/abs/2411.10323)  |
| MobA: Multifaceted Memory-Enhanced Adaptive Planning for Efficient Mobile Task Automation (2024-10) (2025 NAACL Demo) |  <img width="1002" alt="image" src="figures/moba.png">  |  [Github](https://github.com/OpenDFM/MobA/) <br/> [Paper](https://aclanthology.org/2025.naacl-demo.43/)  |
| OS-ATLAS: A FOUNDATION ACTION MODEL FOR  GENERALIST GUI AGENTS (2024-10) |   <img width="1002" alt="image" src="figures/osatlas.png">   | [Github](https://github.com/OS-Copilot/OS-Atlas) <br/> [Paper](https://arxiv.org/abs/2410.23218) |
| AutoGLM: Autonomous Foundation Agents for GUIs (2024-10)     |   <img width="1002" alt="image" src="figures/autoglm.png">   |  [Github]() <br/> [Paper](https://arxiv.org/abs/2411.00820)  |
| FERRET-UI 2: MASTERING UNIVERSAL USER INTERFACE UNDERSTANDING ACROSS PLATFORMS (2024-10) |  <img width="1002" alt="image" src="figures/ferretui2.png">  | [Github](https://github.com/apple/ml-ferret) <br/> [Paper](https://arxiv.org/abs/2410.18967) |
| AutoWebGLM: A Large Language Model-based Web Navigating Agent  (2024-10) | <img width="1002" alt="image" src="figures/autowebglm.png">  | [Github](https://github.com/THUDM/AutoWebGLM) <br/> [Paper](https://arxiv.org/abs/2404.03648) |
| AGENT S: AN OPEN AGENTIC FRAMEWORK THAT  USES COMPUTERS LIKE A HUMAN  (2024-10) |   <img width="1002" alt="image" src="figures/agentS.png">    | [Github](https://github.com/simular-ai/Agent-S) <br/> [Paper](https://arxiv.org/abs/2410.08164) |
| Navigating the Digital World as Humans Do:  UNIVERSAL VISUAL GROUNDING FOR GUI AGENTS (2024-10) |   <img width="1002" alt="image" src="figures/uground.png">   | [Github](https://github.com/OSU-NLP-Group/UGround) <br/> [Paper](https://arxiv.org/abs/2410.05243) |
| MOBILEFLOW: A MULTIMODAL LLM FOR MOBILE GUI AGENT (2024-8)   | <img width="1002" alt="image" src="figures/mobileflow.png">  |  [Github]() <br/> [Paper](https://arxiv.org/abs/2407.04346)  |
| AppAgent v2: Advanced Agent for Flexible Mobile Interactions (2024-8) | <img width="1002" alt="image" src="figures/appagentv2.png">  | [Github](https://github.com/TencentQQGYLab/AppAgent) <br/> [Paper](https://arxiv.org/abs/2408.11824) |
| OmniParser for Pure Vision Based GUI Agent (2024-8)          | <img width="1002" alt="image" src="figures/omniparser.png">  | [Github](https://github.com/microsoft/OmniParser) <br/> [Paper](https://arxiv.org/abs/2408.00203) |
| OmniACT: A Dataset and Benchmark for Enabling  Multimodal Generalist Autonomous Agents for Desktop (2024-7) |   <img width="1002" alt="image" src="figures/omniact.png">   |  [Github]() <br/> [Paper](https://arxiv.org/abs/2402.17553)  |
| Android in the Zoo: Chain-of-Action-Thought for GUI Agents (2024-7) |    <img width="1002" alt="image" src="figures/aitz.png">     | [Github](https://github.com/IMNearth/CoAT) <br/> [Paper](https://arxiv.org/abs/2403.02713) |
| CRADLE: Empowering Foundation Agents Towards General Computer Control (2024-7) |   <img width="1002" alt="image" src="figures/cradle.png">    | [Github](https://github.com/BAAI-Agents/Cradle) <br/> [Paper](https://arxiv.org/abs/2403.03186) |
| VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning (2024-6) |     <img width="1002" alt="image" src="figures/vga.png">     | [Github](https://github.com/Linziyang1999/Vision-GUI-assistant) <br/> [Paper](https://arxiv.org/abs/2406.14056) |
| WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models (2024-6) | <img width="1002" alt="image" src="figures/webvoyager.png">  | [Github](https://github.com/MinorJerry/WebVoyager) <br/> [Paper](https://arxiv.org/abs/2401.13919) |
| Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration (2024-6) | <img width="1002" alt="image" src="figures/mobileagent-v2.png"> | [Github](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v2) <br/> [Paper](https://arxiv.org/abs/2406.01014) |
| SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering (2024-5) |  <img width="1002" alt="image" src="figures/sweagent.png">   | [Github](https://github.com/SWE-agent/SWE-agent) <br/> [Paper](https://arxiv.org/abs/2405.15793) |
| UFO : A UI-Focused Agent for Windows OS Interaction (2024-5) |     <img width="1002" alt="image" src="figures/ufo.png">     | [Github](https://github.com/microsoft/UFO) <br/> [Paper](https://arxiv.org/abs/2402.07939) |
| MOBILE-AGENT: AUTONOMOUS MULTI-MODAL MOBILE  DEVICE AGENT WITH VISUAL PERCEPTION (2024-4) | <img width="1002" alt="image" src="figures/mobile-agent.png"> | [Github](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent) <br/> [Paper](https://arxiv.org/abs/2401.16158) |
| WebArena: A REALISTIC WEB ENVIRONMENT FOR  BUILDING AUTONOMOUS AGENTS (2024-4) |  <img width="1002" alt="image" src="figures/webarena.png">   | [Github](https://github.com/web-arena-x/webarena) <br/> [Paper](https://arxiv.org/abs/2307.13854) |
| TRAINING A VISION LANGUAGE MODEL AS  SMARTPHONE ASSISTANT (2024-4) |  <img width="1002" alt="image" src="figures/tdecison.png">   |  [Github]() <br/> [Paper](https://arxiv.org/abs/2404.08755)  |
| GPT-4V(ision) is a Generalist Web Agent, if Grounded (SeeAct)(2024-3) | <img width="1002" alt="image" src="figures/gpt4vgrounded.png"> | [Github](https://github.com/OSU-NLP-Group/SeeAct) <br/> [Paper](https://arxiv.org/abs/2401.01614) |
| AutoDroid: LLM-powered Task Automation in Android (2024-3)   |  <img width="1002" alt="image" src="figures/autodroid.png">  | [Github](https://github.com/MobileLLM/AutoDroid) <br/> [Paper](https://arxiv.org/abs/2308.15272) |
| SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents (2024-2) |  <img width="1002" alt="image" src="figures/seeclick.png">   | [Github](https://github.com/njucckevin/SeeClick) <br/> [Paper](https://arxiv.org/abs/2401.10935) |
| OS-Copilot: Towards Generalist Computer Agents with Self-Improvement (2024-2) |  <img width="1002" alt="image" src="figures/oscopilot.png">  | [Github](https://github.com/OS-Copilot/OS-Copilot) <br/> [Paper](https://arxiv.org/abs/2402.07456) |
| Understanding the Weakness of Large Language Model Agents within a Complex Android Environment (2024-2) | <img width="1002" alt="image" src="figures/scalable_mobile.png"> |  [Github]() <br/> [Paper](https://arxiv.org/abs/2402.06596)  |
| MOBILEAGENT: ENHANCING MOBILE CONTROL VIA  HUMAN-MACHINE INTERACTION AND SOP INTEGRATION (2024-1) | <img width="1002" alt="image" src="figures/mobileagent.png"> |  [Github]() <br/> [Paper](https://arxiv.org/abs/2401.04124)  |
| ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation (2024-1) |  <img width="1002" alt="image" src="figures/assistgui.png">  | [Github](https://github.com/showlab/assistgui) <br/> [Paper](https://arxiv.org/abs/2312.13108) |
| AppAgent: Multimodal Agents as Smartphone Users Ôºà2023-12Ôºâ  |  <img width="1002" alt="image" src="figures/appagent.png">   | [Github](https://github.com/TencentQQGYLab/AppAgent) <br/> [Paper](https://arxiv.org/abs/2312.13771) |
| CogAgent: A Visual Language Model for GUI Agents Ôºà2023-12Ôºâ | <img width="650" height="450" alt="image" src="figures/cogagent.png"> | [Github](https://github.com/THUDM/CogAgent) <br/> [Paper](https://arxiv.org/abs/2312.08914) |
| MIND2WEB: Towards a Generalist Agent for the Web Ôºà2023-12Ôºâ | <img width="1002" height="280" alt="image" src="figures/mind2web.png"> | [Github](https://github.com/OSU-NLP-Group/Mind2Web) <br/> [Paper](https://arxiv.org/abs/2306.06070) |
| Set-of-Mark Prompting Unleashes  Extraordinary Visual Grounding in GPT-4V Ôºà2023-11Ôºâ | <img width="1002" height="350" alt="image" src="figures/som.png"> | [Github](https://github.com/microsoft/SoM) <br/> [Paper](https://arxiv.org/abs/2310.11441) |
| META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI (2022-12) |<img width="1002" alt="image" src="figures/metagui.png"> |[Github](https://github.com/X-LANCE/META-GUI-baseline) <br/> [Paper](https://arxiv.org/abs/2205.11029)|
|UIBert: Learning Generic Multimodal Representations for UI Understanding (2021-8) |<img width="1002" alt="image" src="figures/UIBert.png"> |[Github](https://github.com/google-research-datasets/uibert) <br> [Paper](https://arxiv.org/abs/2107.13731)|


### [GUI Agents  Datasets & Benchmark List](#datasets)
| Title & Time                                                 |                         Introduction                         |                            Links                             |
| :----------------------------------------------------------- | :----------------------------------------------------------: | :----------------------------------------------------------: |
| UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis Ôºà2025-04Ôºâ | ![image-20250323214650793](figures/UI-E2I-Synth.jpg) | [Github](https://colmon46.github.io/i2e-bench-leaderboard/) <br> [Paper](https://arxiv.org/abs/2504.11257) |
| Ui-vision: A desktop-centric gui benchmark for visual perception and interaction Ôºà2025-03Ôºâ | ![image-20250323214650793](figures/image-20250323214650793.png) | [Github](https://uivision.github.io/) <br> [Paper](https://arxiv.org/abs/2503.15661) |
| Worldgui: Dynamic testing for comprehensive desktop gui automation Ôºà2025-02Ôºâ | ![image-20250323214915260](figures/image-20250323214915260.png) | [Github](https://github.com/showlab/WorldGUI) <br/> [Paper](https://arxiv.org/abs/2502.08047) |
| Screenspot-pro: Gui grounding for professional high-resolution computer use Ôºà2025-01Ôºâ | ![image-20250323215325569](figures/image-20250323215325569.png) | [Github](https://github.com/likaixin2000/ScreenSpot-Pro-GUI-Grounding) <br/> [Paper](https://openreview.net/forum?id=XaKNDIAHas&referrer=%5Bthe%20profile%20of%20Tat-Seng%20Chua%5D(%2Fprofile%3Fid%3D~Tat-Seng_Chua2)) |
| Webwalker: Benchmarking llms in web traversal Ôºà2025-01Ôºâ    | ![image-20250323215453818](figures/image-20250323215453818.png) | [Github](https://github.com/Alibaba-NLP/WebWalker) <br/> [Paper](https://arxiv.org/abs/2501.07572) |
| A3: Android agent arena for mobile gui agents Ôºà2025-01Ôºâ    | ![image-20250323215608637](figures/image-20250323215608637.png) | [Github](https://yuxiangchai.github.io/Android-Agent-Arena/) <br/> [Paper](https://arxiv.org/abs/2501.01149) |
| Gui testing arena: A unified benchmark for advancing autonomous gui testing agent Ôºà2024-12Ôºâ | ![image-20250323215712991](figures/image-20250323215712991.png) | [Github](https://github.com/ZJU-ACES-ISE/ChatUITest) <br/> [Paper](https://arxiv.org/abs/2412.18426) |
| Harnessing web page uis for text-rich visual understanding Ôºà2024-11Ôºâ | ![image-20250323215852537](figures/image-20250323215852537.png) | [Github](https://neulab.github.io/MultiUI/) <br/> [Paper](https://arxiv.org/abs/2410.13824) |
| On the effects of data scale on computer control agents Ôºà2024-11Ôºâ | ![image-20250323220022318](figures/image-20250323220022318.png) | [Github](https://github.com/google-research/google-research/tree/master/android_control) <br/> [Paper](https://arxiv.org/abs/2406.03679) |
| AndroidLab: training and systematic benchmarking of android autonomous agentsÔºà2024-10Ôºâ | ![image-20250323220426896](figures/image-20250323220426896.png) | [Github](https://github.com/THUDM/Android-Lab) <br/> [Paper](https://arxiv.org/abs/2410.24024) |
| Spa-bench: A comprehensive benchmark for smartphone agent evaluation Ôºà2024-10Ôºâ | ![image-20250323220521256](figures/image-20250323220521256.png) | [Github](https://ai-agents-2030.github.io/SPA-Bench/) <br/> [Paper](https://arxiv.org/abs/2410.15164) |
| Read anywhere pointed: Layout-aware gui screen reading with tree of-lens grounding Ôºà2024-10Ôºâ | ![image-20250323220623487](figures/image-20250323220623487.png) | [Github](https://screen-point-and-read.github.io/) <br/> [Paper](https://arxiv.org/abs/2406.19263) |
| Crab: Cross environment agent benchmark for multimodal lan guage model agents Ôºà2024-10Ôºâ | ![image-20250323220725459](figures/image-20250323220725459.png) | [Github](https://github.com/camel-ai/crab) <br/> [Paper](https://arxiv.org/abs/2407.01511) |
| Androidworld: A dynamic benchmarking environment for autonomous agents Ôºà2024-10Ôºâ | ![image-20250323220826783](figures/image-20250323220826783.png) | [Github](http://github.com/google-research/android_world) <br/> [Paper](https://arxiv.org/abs/2405.14573) |
| Benchmarking mobile device control agents across diverse configurations Ôºà2024-10Ôºâ | ![image-20250323220924295](figures/image-20250323220924295.png) | [Github](https://b-moca.github.io/) <br/> [Paper](https://arxiv.org/abs/2404.16660) |
| Agentstudio: A toolkit for building general virtual agents Ôºà2024-10Ôºâ | ![image-20250323221028583](figures/image-20250323221028583.png) | [Github](https://ltzheng.github.io/agent-studio) <br/> [Paper](https://arxiv.org/abs/2403.17918) |
| Weblinx: Real world website navigation with multi-turn dialogue Ôºà2024-10Ôºâ | ![image-20250323221129917](figures/image-20250323221129917.png) | [Github](https://mcgill-nlp.github.io/weblinx) <br/> [Paper](https://arxiv.org/abs/2402.05930) |
| Windows agent arena: Evaluating multi-modal os agents at scale Ôºà2024-09Ôºâ | ![image-20250323221233227](figures/image-20250323221233227.png) | [Github](https://github.com/microsoft/WindowsAgentArena) <br/> [Paper](https://arxiv.org/abs/2409.08264) |
| Understanding the weakness of large lan guage model agents within a complex android en vironment Ôºà2024-09Ôºâ | ![image-20250323221337948](figures/image-20250323221337948.png) | [Github](https://github.com/AndroidArenaAgent/AndroidArena) <br/> [Paper](https://arxiv.org/abs/2402.06596) |
| Llamatouch: A faithful and scalable testbed for mobile ui automation task evaluation Ôºà2024-08Ôºâ | ![image-20250323221544034](figures/image-20250323221544034.png) | [Github](https://github.com/LlamaTouch/LlamaTouch) <br/> [Paper](https://arxiv.org/abs/2404.16054) |
| Amex: Android multi annotation expo dataset for mobile gui agents Ôºà2024-07Ôºâ | ![image-20250323221638935](figures/image-20250323221638935.png) | [Github](https://yuxiangchai.github.io/AMEX/) <br/> [Paper](https://arxiv.org/abs/2407.17490) |
| Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web Ôºà2024-07Ôºâ | ![image-20250323221809234](figures/image-20250323221809234.png) | [HuggingFace](https://huggingface.co/datasets/Writer/omniact) <br/>[Paper](https://arxiv.org/abs/2402.17553) |
| Spider2-v: Howfar are multimodal agents from automating data science and engineering workflows? Ôºà2024-07Ôºâ | ![image-20250323222017862](figures/image-20250323222017862.png) | [Github](https://spider2-v.github.io/) <br/> [Paper](https://arxiv.org/abs/2407.10956) |
| Webcanvas: Benchmarking web agents in online environments Ôºà2024-07Ôºâ | ![image-20250323222550072](figures/image-20250323222550072.png) | [HuggingFace](https://huggingface.co/datasets/iMeanAI/Mind2Web-Live) <br/> [Paper](https://arxiv.org/abs/2406.12373) |
| Gui odyssey: A comprehensive dataset for cross-app gui navigation on mobile devices Ôºà2024-06Ôºâ | ![image-20250323222659185](figures/image-20250323222659185.png) | [Github](https://github.com/OpenGVLab/GUI-Odyssey) <br/> [Paper](https://arxiv.org/abs/2406.08451) |
| Gui-world: A dataset for gui-oriented multimodal llm-based agents Ôºà2024-06Ôºâ | ![image-20250323222816594](figures/image-20250323222816594.png) | [Github](https://gui-world.github.io/) <br/> [Paper](https://arxiv.org/abs/2406.10819) |
| Guicourse: From general vision language models to versatile gui agents Ôºà2024-06Ôºâ | ![image-20250323222911404](figures/image-20250323222911404.png) | [Github](https://github.com/yiye3/GUICourse) <br/> [Paper](https://arxiv.org/abs/2406.11317) |
| Mobileagentbench: An efficient and user-friendly benchmark for mobile llm agents Ôºà2024-06Ôºâ | ![image-20250323223105886](figures/image-20250323223105886.png) | [Github](https://mobileagentbench.github.io/) <br/> [Paper](https://arxiv.org/abs/2406.08184) |
| Videogui: A benchmark for gui automation from instructional videos Ôºà2024-06Ôºâ | ![image-20250323223251691](figures/image-20250323223251691.png) | [Github](https://showlab.github.io/videogui/) <br/> [Paper](https://arxiv.org/abs/2406.10227) |
| Visualwebarena: Evaluating multimodal agents on realistic visual web tasks Ôºà2024-06Ôºâ | ![image-20250323223336436](figures/image-20250323223336436.png) | [HomePage](https://jykoh.com/vwa) <br/> [Paper](https://arxiv.org/abs/2401.13649) |
| Mobile-env: an evaluation platform and benchmark for llm-gui interaction Ôºà2024-06Ôºâ | ![image-20250323223530101](figures/image-20250323223530101.png) | [Github](https://github.com/X-LANCE/Mobile-Env) <br/> [Paper](https://arxiv.org/abs/2305.08144) |
| Os world: Benchmarking multimodal agents for open ended tasks in real computer environments Ôºà2024-05Ôºâ | ![image-20250323223649729](figures/image-20250323223649729.png) | [Github](https://os-world.github.io/) <br/> [Paper](https://arxiv.org/abs/2404.07972) |
| Visualwebbench: How far have multi modal llms evolved in web page understanding and grounding? Ôºà2024-04Ôºâ | ![image-20250323223858931](figures/image-20250323223858931.png) | [Github](https://github.com/microsoft/LMOps/tree/main/minillm) <br/> [Paper](https://arxiv.org/abs/2404.05955) |
| Mmina: Benchmarking multihop multimodal internet agents Ôºà2024-04Ôºâ | ![image-20250323223839290](figures/image-20250323223839290.png) | [HomePage](https://mmina.cliangyu.com/) <br/> [Paper](https://arxiv.org/abs/2404.09992) |
| Webarena: Arealistic web environment for building autonomous agents Ôºà2024-04Ôºâ | ![image-20250323224000892](figures/image-20250323224000892.png) | [HomePage](https://webarena.dev/) <br> [Paper](https://arxiv.org/abs/2307.13854) |
| Webvln: Vision-and-language navigation on websites Ôºà2024-03Ôºâ | ![image-20250323224126973](figures/image-20250323224126973.png) | [Github](https://github.com/WebVLN/WebVLN) <br/> [Paper](https://arxiv.org/abs/2312.15820) |
| On the multi-turn instruction fol lowing for conversational web agents Ôºà2024-02Ôºâ | ![image-20250323224234089](figures/image-20250323224234089.png) | [Github](https://github.com/magicgh/self-map) <br/> [Paper](https://arxiv.org/abs/2402.15057) |
| Assist gui: Task-oriented desktop graphical user interface automation Ôºà2024-01Ôºâ | ![image-20250323224337066](figures/image-20250323224337066.png) | [Github](https://showlab.github.io/assistgui/) <br> [Paper](https://arxiv.org/abs/2312.13108) |
| Mind2web: Towards a generalist agent for the web Ôºà2023-12Ôºâ | ![image-20250323224440974](figures/image-20250323224440974.png) | [Github](https://osu-nlp-group.github.io/Mind2Web) <br/> [Paper](https://arxiv.org/abs/2306.06070) |
| Androidinthewild: A large-scale dataset for an droid device control Ôºà2023-10Ôºâ | ![image-20250323224529268](figures/image-20250323224529268.png) | [Github](https://github.com/google-research/google-research/tree/master/android_in_the_wild) <br> [Paper](https://arxiv.org/abs/2307.10088) |
| Web shop: Towards scalable real-world web interaction with grounded language agents Ôºà2023-02Ôºâ | ![image-20250323224619855](figures/image-20250323224619855.png) | [Github](https://webshop-pnlp.github.io/) <br/> [Paper](https://arxiv.org/abs/2207.01206) |
| Meta-gui: Towards multi-modal con versational agents on mobile gui Ôºà2022-11Ôºâ | ![image-20250323224706852](figures/image-20250323224706852.png) | [Github](https://x-lance.github.io/META-GUI-Leaderboard/) <br> [Paper](https://arxiv.org/abs/2205.11029) |
| A dataset for interactive vision language navigation with unknown command fea sibility Ôºà2022-08Ôºâ | ![image-20250323224832001](figures/image-20250323224832001.png) | [Github](https://github.com/aburns4/MoTIF) <br/> [Paper](https://arxiv.org/abs/2202.02312) |
| Websrc: A dataset for web-based structural reading comprehension Ôºà2021-11Ôºâ | ![image-20250323224916473](figures/image-20250323224916473.png) | [Github](https://x-lance.github.io/WebSRC/) <br/> [Paper](https://arxiv.org/abs/2101.09465) |
| Mapping natural language instructions to mobile ui action sequences Ôºà2020-06Ôºâ | ![image-20250323225018260](figures/image-20250323225018260.png) | [Github](https://github.com/google-research/google-research/tree/master/seq2act) <br/> [Paper](https://arxiv.org/abs/2005.03776) |
| Reinforcement learning on web interfaces using workflow-guided exploration Ôºà2018-02Ôºâ | ![image-20250323225104521](figures/image-20250323225104521.png) | [Github](https://github.com/stanfordnlp/wge) <br/> [Paper](https://arxiv.org/abs/1802.08802) |
| Rico: A mobile app dataset for building data-driven design applications Ôºà2017-10Ôºâ | ![image-20250323225453702](figures/image-20250323225453702.png) | [HomePage](http://interactionmining.org/rico) <br/> [Paper](https://dl.acm.org/doi/10.1145/3126594.3126651) |
| World of bits: An open-domain platform for web based agents Ôºà2017-08Ôºâ | ![image-20250323225547948](figures/image-20250323225547948.png) |    [Paper](https://proceedings.mlr.press/v70/shi17a.html)    |
