# Awesome-GUI-Agents
A curated list for **GUI Agents**

**Table Content:** 

- [Updates](#updates) 
- [Contributing](#contri) 
- [Four Module of GUI Agents](#module)
- [Paper List](#paper-list) 
  - [GUI Agents List](#agentlist) 
  - [GUI Agents Datasets & Benchmark List](#datasets) 

![Alt text](figures/overview.png)

## ðŸš€ [Updates](#updates) 
* April 22, 2025: We updated GUI Agents List based on RL (R1 Style).
  
* April 2, 2025: We have already uploaded the paper to arXiv, please wait for some time. Meanwhile, we will keep updating this repo.

* March 24, 2025: We updated the repository and released our comprehensive survey on GUI Agents.

## ðŸ’® [Contributing](#contri)

If you'd like to include your paper, or need to update any details such as github repo information or code URLs, please feel free to submit a pull request. 

## [Four Module of GUI Agents](#module)

We have divided GUI Agents into four modules: perception, exploration, planning, and interaction, as shown below:

![Alt text](figures/module.jpg)

## [Paper List](#paperlist)

We have dedicated a separate chapter to datasets and benchmarks for GUI Agents, with all content presented in chronological order.

### R1-Style GUI Agents
2025-4-19: InfiGUI-R1: Advancing Multimodal GUI Agents from Reactive Actors to Deliberative Reasoners
2025-4-14: GUI-R1 : A Generalist R1-Style Vision-Language Action Model For GUI Agents
2025-3-27: UI-R1: Enhancing Action Prediction of GUI Agents by Reinforcement Learning

### [GUI Agents List](#agentlist)
| Title & Time | Introduction | Links |
|:--|  :----: | :---:|
| Advancing Mobile GUI Agents: A Verifier-Driven  Approach to Practical Deployment (2025-3) | <img width="1002" alt="image" src="figures/verifiergui.png" > | [Github]() <br/> [Paper](https://www.arxiv.org/abs/2503.15937) |
| API Agents vs. GUI Agents: Divergence and Convergence (2025-3) | <img width="1002" alt="image" src="figures/apivsgui.png" > | [Github]() <br/> [Paper](https://arxiv.org/abs/2503.11069) |
| AppAgentX: Evolving GUI Agents as Proficient Smartphone Users (2025-3) | <img width="1002" alt="image" src="figures/appagentx.png" > | [Github](https://github.com/Westlake-AGI-Lab/AppAgentX) <br/> [Paper](https://arxiv.org/pdf/2503.02268) |
| Think Twice, Click Once: Enhancing GUI Grounding via Fast and Slow Systems (2025-3) | <img width="1002" alt="image" src="figures/focus.png" > | [Github](https://github.com/sugarandgugu/Focus) <br/> [Paper](https://arxiv.org/pdf/2503.06470) |
| UI-TARS: Pioneering Automated GUI Interaction with Native Agents  (2025-2) | <img width="1002" alt="image" src="figures/uitars.png" > | [Github](https://github.com/bytedance/UI-TARS) <br/> [Paper](https://arxiv.org/abs/2501.12326) |
| Mobile-Agent-E: Self-Evolving Mobile Assistant for Complex Tasks (2025-1) | <img width="1002" alt="image" src="figures/mobileagente.png" > | [Github](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-E) <br/> [Paper](https://arxiv.org/abs/2501.11733) |
| InfiGUIAgent: A Multimodal Generalist GUI Agent with Native Reasoning and Reflection (2025-1) | <img width="1002" alt="image" src="figures/infiagent.png" > | [Github](https://github.com/Reallm-Labs/InfiGUIAgent) <br/> [Paper](https://arxiv.org/abs/2501.04575) |
| OS-Genesis: Automating GUI Agent Trajectory Construction via  Reverse Task Synthesis (2024-12) | <img width="1002" alt="image" src="figures/osgenesis.png" > | [Github](https://github.com/OS-Copilot/OS-Genesis) <br/> [Paper](https://arxiv.org/abs/2412.19723) |
| PC Agent: While You Sleep, AI Works - A Cognitive Journey into Digital World (2024-12) | <img width="1002" alt="image" src="figures/pcagent.png" > | [Github](https://github.com/GAIR-NLP/PC-Agent) <br/> [Paper](https://arxiv.org/abs/2412.17589) |
| Aria-UI: Visual Grounding for GUI Instructions (2024-12) | <img width="1002" alt="image" src="figures/ariaui.png" > | [Github](https://github.com/AriaUI/Aria-UI) <br/> [Paper](https://arxiv.org/abs/2412.16256) |
| Iris: Breaking GUI Complexity with Adaptive Focus and Self-Refining (2024-12) | <img width="1002" alt="image" src="figures/iris.png" > | [Github]() <br/> [Paper](https://arxiv.org/abs/2412.10342) |
| AgentTrek : AGENT TRAJECTORY SYNTHESIS  VIA GUIDING REPLAY WITH WEB TUTORIALS (2024-12) | <img width="1002" alt="image" src="figures/agenttrek.png" > | [Github](https://github.com/xlang-ai/AgentTrek) <br/> [Paper](https://arxiv.org/abs/2412.09605) |
| AGUVIS: UNIFIED PURE VISION AGENTS FOR  AUTONOMOUS GUI INTERACTION (2024-12) | <img width="1002" alt="image" src="figures/aguvis.png"> | [Github](https://github.com/xlang-ai/aguvis) <br/> [Paper](https://arxiv.org/abs/2412.04454) |
| Ponder & Press: Advancing Visual GUI Agent  towards General Computer Control (2024-12) |   <img width="1002" alt="image" src="figures/ponder.png">    |  [Github]() <br/> [Paper](https://arxiv.org/abs/2412.01268)  |
| ShowUI: One Vision-Language-Action Model for GUI Visual Agent (2024-11) |   <img width="1002" alt="image" src="figures/showui.png">    | [Github](https://github.com/showlab/ShowUI) <br/> [Paper](https://arxiv.org/abs/2411.17465) |
| The Dawn of GUI Agent: A Preliminary  Case Study with Claude 3.5 Computer Use (2024-11) |  <img width="1002" alt="image" src="figures/claude3.5.png">  |  [Github]() <br/> [Paper](https://arxiv.org/abs/2411.10323)  |
| OS-ATLAS: A FOUNDATION ACTION MODEL FOR  GENERALIST GUI AGENTS (2024-10) |   <img width="1002" alt="image" src="figures/osatlas.png">   | [Github](https://github.com/OS-Copilot/OS-Atlas) <br/> [Paper](https://arxiv.org/abs/2410.23218) |
| AutoGLM: Autonomous Foundation Agents for GUIs (2024-10)     |   <img width="1002" alt="image" src="figures/autoglm.png">   |  [Github]() <br/> [Paper](https://arxiv.org/abs/2411.00820)  |
| FERRET-UI 2: MASTERING UNIVERSAL USER INTERFACE UNDERSTANDING ACROSS PLATFORMS (2024-10) |  <img width="1002" alt="image" src="figures/ferretui2.png">  | [Github](https://github.com/apple/ml-ferret) <br/> [Paper](https://arxiv.org/abs/2410.18967) |
| AutoWebGLM: A Large Language Model-based Web Navigating Agent  (2024-10) | <img width="1002" alt="image" src="figures/autowebglm.png">  | [Github](https://github.com/THUDM/AutoWebGLM) <br/> [Paper](https://arxiv.org/abs/2404.03648) |
| AGENT S: AN OPEN AGENTIC FRAMEWORK THAT  USES COMPUTERS LIKE A HUMAN  (2024-10) |   <img width="1002" alt="image" src="figures/agentS.png">    | [Github](https://github.com/simular-ai/Agent-S) <br/> [Paper](https://arxiv.org/abs/2410.08164) |
| Navigating the Digital World as Humans Do:  UNIVERSAL VISUAL GROUNDING FOR GUI AGENTS (2024-10) |   <img width="1002" alt="image" src="figures/uground.png">   | [Github](https://github.com/OSU-NLP-Group/UGround) <br/> [Paper](https://arxiv.org/abs/2410.05243) |
| MOBILEFLOW: A MULTIMODAL LLM FOR MOBILE GUI AGENT (2024-8)   | <img width="1002" alt="image" src="figures/mobileflow.png">  |  [Github]() <br/> [Paper](https://arxiv.org/abs/2407.04346)  |
| AppAgent v2: Advanced Agent for Flexible Mobile Interactions (2024-8) | <img width="1002" alt="image" src="figures/appagentv2.png">  | [Github](https://github.com/TencentQQGYLab/AppAgent) <br/> [Paper](https://arxiv.org/abs/2408.11824) |
| OmniParser for Pure Vision Based GUI Agent (2024-8)          | <img width="1002" alt="image" src="figures/omniparser.png">  | [Github](https://github.com/microsoft/OmniParser) <br/> [Paper](https://arxiv.org/abs/2408.00203) |
| OmniACT: A Dataset and Benchmark for Enabling  Multimodal Generalist Autonomous Agents for Desktop (2024-7) |   <img width="1002" alt="image" src="figures/omniact.png">   |  [Github]() <br/> [Paper](https://arxiv.org/abs/2402.17553)  |
| Android in the Zoo: Chain-of-Action-Thought for GUI Agents (2024-7) |    <img width="1002" alt="image" src="figures/aitz.png">     | [Github](https://github.com/IMNearth/CoAT) <br/> [Paper](https://arxiv.org/abs/2403.02713) |
| CRADLE: Empowering Foundation Agents Towards General Computer Control (2024-7) |   <img width="1002" alt="image" src="figures/cradle.png">    | [Github](https://github.com/BAAI-Agents/Cradle) <br/> [Paper](https://arxiv.org/abs/2403.03186) |
| VGA: Vision GUI Assistant - Minimizing Hallucinations through Image-Centric Fine-Tuning (2024-6) |     <img width="1002" alt="image" src="figures/vga.png">     | [Github](https://github.com/Linziyang1999/Vision-GUI-assistant) <br/> [Paper](https://arxiv.org/abs/2406.14056) |
| WebVoyager: Building an End-to-End Web Agent with Large Multimodal Models (2024-6) | <img width="1002" alt="image" src="figures/webvoyager.png">  | [Github](https://github.com/MinorJerry/WebVoyager) <br/> [Paper](https://arxiv.org/abs/2401.13919) |
| Mobile-Agent-v2: Mobile Device Operation Assistant with Effective Navigation via Multi-Agent Collaboration (2024-6) | <img width="1002" alt="image" src="figures/mobileagent-v2.png"> | [Github](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent-v2) <br/> [Paper](https://arxiv.org/abs/2406.01014) |
| SWE-agent: Agent-Computer Interfaces Enable Automated Software Engineering (2024-5) |  <img width="1002" alt="image" src="figures/sweagent.png">   | [Github](https://github.com/SWE-agent/SWE-agent) <br/> [Paper](https://arxiv.org/abs/2405.15793) |
| UFO : A UI-Focused Agent for Windows OS Interaction (2024-5) |     <img width="1002" alt="image" src="figures/ufo.png">     | [Github](https://github.com/microsoft/UFO) <br/> [Paper](https://arxiv.org/abs/2402.07939) |
| MOBILE-AGENT: AUTONOMOUS MULTI-MODAL MOBILE  DEVICE AGENT WITH VISUAL PERCEPTION (2024-4) | <img width="1002" alt="image" src="figures/mobile-agent.png"> | [Github](https://github.com/X-PLUG/MobileAgent/tree/main/Mobile-Agent) <br/> [Paper](https://arxiv.org/abs/2401.16158) |
| WebArena: A REALISTIC WEB ENVIRONMENT FOR  BUILDING AUTONOMOUS AGENTS (2024-4) |  <img width="1002" alt="image" src="figures/webarena.png">   | [Github](https://github.com/web-arena-x/webarena) <br/> [Paper](https://arxiv.org/abs/2307.13854) |
| TRAINING A VISION LANGUAGE MODEL AS  SMARTPHONE ASSISTANT (2024-4) |  <img width="1002" alt="image" src="figures/tdecison.png">   |  [Github]() <br/> [Paper](https://arxiv.org/abs/2404.08755)  |
| GPT-4V(ision) is a Generalist Web Agent, if Grounded (SeeAct)(2024-3) | <img width="1002" alt="image" src="figures/gpt4vgrounded.png"> | [Github](https://github.com/OSU-NLP-Group/SeeAct) <br/> [Paper](https://arxiv.org/abs/2401.01614) |
| AutoDroid: LLM-powered Task Automation in Android (2024-3)   |  <img width="1002" alt="image" src="figures/autodroid.png">  | [Github](https://github.com/MobileLLM/AutoDroid) <br/> [Paper](https://arxiv.org/abs/2308.15272) |
| SeeClick: Harnessing GUI Grounding for Advanced Visual GUI Agents (2024-2) |  <img width="1002" alt="image" src="figures/seeclick.png">   | [Github](https://github.com/njucckevin/SeeClick) <br/> [Paper](https://arxiv.org/abs/2401.10935) |
| OS-Copilot: Towards Generalist Computer Agents with Self-Improvement (2024-2) |  <img width="1002" alt="image" src="figures/oscopilot.png">  | [Github](https://github.com/OS-Copilot/OS-Copilot) <br/> [Paper](https://arxiv.org/abs/2402.07456) |
| Understanding the Weakness of Large Language Model Agents within a Complex Android Environment (2024-2) | <img width="1002" alt="image" src="figures/scalable_mobile.png"> |  [Github]() <br/> [Paper](https://arxiv.org/abs/2402.06596)  |
| MOBILEAGENT: ENHANCING MOBILE CONTROL VIA  HUMAN-MACHINE INTERACTION AND SOP INTEGRATION (2024-1) | <img width="1002" alt="image" src="figures/mobileagent.png"> |  [Github]() <br/> [Paper](https://arxiv.org/abs/2401.04124)  |
| ASSISTGUI: Task-Oriented Desktop Graphical User Interface Automation (2024-1) |  <img width="1002" alt="image" src="figures/assistgui.png">  | [Github](https://github.com/showlab/assistgui) <br/> [Paper](https://arxiv.org/abs/2312.13108) |
| AppAgent: Multimodal Agents as Smartphone Users ï¼ˆ2023-12ï¼‰  |  <img width="1002" alt="image" src="figures/appagent.png">   | [Github](https://github.com/TencentQQGYLab/AppAgent) <br/> [Paper](https://arxiv.org/abs/2312.13771) |
| CogAgent: A Visual Language Model for GUI Agents ï¼ˆ2023-12ï¼‰ | <img width="650" height="450" alt="image" src="figures/cogagent.png"> | [Github](https://github.com/THUDM/CogAgent) <br/> [Paper](https://arxiv.org/abs/2312.08914) |
| MIND2WEB: Towards a Generalist Agent for the Web ï¼ˆ2023-12ï¼‰ | <img width="1002" height="280" alt="image" src="figures/mind2web.png"> | [Github](https://github.com/OSU-NLP-Group/Mind2Web) <br/> [Paper](https://arxiv.org/abs/2306.06070) |
| Set-of-Mark Prompting Unleashes  Extraordinary Visual Grounding in GPT-4V ï¼ˆ2023-11ï¼‰ | <img width="1002" height="350" alt="image" src="figures/som.png"> | [Github](https://github.com/microsoft/SoM) <br/> [Paper](https://arxiv.org/abs/2310.11441) |
| META-GUI: Towards Multi-modal Conversational Agents on Mobile GUI (2022-12) |<img width="1002" alt="image" src="figures/metagui.png"> |[Github](https://github.com/X-LANCE/META-GUI-baseline) <br/> [Paper](https://arxiv.org/abs/2205.11029)|
|UIBert: Learning Generic Multimodal Representations for UI Understanding (2021-8) |<img width="1002" alt="image" src="figures/UIBert.png"> |[Github](https://github.com/google-research-datasets/uibert) <br> [Paper](https://arxiv.org/abs/2107.13731)|


### [GUI Agents  Datasets & Benchmark List](#datasets)
| Title & Time                                                 |                         Introduction                         |                            Links                             |
| :----------------------------------------------------------- | :----------------------------------------------------------: | :----------------------------------------------------------: |
| Ui-vision: A desktop-centric gui benchmark for visual perception and interaction ï¼ˆ2025-03ï¼‰ | ![image-20250323214650793](figures/image-20250323214650793.png) | [Github](https://uivision.github.io/) <br> [Paper](https://arxiv.org/abs/2503.15661) |
| Worldgui: Dynamic testing for comprehensive desktop gui automation ï¼ˆ2025-02ï¼‰ | ![image-20250323214915260](figures/image-20250323214915260.png) | [Github](https://github.com/showlab/WorldGUI) <br/> [Paper](https://arxiv.org/abs/2502.08047) |
| Screenspot-pro: Gui grounding for professional high-resolution computer use ï¼ˆ2025-01ï¼‰ | ![image-20250323215325569](figures/image-20250323215325569.png) | [Github](https://github.com/likaixin2000/ScreenSpot-Pro-GUI-Grounding) <br/> [Paper](https://openreview.net/forum?id=XaKNDIAHas&referrer=%5Bthe%20profile%20of%20Tat-Seng%20Chua%5D(%2Fprofile%3Fid%3D~Tat-Seng_Chua2)) |
| Webwalker: Benchmarking llms in web traversal ï¼ˆ2025-01ï¼‰    | ![image-20250323215453818](figures/image-20250323215453818.png) | [Github](https://github.com/Alibaba-NLP/WebWalker) <br/> [Paper](https://arxiv.org/abs/2501.07572) |
| A3: Android agent arena for mobile gui agents ï¼ˆ2025-01ï¼‰    | ![image-20250323215608637](figures/image-20250323215608637.png) | [Github](https://yuxiangchai.github.io/Android-Agent-Arena/) <br/> [Paper](https://arxiv.org/abs/2501.01149) |
| Gui testing arena: A unified benchmark for advancing autonomous gui testing agent ï¼ˆ2024-12ï¼‰ | ![image-20250323215712991](figures/image-20250323215712991.png) | [Github](https://github.com/ZJU-ACES-ISE/ChatUITest) <br/> [Paper](https://arxiv.org/abs/2412.18426) |
| Harnessing web page uis for text-rich visual understanding ï¼ˆ2024-11ï¼‰ | ![image-20250323215852537](figures/image-20250323215852537.png) | [Github](https://neulab.github.io/MultiUI/) <br/> [Paper](https://arxiv.org/abs/2410.13824) |
| On the effects of data scale on computer control agents ï¼ˆ2024-11ï¼‰ | ![image-20250323220022318](figures/image-20250323220022318.png) | [Github](https://github.com/google-research/google-research/tree/master/android_control) <br/> [Paper](https://arxiv.org/abs/2406.03679) |
| AndroidLab: training and systematic benchmarking of android autonomous agentsï¼ˆ2024-10ï¼‰ | ![image-20250323220426896](figures/image-20250323220426896.png) | [Github](https://github.com/THUDM/Android-Lab) <br/> [Paper](https://arxiv.org/abs/2410.24024) |
| Spa-bench: A comprehensive benchmark for smartphone agent evaluation ï¼ˆ2024-10ï¼‰ | ![image-20250323220521256](figures/image-20250323220521256.png) | [Github](https://ai-agents-2030.github.io/SPA-Bench/) <br/> [Paper](https://arxiv.org/abs/2410.15164) |
| Read anywhere pointed: Layout-aware gui screen reading with tree of-lens grounding ï¼ˆ2024-10ï¼‰ | ![image-20250323220623487](figures/image-20250323220623487.png) | [Github](https://screen-point-and-read.github.io/) <br/> [Paper](https://arxiv.org/abs/2406.19263) |
| Crab: Cross environment agent benchmark for multimodal lan guage model agents ï¼ˆ2024-10ï¼‰ | ![image-20250323220725459](figures/image-20250323220725459.png) | [Github](https://github.com/camel-ai/crab) <br/> [Paper](https://arxiv.org/abs/2407.01511) |
| Androidworld: A dynamic benchmarking environment for autonomous agents ï¼ˆ2024-10ï¼‰ | ![image-20250323220826783](figures/image-20250323220826783.png) | [Github](http://github.com/google-research/android_world) <br/> [Paper](https://arxiv.org/abs/2405.14573) |
| Benchmarking mobile device control agents across diverse configurations ï¼ˆ2024-10ï¼‰ | ![image-20250323220924295](figures/image-20250323220924295.png) | [Github](https://b-moca.github.io/) <br/> [Paper](https://arxiv.org/abs/2404.16660) |
| Agentstudio: A toolkit for building general virtual agents ï¼ˆ2024-10ï¼‰ | ![image-20250323221028583](figures/image-20250323221028583.png) | [Github](https://ltzheng.github.io/agent-studio) <br/> [Paper](https://arxiv.org/abs/2403.17918) |
| Weblinx: Real world website navigation with multi-turn dialogue ï¼ˆ2024-10ï¼‰ | ![image-20250323221129917](figures/image-20250323221129917.png) | [Github](https://mcgill-nlp.github.io/weblinx) <br/> [Paper](https://arxiv.org/abs/2402.05930) |
| Windows agent arena: Evaluating multi-modal os agents at scale ï¼ˆ2024-09ï¼‰ | ![image-20250323221233227](figures/image-20250323221233227.png) | [Github](https://github.com/microsoft/WindowsAgentArena) <br/> [Paper](https://arxiv.org/abs/2409.08264) |
| Understanding the weakness of large lan guage model agents within a complex android en vironment ï¼ˆ2024-09ï¼‰ | ![image-20250323221337948](figures/image-20250323221337948.png) | [Github](https://github.com/AndroidArenaAgent/AndroidArena) <br/> [Paper](https://arxiv.org/abs/2402.06596) |
| Llamatouch: A faithful and scalable testbed for mobile ui automation task evaluation ï¼ˆ2024-08ï¼‰ | ![image-20250323221544034](figures/image-20250323221544034.png) | [Github](https://github.com/LlamaTouch/LlamaTouch) <br/> [Paper](https://arxiv.org/abs/2404.16054) |
| Amex: Android multi annotation expo dataset for mobile gui agents ï¼ˆ2024-07ï¼‰ | ![image-20250323221638935](figures/image-20250323221638935.png) | [Github](https://yuxiangchai.github.io/AMEX/) <br/> [Paper](https://arxiv.org/abs/2407.17490) |
| Omniact: A dataset and benchmark for enabling multimodal generalist autonomous agents for desktop and web ï¼ˆ2024-07ï¼‰ | ![image-20250323221809234](figures/image-20250323221809234.png) | [HuggingFace](https://huggingface.co/datasets/Writer/omniact) <br/>[Paper](https://arxiv.org/abs/2402.17553) |
| Spider2-v: Howfar are multimodal agents from automating data science and engineering workflows? ï¼ˆ2024-07ï¼‰ | ![image-20250323222017862](figures/image-20250323222017862.png) | [Github](https://spider2-v.github.io/) <br/> [Paper](https://arxiv.org/abs/2407.10956) |
| Webcanvas: Benchmarking web agents in online environments ï¼ˆ2024-07ï¼‰ | ![image-20250323222550072](figures/image-20250323222550072.png) | [HuggingFace](https://huggingface.co/datasets/iMeanAI/Mind2Web-Live) <br/> [Paper](https://arxiv.org/abs/2406.12373) |
| Gui odyssey: A comprehensive dataset for cross-app gui navigation on mobile devices ï¼ˆ2024-06ï¼‰ | ![image-20250323222659185](figures/image-20250323222659185.png) | [Github](https://github.com/OpenGVLab/GUI-Odyssey) <br/> [Paper](https://arxiv.org/abs/2406.08451) |
| Gui-world: A dataset for gui-oriented multimodal llm-based agents ï¼ˆ2024-06ï¼‰ | ![image-20250323222816594](figures/image-20250323222816594.png) | [Github](https://gui-world.github.io/) <br/> [Paper](https://arxiv.org/abs/2406.10819) |
| Guicourse: From general vision language models to versatile gui agents ï¼ˆ2024-06ï¼‰ | ![image-20250323222911404](figures/image-20250323222911404.png) | [Github](https://github.com/yiye3/GUICourse) <br/> [Paper](https://arxiv.org/abs/2406.11317) |
| Mobileagentbench: An efficient and user-friendly benchmark for mobile llm agents ï¼ˆ2024-06ï¼‰ | ![image-20250323223105886](figures/image-20250323223105886.png) | [Github](https://mobileagentbench.github.io/) <br/> [Paper](https://arxiv.org/abs/2406.08184) |
| Videogui: A benchmark for gui automation from instructional videos ï¼ˆ2024-06ï¼‰ | ![image-20250323223251691](figures/image-20250323223251691.png) | [Github](https://showlab.github.io/videogui/) <br/> [Paper](https://arxiv.org/abs/2406.10227) |
| Visualwebarena: Evaluating multimodal agents on realistic visual web tasks ï¼ˆ2024-06ï¼‰ | ![image-20250323223336436](figures/image-20250323223336436.png) | [HomePage](https://jykoh.com/vwa) <br/> [Paper](https://arxiv.org/abs/2401.13649) |
| Mobile-env: an evaluation platform and benchmark for llm-gui interaction ï¼ˆ2024-06ï¼‰ | ![image-20250323223530101](figures/image-20250323223530101.png) | [Github](https://github.com/X-LANCE/Mobile-Env) <br/> [Paper](https://arxiv.org/abs/2305.08144) |
| Os world: Benchmarking multimodal agents for open ended tasks in real computer environments ï¼ˆ2024-05ï¼‰ | ![image-20250323223649729](figures/image-20250323223649729.png) | [Github](https://os-world.github.io/) <br/> [Paper](https://arxiv.org/abs/2404.07972) |
| Visualwebbench: How far have multi modal llms evolved in web page understanding and grounding? ï¼ˆ2024-04ï¼‰ | ![image-20250323223858931](figures/image-20250323223858931.png) | [Github](https://github.com/microsoft/LMOps/tree/main/minillm) <br/> [Paper](https://arxiv.org/abs/2404.05955) |
| Mmina: Benchmarking multihop multimodal internet agents ï¼ˆ2024-04ï¼‰ | ![image-20250323223839290](figures/image-20250323223839290.png) | [HomePage](https://mmina.cliangyu.com/) <br/> [Paper](https://arxiv.org/abs/2404.09992) |
| Webarena: Arealistic web environment for building autonomous agents ï¼ˆ2024-04ï¼‰ | ![image-20250323224000892](figures/image-20250323224000892.png) | [HomePage](https://webarena.dev/) <br> [Paper](https://arxiv.org/abs/2307.13854) |
| Webvln: Vision-and-language navigation on websites ï¼ˆ2024-03ï¼‰ | ![image-20250323224126973](figures/image-20250323224126973.png) | [Github](https://github.com/WebVLN/WebVLN) <br/> [Paper](https://arxiv.org/abs/2312.15820) |
| On the multi-turn instruction fol lowing for conversational web agents ï¼ˆ2024-02ï¼‰ | ![image-20250323224234089](figures/image-20250323224234089.png) | [Github](https://github.com/magicgh/self-map) <br/> [Paper](https://arxiv.org/abs/2402.15057) |
| Assist gui: Task-oriented desktop graphical user interface automation ï¼ˆ2024-01ï¼‰ | ![image-20250323224337066](figures/image-20250323224337066.png) | [Github](https://showlab.github.io/assistgui/) <br> [Paper](https://arxiv.org/abs/2312.13108) |
| Mind2web: Towards a generalist agent for the web ï¼ˆ2023-12ï¼‰ | ![image-20250323224440974](figures/image-20250323224440974.png) | [Github](https://osu-nlp-group.github.io/Mind2Web) <br/> [Paper](https://arxiv.org/abs/2306.06070) |
| Androidinthewild: A large-scale dataset for an droid device control ï¼ˆ2023-10ï¼‰ | ![image-20250323224529268](figures/image-20250323224529268.png) | [Github](https://github.com/google-research/google-research/tree/master/android_in_the_wild) <br> [Paper](https://arxiv.org/abs/2307.10088) |
| Web shop: Towards scalable real-world web interaction with grounded language agents ï¼ˆ2023-02ï¼‰ | ![image-20250323224619855](figures/image-20250323224619855.png) | [Github](https://webshop-pnlp.github.io/) <br/> [Paper](https://arxiv.org/abs/2207.01206) |
| Meta-gui: Towards multi-modal con versational agents on mobile gui ï¼ˆ2022-11ï¼‰ | ![image-20250323224706852](figures/image-20250323224706852.png) | [Github](https://x-lance.github.io/META-GUI-Leaderboard/) <br> [Paper](https://arxiv.org/abs/2205.11029) |
| A dataset for interactive vision language navigation with unknown command fea sibility ï¼ˆ2022-08ï¼‰ | ![image-20250323224832001](figures/image-20250323224832001.png) | [Github](https://github.com/aburns4/MoTIF) <br/> [Paper](https://arxiv.org/abs/2202.02312) |
| Websrc: A dataset for web-based structural reading comprehension ï¼ˆ2021-11ï¼‰ | ![image-20250323224916473](figures/image-20250323224916473.png) | [Github](https://x-lance.github.io/WebSRC/) <br/> [Paper](https://arxiv.org/abs/2101.09465) |
| Mapping natural language instructions to mobile ui action sequences ï¼ˆ2020-06ï¼‰ | ![image-20250323225018260](figures/image-20250323225018260.png) | [Github](https://github.com/google-research/google-research/tree/master/seq2act) <br/> [Paper](https://arxiv.org/abs/2005.03776) |
| Reinforcement learning on web interfaces using workflow-guided exploration ï¼ˆ2018-02ï¼‰ | ![image-20250323225104521](figures/image-20250323225104521.png) | [Github](https://github.com/stanfordnlp/wge) <br/> [Paper](https://arxiv.org/abs/1802.08802) |
| Rico: A mobile app dataset for building data-driven design applications ï¼ˆ2017-10ï¼‰ | ![image-20250323225453702](figures/image-20250323225453702.png) | [HomePage](http://interactionmining.org/rico) <br/> [Paper](https://dl.acm.org/doi/10.1145/3126594.3126651) |
| World of bits: An open-domain platform for web based agents ï¼ˆ2017-08ï¼‰ | ![image-20250323225547948](figures/image-20250323225547948.png) |    [Paper](https://proceedings.mlr.press/v70/shi17a.html)    |
